import tensorflow as tf
from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Add
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model

WORKING_DIR = 'model'

class TransformerDecoderModel(Model):
    def __init__(self, vocab_size, max_length, d_model=128, num_heads=4, dff=256, num_layers=4):
        super().__init__()
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.d_model = d_model
        self.num_heads = num_heads
        self.dff = dff
        self.num_layers = num_layers
        
        # Define the layers
        self.dropout1 = Dropout(0.4)
        self.dense1 = Dense(d_model, activation='relu')
        
        self.embedding = Embedding(self.vocab_size, d_model, mask_zero=True)
        self.dropout2 = Dropout(0.4)
        
        self.transformer_layers = [self._create_transformer_layer() for _ in range(num_layers)]
        
        self.global_avg_pooling = GlobalAveragePooling1D()
        self.add_layer = Add()
        self.dense2 = Dense(d_model, activation='relu')
        self.output_layer = Dense(self.vocab_size, activation='softmax')

        # Build the model
        self.build_model()

    def _create_transformer_layer(self):
        mha = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.d_model)
        ffn = tf.keras.Sequential([
            Dense(self.dff, activation='relu'),
            Dense(self.d_model)
        ])
        layernorm1 = LayerNormalization(epsilon=1e-6)
        layernorm2 = LayerNormalization(epsilon=1e-6)
        return lambda x, training: self._transformer_layer(x, mha, ffn, layernorm1, layernorm2, training)
    
    def _transformer_layer(self, x, mha, ffn, layernorm1, layernorm2, training):
        attn_output = mha(x, x)
        attn_output = Dropout(0.1)(attn_output, training=training)
        out1 = layernorm1(x + attn_output)
        
        ffn_output = ffn(out1)
        ffn_output = Dropout(0.1)(ffn_output, training=training)
        out2 = layernorm2(out1 + ffn_output)
        
        return out2

    def build_model(self):
        # Define inputs
        inputs1 = Input(shape=(4096,))
        inputs2 = Input(shape=(self.max_length,))

        # Process image features
        x1 = self.dropout1(inputs1)
        x1 = self.dense1(x1)
        
        # Process sequence features
        x2 = self.embedding(inputs2)
        x2 = self.dropout2(x2)
        
        for layer in self.transformer_layers:
            x2 = layer(x2, training=True)
        
        # Pooling to reduce sequence length dimension
        x2 = self.global_avg_pooling(x2)
        
        # Combine features
        combined_features = self.add_layer([x1, x2])
        x = self.dense2(combined_features)
        outputs = self.output_layer(x)
        
        # Compile model
        self.model = Model(inputs=[inputs1, inputs2], outputs=outputs)
        self.model.compile(loss='categorical_crossentropy', optimizer='adam')

    def summary(self):
        return self.model.summary()

    def plot(self, file_path=WORKING_DIR + '/TransformerDecoderModel.png'):
        try:
            plot_model(self.model, show_shapes=True, to_file=file_path)
            print(f"Model diagram saved to {file_path}")
        except Exception as e:
            print(f"Error in saving model diagram: {e}")

    def fit(self, generator, epochs=1, steps_per_epoch=None, validation_data=None, verbose=1):
        """Train the model with data generated by the generator."""
        self.model.fit(generator,
                       epochs=epochs,
                       steps_per_epoch=steps_per_epoch,
                       validation_data=validation_data,
                       verbose=verbose)
