# ImageCaption
### Introduction
This neural network system for image captioning is developed based on the combination of ViT (Vision Transformer) for visual feature extraction and GPT-2 for natural language generation. The system takes an image as input and produces a descriptive sentence as output. ViT, an advanced transformer-based model in computer vision, is used to extract meaningful features from the input image. These features are then passed to GPT-2, a powerful transformer-based language model, to generate a fluent and contextually accurate caption. The system is implemented using the PyTorch library, enabling efficient end-to-end model training and inference. Additionally, MLFlow is integrated for model tracking and registration, while MinIO is used as a model storage backend, making it easier to manage and deploy the model in real-world applications.

### Prerequisites
* **Pytorch** ([instructions](https://pytorch.org))
* **MLFlow** ([instructions](https://mlflow.org/))
* **Transformers** ([instructions](https://pypi.org/project/transformers/))
* **NumPy** ([instructions](https://scipy.org/install.html))
* **OpenCV** ([instructions](https://pypi.python.org/pypi/opencv-python))
* **PIL** ([instructions](https://pillow.readthedocs.io/en/stable/?badge=latest))
* **Pandas** ([instructions](https://pandas.pydata.org/))
* **Matplotlib** ([instructions](https://matplotlib.org/))
* **tqdm** ([instructions](https://pypi.python.org/pypi/tqdm))
* **torchvision** ([instructions](https://pytorch.org/vision/stable/index.html))

### Usage
* **Preparation:** Load the Traffic Vision Captions dataset from DAI TRIEU PHI [here](https://www.kaggle.com/datasets/trieuphi/traffic-pictures-captioning) . The dataset contains a large collection of images along with Vietnamese captions that you can use to train an image captioning model. 

### Results
Here are some captions generated by this model:
![examples](https://github.com/HTN-DT-Beo/ImageCaption/blob/main/BartPho_ViT_GPT2/test_picture_01.png)
![examples](https://github.com/HTN-DT-Beo/ImageCaption/blob/main/BartPho_ViT_GPT2/test_picture_02.png)

### Evaluate (Upcomming)
* **BLEU-1 = 0.547**
* **BLEU-2 = 0.473**
* **BLEU-3 = 0.407**
* **BLEU-4 = 0.341**
* **ROUGE-1 = 0.893**
* **ROUGE-2 = 0.826**
* **ROUGE-L = 0.847**
* **CIDEr = 1.425**

### References
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. ICML 2015.
* [vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)
* K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel and Y. Bengio, "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention," in Computer Vision and Pattern Recognition, 2016.
* P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould and L. Zhang, "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering," in Computer Vision and Pattern Recognition, 2018.
* Q. H. Lam, Q. D. Le, V. K. Nguyen, N. Luu and T. Nguyen, "UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning," in Computational Collective Intelligence, 2020. 
* A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn and X. Zhai, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale," in International Conference on Learning Representations, 2021.
* B. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu and L. Yuan, "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks," in Conference on Computer Vision and Pattern Recognition, 2023. 
* P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi and J. Gao, "VinVL: Revisiting Visual Representations in Vision-Language Models," Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, p. 579–5588, 2021. 
* L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso and J. Gao, "Unified Vision-Language Pre-Training for Image Captioning and VQA," Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, p. 13041–13049, 2019. 
* S. Herdade, A. Kappeler, K. Boakye and J. Soares, "Image Captioning: Transforming Objects into Words," Advances in Neural Information Processing Systems, vol. 32, 2019. 
* Y. Pan, T. Yao, Y. Li and T. Mei, "X-Linear Attention Networks for Image Captioning," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
* M. Cornia, M. Stefanini, L. Baraldi and R. Cucchiara, "Meshed-Memory Transformer for Image Captioning," p. 10578–10587, 2020. 
* A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin, "Attention Is All You Need," in Advances in Neural Information Processing Systems, 2017.
* L. Huang, W. Wang, J. Chen and X.-Y. Wei, "Attention on Attention for Image Captioning," Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4634-4643, 2019. 
* G. O. d. Santos, E. L. Colombini and S. Avila, "CIDEr-R: Robust Consensus-based Image Description Evaluation," in Computer Vision and Pattern Recognition, 2021.
* A. M. Rinaldi, C. Russo and C. Tommasino, "Automatic image captioning combining natural language processing and deep neural networks," Results in Engineering, p. 14, 2023. 
* R. khan, M. S. Islam, K. Kanwal, M. Iqbal, M. I. Hossain and Z. Ye, "A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism," nformation Technology and Control, p. 16, 2022. 
* K. NGUYEN, "EMPIRICAL STUDY OF FEATURE EXTRACTION APPROACHESFOR IMAGE CAPTIONING IN VIETNAMESE," Journal of Computer Science and Cybernetics, vol. 38, p. 327–346, 2022. 
* E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang and W. Chen, "LoRA: Low-Rank Adaptation of Large Language Models," in International Conference on Learning Representations (ICLR), 2023.
* N. L. Tran, D. M. Le and D. Q. Nguyen, "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese," in INTERSPEECH, 2022. 
* C. A. Pham, Q. V. Nguyen, H. T. Vuong and T. Q. Ha, "KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain," arXiv, 2024.
