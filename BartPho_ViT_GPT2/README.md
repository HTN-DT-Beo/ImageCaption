# ImageCaption
### Introduction
This neural system for image captioning is based on the combination of YOLOv8 for visual feature extraction and GPT-2 for natural language generation. The input is an image, and the output is a descriptive sentence of the image content. YOLOv8, a state-of-the-art object detection model, is used to extract relevant visual features from the image. These features are then passed to GPT-2, a transformer-based language model, which generates a fluent and coherent caption describing the image. The system is implemented using the PyTorch library for model training and inference, allowing for efficient end-to-end training of the image feature extraction and caption generation components.

### Prerequisites
* **Pytorch** ([instructions](https://pytorch.org))
* **Transformers** ([instructions](https://pypi.org/project/transformers/))
* **NumPy** ([instructions](https://scipy.org/install.html))
* **OpenCV** ([instructions](https://pypi.python.org/pypi/opencv-python))
* **PIL** ([instructions](https://pillow.readthedocs.io/en/stable/?badge=latest))
* **Pandas** ([instructions](https://pandas.pydata.org/))
* **Matplotlib** ([instructions](https://matplotlib.org/))
* **tqdm** ([instructions](https://pypi.python.org/pypi/tqdm))
* **torchvision** ([instructions](https://pytorch.org/vision/stable/index.html))

### Usage
* **Preparation:** Load the Traffic Vision Captions dataset from DAI TRIEU PHI [here](https://www.kaggle.com/datasets/trieuphi/traffic-pictures-captioning) . The dataset contains a large collection of images along with Vietnamese captions that you can use to train an image captioning model. 

### Results
Here are some captions generated by this model:
![examples](https://github.com/HTN-DT-Beo/ImageCaption/blob/main/BartPho_ViT_GPT2/test_picture_01.png)
![examples](https://github.com/HTN-DT-Beo/ImageCaption/blob/main/BartPho_ViT_GPT2/test_picture_02.png)

### Evaluate (Upcomming)
* **BLEU-1 = %**
* **BLEU-2 = %**
* **BLEU-3 = %**
* **BLEU-4 = %**
* **ROUGE-1 = %**
* **ROUGE-2 = %**
* **ROUGE-L = %**
* **CIDEr = %**
* **METEOR = %**

### References
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. ICML 2015.
* [vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)
