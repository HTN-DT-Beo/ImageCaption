# ImageCaption
### Introduction
This neural network system for image captioning is developed based on the combination of ViT (Vision Transformer) for visual feature extraction and GPT-2 for natural language generation. The system takes an image as input and produces a descriptive sentence as output. ViT, an advanced transformer-based model in computer vision, is used to extract meaningful features from the input image. These features are then passed to GPT-2, a powerful transformer-based language model, to generate a fluent and contextually accurate caption. The system is implemented using the PyTorch library, enabling efficient end-to-end model training and inference. Additionally, MLFlow is integrated for model tracking and registration, while MinIO is used as a model storage backend, making it easier to manage and deploy the model in real-world applications.

### Prerequisites
* **Pytorch** ([instructions](https://pytorch.org))
* **MLFlow** ([instructions](https://mlflow.org/))
* **Transformers** ([instructions](https://pypi.org/project/transformers/))
* **NumPy** ([instructions](https://scipy.org/install.html))
* **OpenCV** ([instructions](https://pypi.python.org/pypi/opencv-python))
* **PIL** ([instructions](https://pillow.readthedocs.io/en/stable/?badge=latest))
* **Pandas** ([instructions](https://pandas.pydata.org/))
* **Matplotlib** ([instructions](https://matplotlib.org/))
* **tqdm** ([instructions](https://pypi.python.org/pypi/tqdm))
* **torchvision** ([instructions](https://pytorch.org/vision/stable/index.html))

### Usage
* **Preparation:** Load the Traffic Vision Captions dataset from DAI TRIEU PHI [here](https://www.kaggle.com/datasets/trieuphi/traffic-pictures-captioning) . The dataset contains a large collection of images along with Vietnamese captions that you can use to train an image captioning model. 

### Results
Here are some captions generated by this model:
![examples](https://github.com/HTN-DT-Beo/ImageCaption/blob/main/BartPho_ViT_GPT2/test_picture_01.png)
![examples](https://github.com/HTN-DT-Beo/ImageCaption/blob/main/BartPho_ViT_GPT2/test_picture_03.png)

### Evaluate (Upcomming)
* **BLEU-1 = 0.357**
* **BLEU-2 = 0.246**
* **BLEU-3 = 0.169**
* **BLEU-4 = 0.117**
* **ROUGE-1 = 0.701**
* **ROUGE-2 = 0.465**
* **ROUGE-L = 0.528**
* **CIDEr = 0.247**

### References
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. ICML 2015.
* [vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)
