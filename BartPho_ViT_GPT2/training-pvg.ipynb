{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11334502,"sourceType":"datasetVersion","datasetId":6723117},{"sourceId":11337142,"sourceType":"datasetVersion","datasetId":7085417,"isSourceIdPinned":true},{"sourceId":331865,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":277580,"modelId":298468},{"sourceId":372272,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":307843,"modelId":328293}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow pyvi minio -q\n!pip install hf_xet -q\n\nimport mlflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:35:32.960635Z","iopub.execute_input":"2025-05-12T13:35:32.960968Z","iopub.status.idle":"2025-05-12T13:35:55.274987Z","shell.execute_reply.started":"2025-05-12T13:35:32.960948Z","shell.execute_reply":"2025-05-12T13:35:55.274149Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"mlflow.set_tracking_uri(\"http://36.50.135.226:7893/\")\n\n# from mlflow.tracking import MlflowClient\n\n# client = MlflowClient()\n# client.restore_experiment(experiment_id=\"2\")\n\nmlflow.set_experiment(experiment_id=\"10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:35:55.276801Z","iopub.execute_input":"2025-05-12T13:35:55.277228Z","iopub.status.idle":"2025-05-12T13:35:55.746006Z","shell.execute_reply.started":"2025-05-12T13:35:55.277210Z","shell.execute_reply":"2025-05-12T13:35:55.745254Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='s3://mlflow/6', creation_time=1745743711248, experiment_id='10', last_update_time=1745743711248, lifecycle_stage='active', name='ImageCaption_TPC37k_BartPho-ViT-GPT2_LoRALayerFT', tags={}>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\n\nos.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://36.50.135.226:9000\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:35:55.746702Z","iopub.execute_input":"2025-05-12T13:35:55.746965Z","iopub.status.idle":"2025-05-12T13:35:55.750752Z","shell.execute_reply.started":"2025-05-12T13:35:55.746947Z","shell.execute_reply":"2025-05-12T13:35:55.750059Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# MinIO","metadata":{}},{"cell_type":"code","source":"from minio import Minio\nfrom minio.error import S3Error\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:35:55.751504Z","iopub.execute_input":"2025-05-12T13:35:55.751756Z","iopub.status.idle":"2025-05-12T13:35:56.181704Z","shell.execute_reply.started":"2025-05-12T13:35:55.751716Z","shell.execute_reply":"2025-05-12T13:35:56.180930Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nimport transformers\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor\nimport torch\nfrom PIL import Image\nimport torch.nn as nn\nimport cv2\nimport torchvision\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom collections import OrderedDict\nfrom transformers import GPT2Config, GPT2LMHeadModel\nimport mlflow\nfrom mlflow.models import infer_signature\nimport mlflow.pytorch\nimport re\nfrom pyvi import ViTokenizer\nfrom torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:35:56.182506Z","iopub.execute_input":"2025-05-12T13:35:56.182776Z","iopub.status.idle":"2025-05-12T13:36:29.823675Z","shell.execute_reply.started":"2025-05-12T13:35:56.182752Z","shell.execute_reply":"2025-05-12T13:36:29.822938Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 13:36:11.034135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747056971.443562      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747056971.548976      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 1. Chuẩn bị file thư viện","metadata":{}},{"cell_type":"code","source":"pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:29.824549Z","iopub.execute_input":"2025-05-12T13:36:29.825202Z","iopub.status.idle":"2025-05-12T13:36:31.605690Z","shell.execute_reply.started":"2025-05-12T13:36:29.825176Z","shell.execute_reply":"2025-05-12T13:36:31.604743Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"with open(\"requirements.txt\") as f:\n    pip_reqs = [line.strip() for line in f if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:31.608685Z","iopub.execute_input":"2025-05-12T13:36:31.608980Z","iopub.status.idle":"2025-05-12T13:36:31.614116Z","shell.execute_reply.started":"2025-05-12T13:36:31.608959Z","shell.execute_reply":"2025-05-12T13:36:31.613473Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 2. Load data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef load_data(data_path=\"/kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv\"):\n    try:\n        df = pd.read_csv(data_path)\n        df = df[['original_url','local_path', 'search_query', 'short_caption']].rename(columns={\n            'original_url': 'original_url',\n            'local_path': 'url',\n            'search_query': 'search_query',\n            'short_caption': 'caption'\n        })\n        print(f\"Đã tải CSV từ: {data_path} (Kích thước: {df.shape})\")\n        return df\n    except FileNotFoundError:\n        print(f\"Lỗi: Không tìm thấy file CSV tại {data_path}\")\n        raise\n    except Exception as e:\n        print(f\"Lỗi khi đọc CSV: {e}\")\n        raise\n\n# Load dữ liệu\ndf = load_data()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:31.615023Z","iopub.execute_input":"2025-05-12T13:36:31.615306Z","iopub.status.idle":"2025-05-12T13:36:32.202628Z","shell.execute_reply.started":"2025-05-12T13:36:31.615277Z","shell.execute_reply":"2025-05-12T13:36:32.201876Z"}},"outputs":[{"name":"stdout","text":"Đã tải CSV từ: /kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv (Kích thước: (37056, 4))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n# # Chia DataFrame thành 3 phần bằng nhau\n# n = len(df) // 3  # Số hàng mỗi phần (làm tròn xuống)\n# df_1 = df.iloc[:n]\n# df_2 = df.iloc[n:2*n]\n# df_3 = df.iloc[2*n:]\n\n# # In thông tin để kiểm tra\n# print(f\"Kích thước df_1: {df_1.shape}\")\n# print(f\"Kích thước df_2: {df_2.shape}\")\n# print(f\"Kích thước df_3: {df_3.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:32.203550Z","iopub.execute_input":"2025-05-12T13:36:32.203828Z","iopub.status.idle":"2025-05-12T13:36:32.207247Z","shell.execute_reply.started":"2025-05-12T13:36:32.203811Z","shell.execute_reply":"2025-05-12T13:36:32.206703Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 3. Split data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Hàm chia dữ liệu giữ nguyên\ndef split_data(df, stratify_col='search_query', test_size=0.1, val_size=0.1, random_state=42):\n    unique_urls = df.drop_duplicates('original_url')\n\n    sss_1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_val_idx, test_idx in sss_1.split(unique_urls, unique_urls[stratify_col]):\n        train_val_urls = unique_urls.iloc[train_val_idx]['original_url']\n        test_urls = unique_urls.iloc[test_idx]['original_url']\n\n    df_train_val = df[df['original_url'].isin(train_val_urls)]\n    df_test = df[df['original_url'].isin(test_urls)]\n\n    unique_train_val_urls = df_train_val.drop_duplicates('original_url')\n    val_ratio = val_size / (1 - test_size)\n    sss_2 = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n    for train_idx, val_idx in sss_2.split(unique_train_val_urls, unique_train_val_urls[stratify_col]):\n        train_urls = unique_train_val_urls.iloc[train_idx]['original_url']\n        val_urls = unique_train_val_urls.iloc[val_idx]['original_url']\n\n    df_train = df_train_val[df_train_val['original_url'].isin(train_urls)]\n    df_val = df_train_val[df_train_val['original_url'].isin(val_urls)]\n\n    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:32.208025Z","iopub.execute_input":"2025-05-12T13:36:32.208311Z","iopub.status.idle":"2025-05-12T13:36:32.228495Z","shell.execute_reply.started":"2025-05-12T13:36:32.208291Z","shell.execute_reply":"2025-05-12T13:36:32.227934Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_split_save(df: pd.DataFrame): \n    # Split\n    train_df, val_df, test_df = split_data(df, stratify_col='search_query')\n    \n    # Kiểm tra\n    print(\"Train size:\", len(train_df))\n    print(\"Val size:\", len(val_df))\n    print(\"Test size:\", len(test_df))\n    print(train_df['search_query'].value_counts(normalize=True))\n    print(val_df['search_query'].value_counts(normalize=True))\n    print(test_df['search_query'].value_counts(normalize=True))\n\n    # Save file\n    # Reset index\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    test_df = test_df.reset_index(drop=True)\n    \n    # Kiểm tra số lượng mẫu sau khi chia\n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n    \n    # Lưu các DataFrame vào JSON\n    train_df.to_json(\"train.json\", orient='records', indent=4, force_ascii=False)\n    val_df.to_json(\"val.json\", orient='records', indent=4, force_ascii=False)\n    test_df.to_json(\"test.json\", orient='records', indent=4, force_ascii=False)\n    \n    print(\"Đã lưu train.json, val.json, test.json\")\n    return train_df, val_df, test_df, df\n\ntrain_df, val_df, test_df, df = load_split_save(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:32.229193Z","iopub.execute_input":"2025-05-12T13:36:32.229394Z","iopub.status.idle":"2025-05-12T13:36:32.483173Z","shell.execute_reply.started":"2025-05-12T13:36:32.229378Z","shell.execute_reply":"2025-05-12T13:36:32.482405Z"}},"outputs":[{"name":"stdout","text":"Train size: 29640\nVal size: 3708\nTest size: 3708\nsearch_query\nvỉa hè đường phố việt nam               0.031039\nđường phố sau bão                       0.023212\nngười đợi xe bus tại trạm               0.023077\nnắng nóng đường phố việt nam            0.022807\nngười đi bộ tại ngã tư                  0.022267\n                                          ...   \nchướng ngại vật trên đường đi bộ        0.004049\ncông trình đào đường                    0.003914\nvạch dành cho người khiếm thị           0.002834\nbiển báo chữ nổi cho người khiếm thị    0.001754\nlối đi dành cho người khuyết tật        0.001350\nName: proportion, Length: 65, dtype: float64\nsearch_query\nvỉa hè đường phố việt nam               0.031284\nđường phố sau bão                       0.023732\nnắng nóng đường phố việt nam            0.022654\nngười đợi xe bus tại trạm               0.022654\nngười đi bộ tại ngã tư                  0.022654\n                                          ...   \nvạch kẻ đường cho người đi bộ           0.004315\ncông trình đào đường                    0.003236\nvạch dành cho người khiếm thị           0.003236\nbiển báo chữ nổi cho người khiếm thị    0.002157\nlối đi dành cho người khuyết tật        0.001079\nName: proportion, Length: 65, dtype: float64\nsearch_query\nvỉa hè đường phố việt nam               0.031284\nđường phố sau bão                       0.023732\nngười đi bộ tại ngã tư                  0.022654\nnắng nóng đường phố việt nam            0.022654\nngười đợi xe bus tại trạm               0.022654\n                                          ...   \nvạch kẻ đường cho người đi bộ           0.004315\ncông trình đào đường                    0.003236\nvạch dành cho người khiếm thị           0.003236\nbiển báo chữ nổi cho người khiếm thị    0.002157\nlối đi dành cho người khuyết tật        0.001079\nName: proportion, Length: 65, dtype: float64\nTrain: 29640, Val: 3708, Test: 3708\nĐã lưu train.json, val.json, test.json\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# 4. Load feature extractor","metadata":{}},{"cell_type":"code","source":"feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n\ndef load_image(local_path, dataset_base_path=\"/kaggle/input/traffic-pictures-captioning/\"):\n    \"\"\"\n    Load ảnh từ local_path trong dataset trên Kaggle.\n    \n    Parameters:\n    - local_path (str): Đường dẫn tương đối của ảnh (từ cột 'url' trong DataFrame)\n    - dataset_base_path (str): Đường dẫn gốc đến dataset\n    \n    Returns:\n    - image_rgb (numpy.ndarray): Ảnh ở định dạng RGB, hoặc None nếu lỗi\n    \"\"\"\n    try:\n        # Chuẩn hóa đường dẫn ảnh\n        local_path = local_path.lstrip('./')  # Loại bỏ './' nếu có\n        full_image_path = os.path.join(dataset_base_path, local_path)\n        \n        # Đọc ảnh bằng OpenCV\n        image = cv2.imread(full_image_path)\n        if image is None:\n            print(f\"Lỗi: Không thể đọc ảnh từ {full_image_path}\")\n            return None\n        \n        # Chuyển từ BGR sang RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image_rgb\n    \n    except Exception as e:\n        print(f\"Lỗi khi xử lý ảnh {full_image_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:32.483958Z","iopub.execute_input":"2025-05-12T13:36:32.484147Z","iopub.status.idle":"2025-05-12T13:36:32.654637Z","shell.execute_reply.started":"2025-05-12T13:36:32.484133Z","shell.execute_reply":"2025-05-12T13:36:32.653726Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f60f0134b48447899f5106831d54235"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# 5. Load Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer của BartPho\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n\n# Kiểm tra vocab size\nvocab_size = tokenizer.vocab_size\nprint(f\"VOCAB SIZE: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:32.655531Z","iopub.execute_input":"2025-05-12T13:36:32.656372Z","iopub.status.idle":"2025-05-12T13:36:33.766866Z","shell.execute_reply.started":"2025-05-12T13:36:32.656341Z","shell.execute_reply":"2025-05-12T13:36:33.766086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/897 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e8b721e3844f40a5e33f5e73575fc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b723b4266626464cb5900f70b8e8c51c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1c5a432d52494ea595d1e4b8c272c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b739676bd944eb8225eb799d0dec14"}},"metadata":{}},{"name":"stdout","text":"VOCAB SIZE: 64000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# In ra các special tokens\nprint(\"Special Tokens:\", tokenizer.special_tokens_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:33.767723Z","iopub.execute_input":"2025-05-12T13:36:33.768172Z","iopub.status.idle":"2025-05-12T13:36:33.772266Z","shell.execute_reply.started":"2025-05-12T13:36:33.768145Z","shell.execute_reply":"2025-05-12T13:36:33.771455Z"}},"outputs":[{"name":"stdout","text":"Special Tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# 6. Các hàm tiền xử lý caption","metadata":{}},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    return re.sub(r\"[^\\w\\s,!?.]\", \"\", text).strip()\n\ndef to_lowercase(text: str) -> str:\n    return text.lower()\n\ndef join_vietnamese_compounds(text: str) -> str:\n    return ViTokenizer.tokenize(text)\n\ndef caption_preprocess(text: str) -> str:\n    text = clean_text(text)\n    text = to_lowercase(text)\n    text = join_vietnamese_compounds(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:33.772976Z","iopub.execute_input":"2025-05-12T13:36:33.773223Z","iopub.status.idle":"2025-05-12T13:36:33.792073Z","shell.execute_reply.started":"2025-05-12T13:36:33.773206Z","shell.execute_reply":"2025-05-12T13:36:33.791416Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"text = caption_preprocess(\"Kiểm tra phân tách từ\")\n\n# Tách token\ntokens = tokenizer.tokenize(text)\n\n# Chuyển token thành ID\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\ntext = tokenizer.decode(token_ids, skip_special_tokens=True)\n# In kết quả\nprint(\"List Word (Tokenized):\", tokens)\nprint(\"List Token ID:\", token_ids)\nprint(\"Text:\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:33.792923Z","iopub.execute_input":"2025-05-12T13:36:33.793158Z","iopub.status.idle":"2025-05-12T13:36:33.814291Z","shell.execute_reply.started":"2025-05-12T13:36:33.793143Z","shell.execute_reply":"2025-05-12T13:36:33.813538Z"}},"outputs":[{"name":"stdout","text":"List Word (Tokenized): ['kiểm_tra', 'phân_tách', 'từ']\nList Token ID: [342, 31166, 39]\nText: kiểm_tra phân_tách từ\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Hàm tính số từ của một caption\ndef count_words(caption):\n    preprocessed_caption = caption_preprocess(caption)\n    tokens = tokenizer.tokenize(preprocessed_caption)\n    return len(tokens)\n\n# Áp dụng hàm cho toàn bộ dataframe\ndf['word_count'] = df['caption'].apply(count_words)\n\n# Lấy độ dài lớn nhất\nmax_length = df['word_count'].max()\n\n# In ra\nprint(f\"Độ dài caption dài nhất là: {max_length} từ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:33.815071Z","iopub.execute_input":"2025-05-12T13:36:33.815362Z","iopub.status.idle":"2025-05-12T13:36:55.547476Z","shell.execute_reply.started":"2025-05-12T13:36:33.815342Z","shell.execute_reply":"2025-05-12T13:36:55.546798Z"}},"outputs":[{"name":"stdout","text":"Độ dài caption dài nhất là: 71 từ\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Thống kê tập dữ liệu","metadata":{}},{"cell_type":"code","source":"# Kiểm tra vocal size\nprint(f\"VOCAB SIZE: {vocab_size}\")\n\n# Kiểm tra maxlength từng tập train_df, val_df, test_df\ndf['word_count'] = train_df['caption'].apply(count_words)\nmax_length = df['word_count'].max()\nmean_length = round(df['word_count'].mean(), 2)\nprint(f\"Train max_length: {max_length} từ\")\nprint(f\"Train mean_length: {mean_length} từ\")\n\n\ndf['word_count'] = val_df['caption'].apply(count_words)\nmax_length = df['word_count'].max()\nmean_length = round(df['word_count'].mean(), 2)\nprint(f\"Train max_length: {max_length} từ\")\nprint(f\"Train mean_length: {mean_length} từ\")\n\ndf['word_count'] = test_df['caption'].apply(count_words)\nmax_length = df['word_count'].max()\nmean_length = round(df['word_count'].mean(), 2)\nprint(f\"Train max_length: {max_length} từ\")\nprint(f\"Train mean_length: {mean_length} từ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:36:55.548172Z","iopub.execute_input":"2025-05-12T13:36:55.548398Z","iopub.status.idle":"2025-05-12T13:37:17.286608Z","shell.execute_reply.started":"2025-05-12T13:36:55.548384Z","shell.execute_reply":"2025-05-12T13:37:17.285783Z"}},"outputs":[{"name":"stdout","text":"VOCAB SIZE: 64000\nTrain max_length: 71.0 từ\nTrain mean_length: 31.34 từ\nTrain max_length: 64.0 từ\nTrain mean_length: 31.12 từ\nTrain max_length: 58.0 từ\nTrain mean_length: 31.16 từ\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Số caption unique trong df:\", df['caption'].nunique())\nprint(\"Số caption unique trong train df:\", train_df['caption'].nunique())\nprint(\"Số caption unique trong val df:\", val_df['caption'].nunique())\nprint(\"Số caption unique trong test df:\", test_df['caption'].nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:17.287483Z","iopub.execute_input":"2025-05-12T13:37:17.287686Z","iopub.status.idle":"2025-05-12T13:37:17.321435Z","shell.execute_reply.started":"2025-05-12T13:37:17.287665Z","shell.execute_reply":"2025-05-12T13:37:17.320867Z"}},"outputs":[{"name":"stdout","text":"Số caption unique trong df: 9081\nSố caption unique trong train df: 7266\nSố caption unique trong val df: 922\nSố caption unique trong test df: 920\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from tqdm import tqdm\n\nfrom tqdm import tqdm\n\ndef get_token_word_stats(df, tokenizer, caption_preprocess):\n    \"\"\"\n    Tính tổng số và số lượng duy nhất của tokens và words trong cột 'caption'\n\n    Args:\n        df (pd.DataFrame): DataFrame chứa cột 'caption'.\n        tokenizer: Tokenizer đã load.\n        caption_preprocess (func): Hàm xử lý caption trước khi tokenize.\n\n    Returns:\n        dict: {\n            'total_tokens': int,\n            'unique_tokens': int,\n            'total_words': int,\n            'unique_words': int\n        }\n    \"\"\"\n    unique_tokens = set()\n    total_tokens = 0\n\n    unique_words = set()\n    total_words = 0\n\n    for caption in tqdm(df['caption']):\n        # Word: tách theo khoảng trắng\n        words = caption.split()\n        unique_words.update(words)\n        total_words += len(words)\n        \n        text = caption_preprocess(caption)\n\n        # Token: dựa vào tokenizer\n        tokens = tokenizer.tokenize(text)\n        unique_tokens.update(tokens)\n        total_tokens += len(tokens)\n\n\n    return total_tokens, len(unique_tokens), total_words, len(unique_words)\n\n\ntotal_tokens, unique_tokens, total_words, unique_words = get_token_word_stats(df, tokenizer, caption_preprocess)\n\nprint(\"Type    |    Total    |    Unique    \")\nprint(f\"Token   |   {total_tokens}   |     {unique_tokens}\")\nprint(f\"Work    |   {total_words}   |     {unique_words}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:17.322121Z","iopub.execute_input":"2025-05-12T13:37:17.322310Z","iopub.status.idle":"2025-05-12T13:37:39.826140Z","shell.execute_reply.started":"2025-05-12T13:37:17.322296Z","shell.execute_reply":"2025-05-12T13:37:39.825419Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 37056/37056 [00:22<00:00, 1647.33it/s]","output_type":"stream"},{"name":"stdout","text":"Type    |    Total    |    Unique    \nToken   |   1160004   |     1731\nWork    |   1267648   |     2356\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Tiền xử lý input model","metadata":{}},{"cell_type":"code","source":"def process_data(image_url, caption):\n    try:\n        img_array = load_image(image_url)\n        if img_array is None:\n            return None\n        \n        pixel_values = feature_extractor(img_array, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        caption = caption_preprocess(caption)\n        tokenized_caption = tokenizer(caption, padding=\"max_length\", max_length=max_length, truncation=True)\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": torch.tensor(tokenized_caption[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(tokenized_caption[\"attention_mask\"])\n        }\n    except Exception as e:\n        print(f\"Error processing data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:39.826876Z","iopub.execute_input":"2025-05-12T13:37:39.827073Z","iopub.status.idle":"2025-05-12T13:37:39.831976Z","shell.execute_reply.started":"2025-05-12T13:37:39.827059Z","shell.execute_reply":"2025-05-12T13:37:39.831411Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# 8. Tạo tập dữ liệu huấn luyện","metadata":{}},{"cell_type":"code","source":"class ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions):\n        self.image_paths = image_paths\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        data = process_data(self.image_paths[idx], self.captions[idx])\n        if data is None:\n            return self.__getitem__((idx + 1) % len(self))\n        return data\n\ndef custom_collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    return {\n        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch])\n    }\n\ntrain_dataset = ImageCaptionDataset(train_df[\"url\"], train_df[\"caption\"])\nval_dataset = ImageCaptionDataset(val_df[\"url\"], val_df[\"caption\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:39.835078Z","iopub.execute_input":"2025-05-12T13:37:39.835291Z","iopub.status.idle":"2025-05-12T13:37:39.860323Z","shell.execute_reply.started":"2025-05-12T13:37:39.835276Z","shell.execute_reply":"2025-05-12T13:37:39.859613Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# 9. Cấu hình LoRA","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom peft import PeftModel\n\n# Cấu hình LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],  # Các lớp trong GPT2\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:39.860976Z","iopub.execute_input":"2025-05-12T13:37:39.861207Z","iopub.status.idle":"2025-05-12T13:37:40.333125Z","shell.execute_reply.started":"2025-05-12T13:37:39.861191Z","shell.execute_reply":"2025-05-12T13:37:40.332595Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# 10. Định nghĩa mô hình","metadata":{}},{"cell_type":"code","source":"vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\ndel vit_model.classifier\nvit_model.vit.pooler = torch.nn.Sequential(OrderedDict([\n    ('dense', torch.nn.Linear(in_features=768, out_features=768, bias=True)),\n    ('activation', torch.nn.Tanh())\n]))\n\nconfig = GPT2Config.from_pretrained(\"gpt2\")\nconfig.add_cross_attention = True\nconfig.vocab_size = vocab_size\n\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config, ignore_mismatched_sizes=True)\ngpt2_model.resize_token_embeddings(config.vocab_size)\n\n# Áp dụng LoRA vào GPT2\ngpt2_model = get_peft_model(gpt2_model, lora_config)\ngpt2_model.print_trainable_parameters()  # kiểm tra số lượng tham số cần huấn luyện\n\nmodel = VisionEncoderDecoderModel(encoder=vit_model.vit, decoder=gpt2_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:40.333876Z","iopub.execute_input":"2025-05-12T13:37:40.334126Z","iopub.status.idle":"2025-05-12T13:37:46.194337Z","shell.execute_reply.started":"2025-05-12T13:37:40.334105Z","shell.execute_reply":"2025-05-12T13:37:46.193578Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16a732b89c84b7d916a2ba00978a6fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb8baf6f3bf4b13b0c9d778ebae8227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c203669ea5654043b6da010c408838c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac450ad37af345a3b44a58b2d43fea35"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized because the shapes did not match:\n- transformer.wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([64000, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e3164f5ecd4605a5e9d6afee5b5ce1"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,179,648 || all params: 164,540,928 || trainable%: 0.7169\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:37:46.195127Z","iopub.execute_input":"2025-05-12T13:37:46.195374Z","iopub.status.idle":"2025-05-12T13:37:46.202643Z","shell.execute_reply.started":"2025-05-12T13:37:46.195347Z","shell.execute_reply":"2025-05-12T13:37:46.202092Z"}},"outputs":[{"name":"stdout","text":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): Sequential(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): PeftModelForCausalLM(\n    (base_model): LoraModel(\n      (model): GPT2LMHeadModel(\n        (transformer): GPT2Model(\n          (wte): Embedding(64000, 768)\n          (wpe): Embedding(1024, 768)\n          (drop): Dropout(p=0.1, inplace=False)\n          (h): ModuleList(\n            (0-11): 12 x GPT2Block(\n              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (attn): GPT2Attention(\n                (c_attn): lora.Linear(\n                  (base_layer): Conv1D(nf=2304, nx=768)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=2304, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (c_proj): lora.Linear(\n                  (base_layer): Conv1D(nf=768, nx=768)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (attn_dropout): Dropout(p=0.1, inplace=False)\n                (resid_dropout): Dropout(p=0.1, inplace=False)\n              )\n              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (crossattention): GPT2Attention(\n                (c_attn): lora.Linear(\n                  (base_layer): Conv1D(nf=1536, nx=768)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1536, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_attn): Conv1D(nf=768, nx=768)\n                (c_proj): lora.Linear(\n                  (base_layer): Conv1D(nf=768, nx=768)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (attn_dropout): Dropout(p=0.1, inplace=False)\n                (resid_dropout): Dropout(p=0.1, inplace=False)\n              )\n              (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): GPT2MLP(\n                (c_fc): Conv1D(nf=3072, nx=768)\n                (c_proj): lora.Linear(\n                  (base_layer): Conv1D(nf=768, nx=3072)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3072, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): NewGELUActivation()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (lm_head): Linear(in_features=768, out_features=64000, bias=False)\n      )\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# 11. Định nghĩa lớp mô hình","metadata":{}},{"cell_type":"code","source":"class LoRACaptionWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    def load_context(self, context):\n        try:\n            # Đường dẫn đầy đủ đến các thư mục con trong artifact\n            model_path = context.artifacts[\"model_dir\"]\n            tokenizer_path = os.path.join(model_path, \"tokenizer\")\n            feature_extractor_path = os.path.join(model_path, \"feature_extractor\")\n\n            \n            # Load các thành phần với đường dẫn chính xác\n            self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            self.feature_extractor = ViTImageProcessor.from_pretrained(feature_extractor_path)\n            \n            # Đặt model vào chế độ eval\n            self.model.eval()\n        except Exception as e:\n            raise ValueError(f\"Error loading model: {str(e)}\")\n\n    def predict(self, context, model_input):\n        try:\n            # Xử lý đầu vào\n            if isinstance(model_input, dict):\n                image_url = model_input[\"url\"][0] if \"url\" in model_input else model_input[\"image_path\"][0]\n            else:\n                image_url = model_input.iloc[0][\"url\"] if \"url\" in model_input.columns else model_input.iloc[0][\"image_path\"]\n            \n            image = self.load_image(image_url)\n            if image is None:\n                return [\"\"]\n                \n            # Tiền xử lý ảnh và tạo caption\n            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n            \n            output_ids = self.model.generate(\n                pixel_values,\n                max_length=45,\n                min_length=20,\n                num_beams=4,\n                do_sample=True,\n                temperature=0.8,\n                top_k=20,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                repetition_penalty=2.0,\n                early_stopping=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                decoder_start_token_id=self.tokenizer.bos_token_id\n            )\n            \n            caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            return [caption.replace(\"_\", \" \").strip()]\n            \n        except Exception as e:\n            print(f\"[ERROR] during prediction: {e}\")\n            return [\"\"]\n   \n    def load_image(self, image_url):\n        try:\n            import requests\n            import numpy as np\n            from PIL import Image\n            from io import BytesIO\n            \n            response = requests.get(image_url, timeout=10)\n            if response.status_code != 200:\n                return None\n                \n            image = Image.open(BytesIO(response.content))\n            return image.convert(\"RGB\") if image else None\n            \n        except Exception as e:\n            print(f\"[ERROR] loading image: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:23:18.946210Z","iopub.execute_input":"2025-04-27T10:23:18.946771Z","iopub.status.idle":"2025-04-27T10:23:19.043149Z","shell.execute_reply.started":"2025-04-27T10:23:18.946750Z","shell.execute_reply":"2025-04-27T10:23:19.042232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. Huấn luyện mô hình","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ndef evaluate_model(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            encoder_outputs = model.encoder(pixel_values=pixel_values)\n            outputs = model.decoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                encoder_hidden_states=encoder_outputs.last_hidden_state,\n                labels=input_ids\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n    return total_loss / len(val_dataloader)\n\ndef train_model(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr=5e-5, epochs=1, epoch_count=1, model_name='BartPho_ViT_GPT2_LoRA_ICG', data_part=\"Part_1\", df_log=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Optimizer chỉ update các tham số có requires_grad=True (LoRA adapter)\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Thống kê tham số trainable\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n\n    run_name = model_name + \"_\" + data_part\n\n    with mlflow.start_run(run_name=run_name):\n        if df_log is not None:\n            dataset = mlflow.data.from_pandas(df_log, targets=\"caption\")\n            mlflow.log_input(dataset, context=\"training\")\n\n        # Logging các siêu tham số\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"epoch_count\", epoch_count)\n        mlflow.log_param(\"data_part\", data_part)\n        mlflow.log_param(\"learning_rate\", lr)\n        mlflow.log_param(\"train_data_size\", len(train_dataloader.dataset))\n        mlflow.log_param(\"val_data_size\", len(val_dataloader.dataset))\n        mlflow.log_param(\"trainable_params\", trainable_params)\n        mlflow.log_param(\"total_params\", total_params)\n        mlflow.log_param(\"trainable_ratio\", round(trainable_params / total_params, 4))\n        mlflow.log_param(\"model_name\", model_name)\n\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch in train_dataloader:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n\n                encoder_outputs = model.encoder(pixel_values=pixel_values)\n                outputs = model.decoder(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_outputs.last_hidden_state,\n                    labels=input_ids\n                )\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_loss = total_loss / len(train_dataloader)\n            val_loss = evaluate_model(model, val_dataloader, device)\n\n            # Log loss theo epoch\n            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n            print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n            # Lưu mô hình sau mỗi epoch\n            epoch_save_path = f\"/kaggle/working/{model_name}_epoch_{epoch+1}\"\n            model.save_pretrained(epoch_save_path)\n            tokenizer.save_pretrained(os.path.join(epoch_save_path, \"tokenizer\"))\n            feature_extractor.save_pretrained(os.path.join(epoch_save_path, \"feature_extractor\"))\n            \n            # Log toàn bộ thư mục như một artifact\n            mlflow.log_artifacts(epoch_save_path, artifact_path=f\"epoch_{epoch+1}\")\n\n        # Lưu model cuối cùng riêng biệt\n        final_model_path = f\"/kaggle/working/{model_name}_final\"\n        model.save_pretrained(final_model_path)\n        tokenizer.save_pretrained(os.path.join(final_model_path, \"tokenizer\"))\n        feature_extractor.save_pretrained(os.path.join(final_model_path, \"feature_extractor\"))\n        mlflow.log_artifacts(final_model_path, artifact_path=\"final_model\")\n\n        # Log model dưới dạng pyfunc (nếu cần)\n        # Đảm bảo bạn đã định nghĩa class ImageCaptionModel trước\n\n        # Infer signature\n        input_example = pd.DataFrame({\"url\": [\"http://example.com/test.jpg\"]})\n        output_example = pd.DataFrame({\"caption\": [\"Mô tả ảnh\"]})\n        \n        # 2. Tạo model signature\n        signature = infer_signature(\n            input_example,\n            output_example,\n            params={\"input_types\": \"string\", \"output_types\": \"string\"}  # Chỉ định kiểu dữ liệu\n        )\n        mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=LoRACaptionWrapper(),\n            artifacts={\"model_dir\": final_model_path},  # Đường dẫn đến thư mục chứa model, tokenizer và feature_extractor\n            registered_model_name=model_name,\n            signature=signature,\n            # input_example=input_example,\n            pip_requirements=pip_reqs\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:23:19.046495Z","iopub.execute_input":"2025-04-27T10:23:19.046758Z","iopub.status.idle":"2025-04-27T10:23:19.683701Z","shell.execute_reply.started":"2025-04-27T10:23:19.046740Z","shell.execute_reply":"2025-04-27T10:23:19.682592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tạo k Subset cho train, val\ntrain_subset = Subset(train_dataset, range(train_df.shape[0]))  # Lấy train_df.shape[0] dữ liệu đầu tiên từ train_dataset\nval_subset = Subset(val_dataset, range(val_df.shape[0]))     # Lấy 4620 dữ liệu đầu tiên từ val_dataset\n\n# Tạo DataLoader cho các subset\ntrain_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\nval_dataloader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:23:19.684960Z","iopub.execute_input":"2025-04-27T10:23:19.685282Z","iopub.status.idle":"2025-04-27T10:23:19.692831Z","shell.execute_reply.started":"2025-04-27T10:23:19.685255Z","shell.execute_reply":"2025-04-27T10:23:19.691808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13. Load model pretrain","metadata":{}},{"cell_type":"code","source":"# Đường dẫn thư mục chứa các tệp mô hình\nmodel_path = '/kaggle/input/phovit-gptcap/pytorch/default/10'\n\n# Tải mô hình VisionEncoderDecoder\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path, ignore_mismatched_sizes=True)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:23:19.693746Z","iopub.execute_input":"2025-04-27T10:23:19.694025Z","iopub.status.idle":"2025-04-27T10:23:34.178478Z","shell.execute_reply.started":"2025-04-27T10:23:19.694000Z","shell.execute_reply":"2025-04-27T10:23:34.177510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 14. Huấn luyện","metadata":{}},{"cell_type":"code","source":"# Huấn luyện mô hình\ntrain_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    tokenizer,\n    feature_extractor,\n    lr=5e-5,\n    epochs=5,\n    epoch_count=15,\n    model_name='BartPho_ViT_GPT2_LoRA_ICG',\n    data_part=\"FULL\",\n    df_log=df\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:23:34.179449Z","iopub.execute_input":"2025-04-27T10:23:34.179760Z","iopub.status.idle":"2025-04-27T10:23:34.183460Z","shell.execute_reply.started":"2025-04-27T10:23:34.179733Z","shell.execute_reply":"2025-04-27T10:23:34.182819Z"}},"outputs":[],"execution_count":null}]}