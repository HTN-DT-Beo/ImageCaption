{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11445241,"sourceType":"datasetVersion","datasetId":7170182}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow pyvi minio -q\nimport mlflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:45.995869Z","iopub.execute_input":"2025-04-17T08:58:45.996129Z","iopub.status.idle":"2025-04-17T08:58:49.361067Z","shell.execute_reply.started":"2025-04-17T08:58:45.996111Z","shell.execute_reply":"2025-04-17T08:58:49.360036Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"mlflow.set_tracking_uri(\"http://36.50.135.226:7893/\")\n\n# from mlflow.tracking import MlflowClient\n\n# client = MlflowClient()\n# client.restore_experiment(experiment_id=\"2\")\n\nmlflow.set_experiment(experiment_id=\"6\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:49.362809Z","iopub.execute_input":"2025-04-17T08:58:49.363651Z","iopub.status.idle":"2025-04-17T08:58:49.752920Z","shell.execute_reply.started":"2025-04-17T08:58:49.363628Z","shell.execute_reply":"2025-04-17T08:58:49.752322Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='s3://mlflow/5', creation_time=1744189879834, experiment_id='6', last_update_time=1744189879834, lifecycle_stage='active', name='RegisterModel', tags={}>"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"import os\n\nos.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://36.50.135.226:9000\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:49.753533Z","iopub.execute_input":"2025-04-17T08:58:49.753720Z","iopub.status.idle":"2025-04-17T08:58:49.757362Z","shell.execute_reply.started":"2025-04-17T08:58:49.753706Z","shell.execute_reply":"2025-04-17T08:58:49.756703Z"}},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":"# MinIO","metadata":{}},{"cell_type":"code","source":"from minio import Minio\nfrom minio.error import S3Error\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:49.759151Z","iopub.execute_input":"2025-04-17T08:58:49.759327Z","iopub.status.idle":"2025-04-17T08:58:49.782002Z","shell.execute_reply.started":"2025-04-17T08:58:49.759313Z","shell.execute_reply":"2025-04-17T08:58:49.781409Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# def upload_to_minio(local_folder, minio_folder):\n#     MINIO_ENDPOINT = \"http://160.191.244.13:9000\"\n#     ACCESS_KEY = \"minio\"\n#     SECRET_KEY = \"minio123\"\n#     BUCKET_NAME = \"mlflow\"\n#     MINIO_FOLDER = \"model/ViT_GPT2_Vi\"\n\n\n#     # Kết nối MinIO\n#     minio_client = Minio(\n#         MINIO_ENDPOINT.replace(\"http://\", \"\").replace(\"https://\", \"\"),\n#         access_key=ACCESS_KEY,\n#         secret_key=SECRET_KEY,\n#         secure=False,\n#     )\n\n#     # Kiểm tra và tạo bucket nếu chưa tồn tại\n#     found = minio_client.bucket_exists(BUCKET_NAME)\n#     if not found:\n#         minio_client.make_bucket(BUCKET_NAME)\n#         print(f\"Bucket '{BUCKET_NAME}' được tạo thành công.\")\n\n#     # Tìm tất cả file trong thư mục local\n#     files = glob.glob(f\"{local_folder}/*\")\n#     for file_path in files:\n#         file_name = os.path.basename(file_path)\n#         minio_path = f\"{minio_folder}/{file_name}\"\n\n#         try:\n#             minio_client.fput_object(BUCKET_NAME, minio_path, file_path)\n#             print(f\"✅ Đã upload: {file_name} → {minio_path}\")\n#         except S3Error as e:\n#             print(f\"❌ Lỗi khi upload {file_name}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:49.782903Z","iopub.execute_input":"2025-04-17T08:58:49.783159Z","iopub.status.idle":"2025-04-17T08:58:49.800034Z","shell.execute_reply.started":"2025-04-17T08:58:49.783128Z","shell.execute_reply":"2025-04-17T08:58:49.799516Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nimport transformers\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor\nimport torch\nfrom PIL import Image\nimport torch.nn as nn\nimport cv2\nimport torchvision\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom collections import OrderedDict\nfrom transformers import GPT2Config, GPT2LMHeadModel\nimport mlflow\nfrom mlflow.models import infer_signature\nimport mlflow.pytorch\nimport re\nfrom pyvi import ViTokenizer\nfrom torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:49.800587Z","iopub.execute_input":"2025-04-17T08:58:49.800794Z","iopub.status.idle":"2025-04-17T08:58:49.829251Z","shell.execute_reply.started":"2025-04-17T08:58:49.800772Z","shell.execute_reply":"2025-04-17T08:58:49.828725Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:49.829984Z","iopub.execute_input":"2025-04-17T08:58:49.830262Z","iopub.status.idle":"2025-04-17T08:58:51.377347Z","shell.execute_reply.started":"2025-04-17T08:58:49.830237Z","shell.execute_reply":"2025-04-17T08:58:51.376443Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"with open(\"requirements.txt\") as f:\n    pip_reqs = [line.strip() for line in f if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:51.378406Z","iopub.execute_input":"2025-04-17T08:58:51.378722Z","iopub.status.idle":"2025-04-17T08:58:51.383824Z","shell.execute_reply.started":"2025-04-17T08:58:51.378691Z","shell.execute_reply":"2025-04-17T08:58:51.383100Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"\"\"\"\n1. Load data\n\"\"\"\n# 1. Load dữ liệu\nbucket = 0\nbucket_path = f\"/kaggle/input/traffic-image-caption-split/output/bucket_{bucket}\"\n\ndef load_data(bucket_path):  \n    train_df = pd.read_json(os.path.join(bucket_path, 'train.json'))\n    val_df = pd.read_json(os.path.join(bucket_path, 'val.json'))\n    test_df = pd.read_json(os.path.join(bucket_path, 'test.json'))\n    return train_df, val_df, test_df\n\ntrain_df, val_df, test_df = load_data(bucket_path)\n# 2. In kích thước từng file\nprint(\"Train size:\", train_df.shape)\nprint(\"Validation size:\", val_df.shape)\nprint(\"Test size:\", test_df.shape)\n\n# 3. Gộp lại thành một dataframe\ndf = pd.concat([train_df, val_df, test_df], ignore_index=True)\n\n# 4. Chọn các cột cần thiết và đổi tên\ndef rename_columns(df):\n    return df[['original_url', 'search_query', 'short_caption']].rename(columns={\n        'original_url': 'url',\n        'search_query': 'search_query',\n        'short_caption': 'caption'\n    })\ndf_new = rename_columns(df)\ntrain_df = rename_columns(train_df)\nval_df = rename_columns(val_df)\ntest_df = rename_columns(test_df)\n\n# 5. In ra 5 dòng đầu tiên\nprint(df_new.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:51.384750Z","iopub.execute_input":"2025-04-17T08:58:51.385073Z","iopub.status.idle":"2025-04-17T08:58:51.556900Z","shell.execute_reply.started":"2025-04-17T08:58:51.385049Z","shell.execute_reply":"2025-04-17T08:58:51.556289Z"}},"outputs":[{"name":"stdout","text":"Train size: (9838, 6)\nValidation size: (1252, 6)\nTest size: (1262, 6)\n                                                 url  \\\n0  https://media.vietnamplus.vn/images/dadb342ab8...   \n1  https://cdn.tuoitre.vn/zoom/700_700/4715847528...   \n2  https://media1.nguoiduatin.vn/media/pham-trong...   \n3  https://laodongthudo.vn/stores/news_dataimages...   \n4  https://binhphuoc.gov.vn/uploads/binhphuoc/stp...   \n\n                          search_query  \\\n0       nhường đường cho xe cứu thương   \n1  lối sang đường dành cho người đi bộ   \n2         nắng nóng đường phố việt nam   \n3            trật tự giao thông ngã tư   \n4               xe máy đậu trên vỉa hè   \n\n                                             caption  \n0  Giao thông tĩnh lặng, nhiều xe cứu thương đậu ...  \n1  Giao thông vắng vẻ, có người băng qua đường tạ...  \n2  Giao thông đường phố khá vắng vẻ.  Biển báo và...  \n3  Giao thông đông xe máy. Biển báo giới hạn chiề...  \n4  Tình trạng giao thông khá vắng vẻ. Biển báo ở ...  \n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"\"\"\"\n3. Load feature extractor\n\"\"\"\nfeature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n\ndef load_image(image_url):\n    try:\n        response = requests.get(image_url, timeout=10)\n        if response.status_code != 200:\n            print(f\"Error: Failed to fetch image, status code {response.status_code}\")\n            return None\n\n        image_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n\n        if image is None:\n            print(\"Error: OpenCV could not decode the image.\")\n            return None\n\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image_rgb\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: Request failed - {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:51.559074Z","iopub.execute_input":"2025-04-17T08:58:51.559271Z","iopub.status.idle":"2025-04-17T08:58:51.703589Z","shell.execute_reply.started":"2025-04-17T08:58:51.559256Z","shell.execute_reply":"2025-04-17T08:58:51.703079Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"\"\"\"\n4. Load Tokenizer\n\"\"\"\nfrom transformers import AutoTokenizer\n\n# Load tokenizer của BartPho\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n\n# Kiểm tra vocab size\nvocab_size = tokenizer.vocab_size\nprint(f\"VOCAB SIZE: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:51.704172Z","iopub.execute_input":"2025-04-17T08:58:51.704360Z","iopub.status.idle":"2025-04-17T08:58:52.154515Z","shell.execute_reply.started":"2025-04-17T08:58:51.704344Z","shell.execute_reply":"2025-04-17T08:58:52.153728Z"}},"outputs":[{"name":"stdout","text":"VOCAB SIZE: 64000\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"# In ra các special tokens\nprint(\"Special Tokens:\", tokenizer.special_tokens_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.155335Z","iopub.execute_input":"2025-04-17T08:58:52.155646Z","iopub.status.idle":"2025-04-17T08:58:52.159469Z","shell.execute_reply.started":"2025-04-17T08:58:52.155628Z","shell.execute_reply":"2025-04-17T08:58:52.158876Z"}},"outputs":[{"name":"stdout","text":"Special Tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"# # Thêm token mới nếu chưa có\n# if \"<|startoftext|>\" not in tokenizer.get_vocab():\n#     tokenizer.add_special_tokens({'bos_token': '<|startoftext|>'})\n\n# vocab_size = tokenizer.vocab_size\n# print(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.160089Z","iopub.execute_input":"2025-04-17T08:58:52.160268Z","iopub.status.idle":"2025-04-17T08:58:52.177868Z","shell.execute_reply.started":"2025-04-17T08:58:52.160255Z","shell.execute_reply":"2025-04-17T08:58:52.177353Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"\"\"\"\n5. Các hàm tiền xử lý caption\n\"\"\"\ndef clean_text(text: str) -> str:\n    return re.sub(r\"[^\\w\\s,!?.]\", \"\", text).strip()\n\ndef to_lowercase(text: str) -> str:\n    return text.lower()\n\ndef join_vietnamese_compounds(text: str) -> str:\n    return ViTokenizer.tokenize(text)\n\ndef caption_preprocess(text: str) -> str:\n    text = clean_text(text)\n    text = to_lowercase(text)\n    text = join_vietnamese_compounds(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.178495Z","iopub.execute_input":"2025-04-17T08:58:52.178734Z","iopub.status.idle":"2025-04-17T08:58:52.204519Z","shell.execute_reply.started":"2025-04-17T08:58:52.178711Z","shell.execute_reply":"2025-04-17T08:58:52.204004Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"text = caption_preprocess(\"Kiểm tra phân tách từ\")\n\n# Tách token\ntokens = tokenizer.tokenize(text)\n\n# Chuyển token thành ID\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\ntext = tokenizer.decode(token_ids, skip_special_tokens=True)\n# In kết quả\nprint(\"List Word (Tokenized):\", tokens)\nprint(\"List Token ID:\", token_ids)\nprint(\"Text:\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.205308Z","iopub.execute_input":"2025-04-17T08:58:52.205552Z","iopub.status.idle":"2025-04-17T08:58:52.225596Z","shell.execute_reply.started":"2025-04-17T08:58:52.205531Z","shell.execute_reply":"2025-04-17T08:58:52.225065Z"}},"outputs":[{"name":"stdout","text":"List Word (Tokenized): ['kiểm_tra', 'phân_tách', 'từ']\nList Token ID: [342, 31166, 39]\nText: kiểm_tra phân_tách từ\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"\"\"\"\n6. Tiền xử lý input model\n\"\"\"\ndef process_data(image_url, caption):\n    try:\n        img_array = load_image(image_url)\n        if img_array is None:\n            return None\n        \n        pixel_values = feature_extractor(img_array, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        caption = caption_preprocess(caption)\n        tokenized_caption = tokenizer(caption, padding=\"max_length\", max_length=315, truncation=True)\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": torch.tensor(tokenized_caption[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(tokenized_caption[\"attention_mask\"])\n        }\n    except Exception as e:\n        print(f\"Error processing data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.226298Z","iopub.execute_input":"2025-04-17T08:58:52.226528Z","iopub.status.idle":"2025-04-17T08:58:52.247239Z","shell.execute_reply.started":"2025-04-17T08:58:52.226509Z","shell.execute_reply":"2025-04-17T08:58:52.246747Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"\"\"\"\n7. Tạo tập dữ liệu huấn luyện\n\"\"\"\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions):\n        self.image_paths = image_paths\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        data = process_data(self.image_paths[idx], self.captions[idx])\n        if data is None:\n            return self.__getitem__((idx + 1) % len(self))\n        return data\n\ndef custom_collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    return {\n        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch])\n    }\n\ntrain_dataset = ImageCaptionDataset(train_df[\"url\"], train_df[\"caption\"])\nval_dataset = ImageCaptionDataset(val_df[\"url\"], val_df[\"caption\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.247917Z","iopub.execute_input":"2025-04-17T08:58:52.248132Z","iopub.status.idle":"2025-04-17T08:58:52.272246Z","shell.execute_reply.started":"2025-04-17T08:58:52.248111Z","shell.execute_reply":"2025-04-17T08:58:52.271573Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"\"\"\"\n8. Cấu hình LoRA\n\"\"\"\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom peft import PeftModel\n\n# Cấu hình LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],  # Các lớp trong GPT2\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.273006Z","iopub.execute_input":"2025-04-17T08:58:52.273203Z","iopub.status.idle":"2025-04-17T08:58:52.295568Z","shell.execute_reply.started":"2025-04-17T08:58:52.273189Z","shell.execute_reply":"2025-04-17T08:58:52.295029Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"\"\"\"\n9. Định nghĩa mô hình\n\"\"\"\nvit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\ndel vit_model.classifier\nvit_model.vit.pooler = torch.nn.Sequential(OrderedDict([\n    ('dense', torch.nn.Linear(in_features=768, out_features=768, bias=True)),\n    ('activation', torch.nn.Tanh())\n]))\n\nconfig = GPT2Config.from_pretrained(\"gpt2\")\nconfig.add_cross_attention = True\nconfig.vocab_size = vocab_size\n\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config, ignore_mismatched_sizes=True)\ngpt2_model.resize_token_embeddings(config.vocab_size)\n\n# Áp dụng LoRA vào GPT2\ngpt2_model = get_peft_model(gpt2_model, lora_config)\ngpt2_model.print_trainable_parameters()  # kiểm tra số lượng tham số cần huấn luyện\n\nmodel = VisionEncoderDecoderModel(encoder=vit_model.vit, decoder=gpt2_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:52.296347Z","iopub.execute_input":"2025-04-17T08:58:52.296592Z","iopub.status.idle":"2025-04-17T08:58:53.762524Z","shell.execute_reply.started":"2025-04-17T08:58:52.296571Z","shell.execute_reply":"2025-04-17T08:58:53.761777Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized because the shapes did not match:\n- transformer.wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([64000, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,179,648 || all params: 164,540,928 || trainable%: 0.7169\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"class LoRACaptionWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    def load_context(self, context):\n        try:\n            # Đường dẫn đầy đủ đến các thư mục con trong artifact\n            model_path = context.artifacts[\"model_dir\"]\n            tokenizer_path = os.path.join(model_path, \"tokenizer\")\n            feature_extractor_path = os.path.join(model_path, \"feature_extractor\")\n\n            \n            # Load các thành phần với đường dẫn chính xác\n            self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            self.feature_extractor = ViTImageProcessor.from_pretrained(feature_extractor_path)\n            \n            # Đặt model vào chế độ eval\n            self.model.eval()\n        except Exception as e:\n            raise ValueError(f\"Error loading model: {str(e)}\")\n\n    def predict(self, context, model_input):\n        try:\n            # Xử lý đầu vào\n            if isinstance(model_input, dict):\n                image_url = model_input[\"url\"][0] if \"url\" in model_input else model_input[\"image_path\"][0]\n            else:\n                image_url = model_input.iloc[0][\"url\"] if \"url\" in model_input.columns else model_input.iloc[0][\"image_path\"]\n            \n            image = self.load_image(image_url)\n            if image is None:\n                return [\"\"]\n                \n            # Tiền xử lý ảnh và tạo caption\n            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n            \n            output_ids = self.model.generate(\n                pixel_values,\n                max_length=45,\n                min_length=20,\n                num_beams=4,\n                do_sample=True,\n                temperature=0.8,\n                top_k=20,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                repetition_penalty=2.0,\n                early_stopping=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                decoder_start_token_id=self.tokenizer.bos_token_id\n            )\n            \n            caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            return [caption.replace(\"_\", \" \").strip()]\n            \n        except Exception as e:\n            print(f\"[ERROR] during prediction: {e}\")\n            return [\"\"]\n   \n    def load_image(self, image_url):\n        try:\n            import requests\n            import numpy as np\n            from PIL import Image\n            from io import BytesIO\n            \n            response = requests.get(image_url, timeout=10)\n            if response.status_code != 200:\n                return None\n                \n            image = Image.open(BytesIO(response.content))\n            return image.convert(\"RGB\") if image else None\n            \n        except Exception as e:\n            print(f\"[ERROR] loading image: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:53.763393Z","iopub.execute_input":"2025-04-17T08:58:53.763659Z","iopub.status.idle":"2025-04-17T08:58:53.773468Z","shell.execute_reply.started":"2025-04-17T08:58:53.763636Z","shell.execute_reply":"2025-04-17T08:58:53.772850Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n  color_warning(\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"def log_datasets_to_mlflow():\n    # Log vào MLflow dưới dạng bảng (MLflow 2.4+)\n    mlflow.log_table(train_df, \"datasets/train.json\")\n    mlflow.log_table(val_df, \"datasets/val.json\")\n    mlflow.log_table(test_df, \"datasets/test.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:53.774204Z","iopub.execute_input":"2025-04-17T08:58:53.774446Z","iopub.status.idle":"2025-04-17T08:58:53.798893Z","shell.execute_reply.started":"2025-04-17T08:58:53.774422Z","shell.execute_reply":"2025-04-17T08:58:53.798366Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"\"\"\"\n10. Huấn luyện mô hình\n\"\"\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ndef evaluate_model(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            encoder_outputs = model.encoder(pixel_values=pixel_values)\n            outputs = model.decoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                encoder_hidden_states=encoder_outputs.last_hidden_state,\n                labels=input_ids\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n    return total_loss / len(val_dataloader)\n\ndef train_model(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr=5e-5, epochs=1, epoch_count=1, model_name='BartPho_ViT_GPT2_LoRA_ICG', df_log=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Optimizer chỉ update các tham số có requires_grad=True (LoRA adapter)\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Thống kê tham số trainable\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n\n    with mlflow.start_run(run_name=model_name):\n        if df_log is not None:\n            dataset = mlflow.data.from_pandas(df_log, targets=\"caption\")\n            mlflow.log_input(dataset, context=\"training\")\n\n        # Logging các siêu tham số\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"epoch_count\", epoch_count)\n        mlflow.log_param(\"learning_rate\", lr)\n        mlflow.log_param(\"train_data_size\", len(train_dataloader.dataset))\n        mlflow.log_param(\"val_data_size\", len(val_dataloader.dataset))\n        mlflow.log_param(\"trainable_params\", trainable_params)\n        mlflow.log_param(\"total_params\", total_params)\n        mlflow.log_param(\"trainable_ratio\", round(trainable_params / total_params, 4))\n        mlflow.log_param(\"model_name\", model_name)\n\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch in train_dataloader:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n\n                encoder_outputs = model.encoder(pixel_values=pixel_values)\n                outputs = model.decoder(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_outputs.last_hidden_state,\n                    labels=input_ids\n                )\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_loss = total_loss / len(train_dataloader)\n            val_loss = evaluate_model(model, val_dataloader, device)\n\n            # Log loss theo epoch\n            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n            print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n            # Lưu mô hình sau mỗi epoch\n            epoch_save_path = f\"/kaggle/working/{model_name}_epoch_{epoch+1}\"\n            model.save_pretrained(epoch_save_path)\n            tokenizer.save_pretrained(os.path.join(epoch_save_path, \"tokenizer\"))\n            feature_extractor.save_pretrained(os.path.join(epoch_save_path, \"feature_extractor\"))\n            \n            # Log toàn bộ thư mục như một artifact\n            mlflow.log_artifacts(epoch_save_path, artifact_path=f\"epoch_{epoch+1}\")\n\n        # Lưu model cuối cùng riêng biệt\n        final_model_path = f\"/kaggle/working/{model_name}_final\"\n        model.save_pretrained(final_model_path)\n        tokenizer.save_pretrained(os.path.join(final_model_path, \"tokenizer\"))\n        feature_extractor.save_pretrained(os.path.join(final_model_path, \"feature_extractor\"))\n        mlflow.log_artifacts(final_model_path, artifact_path=\"final_model\")\n\n        # Log model dưới dạng pyfunc (nếu cần)\n        # Đảm bảo bạn đã định nghĩa class ImageCaptionModel trước\n\n        # Infer signature\n        input_example = pd.DataFrame({\"url\": [\"http://example.com/test.jpg\"]})\n        output_example = pd.DataFrame({\"caption\": [\"Mô tả ảnh\"]})\n        \n        # 2. Tạo model signature\n        signature = infer_signature(\n            input_example,\n            output_example,\n            params={\"input_types\": \"string\", \"output_types\": \"string\"}  # Chỉ định kiểu dữ liệu\n        )\n        mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=LoRACaptionWrapper(),\n            artifacts={\"model_dir\": final_model_path},  # Đường dẫn đến thư mục chứa model, tokenizer và feature_extractor\n            registered_model_name=model_name,\n            signature=signature,\n            # input_example=input_example,\n            pip_requirements=pip_reqs\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:58:53.799573Z","iopub.execute_input":"2025-04-17T08:58:53.799773Z","iopub.status.idle":"2025-04-17T08:58:54.216085Z","shell.execute_reply.started":"2025-04-17T08:58:53.799748Z","shell.execute_reply":"2025-04-17T08:58:54.215280Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Tạo k Subset cho train, val\ntrain_subset = Subset(train_dataset, range(train_df.shape[0]))  # train_df.shape[0]\nval_subset = Subset(val_dataset, range(val_df.shape[0]))\n\n# Tạo DataLoader cho các subset\ntrain_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\nval_dataloader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:04:06.217849Z","iopub.execute_input":"2025-04-17T09:04:06.218429Z","iopub.status.idle":"2025-04-17T09:04:06.222435Z","shell.execute_reply.started":"2025-04-17T09:04:06.218409Z","shell.execute_reply":"2025-04-17T09:04:06.221879Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"# \"\"\"\n# Load model cũ\n# \"\"\"\n# # Đường dẫn thư mục chứa các tệp mô hình\n# model_path = '/kaggle/input/bartpho_vit_gpt2_vi/pytorch/default/5'\n\n# # Tải mô hình VisionEncoderDecoder\n# model = VisionEncoderDecoderModel.from_pretrained(model_path, ignore_mismatched_sizes=True)\n\n# model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:04:06.460655Z","iopub.execute_input":"2025-04-17T09:04:06.460889Z","iopub.status.idle":"2025-04-17T09:04:06.464353Z","shell.execute_reply.started":"2025-04-17T09:04:06.460868Z","shell.execute_reply":"2025-04-17T09:04:06.463591Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# Huấn luyện mô hình\ntrain_model(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr=5e-5, epochs=1, epoch_count=1, model_name='BartPho_ViT_GPT2_LoRA_ICG', df_log=df_new)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:12:28.957755Z","iopub.execute_input":"2025-04-17T09:12:28.958042Z","iopub.status.idle":"2025-04-17T09:18:18.755348Z","shell.execute_reply.started":"2025-04-17T09:12:28.958021Z","shell.execute_reply":"2025-04-17T09:18:18.754798Z"}},"outputs":[{"name":"stdout","text":"Error: OpenCV could not decode the image.\n[Epoch 1/1] Train Loss: 15.1145 | Val Loss: 13.9766\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/__init__.py:3212: UserWarning: \u001b[1;33mAn input example was not provided when logging the model. To ensure the model signature functions correctly, specify the `input_example` parameter. See https://mlflow.org/docs/latest/model/signatures.html#model-input-example for more details about the benefits of using input_example.\u001b[0m\n  color_warning(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc9597a491b4d9e9faac0fe4e02400f"}},"metadata":{}},{"name":"stderr","text":"Successfully registered model 'BartPho_ViT_GPT2_LoRA_ICG'.\n2025/04/17 09:18:17 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: BartPho_ViT_GPT2_LoRA_ICG, version 1\nCreated version '1' of model 'BartPho_ViT_GPT2_LoRA_ICG'.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run BartPho_ViT_GPT2_LoRA_ICG at: http://36.50.135.226:7893/#/experiments/6/runs/c2da55b1378c42c0b95470c1c4ac3aab\n🧪 View experiment at: http://36.50.135.226:7893/#/experiments/6\n","output_type":"stream"}],"execution_count":108},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"# import mlflow\n# import requests\n# from PIL import Image\n# from io import BytesIO\n\n# # 1. Tải model từ MLflow\n# model_uri = \"models:/BartPho_ViT_GPT2_LoRA_ICG/1\"\n# model = mlflow.pyfunc.load_model(model_uri)\n# # print(model.metadata)\n\n# # 2. Chuẩn bị input theo đúng signature\n# image_url = \"https://hunghoangphat.vn/upload/images/NHÀ%20THÉP%20TIỀN%20CHẾ/nha-thep-tien-che-bai-do-xe%20(3).png\"\n\n# # 3. Gọi prediction ĐÚNG CÁCH\n# result = model.predict({\"url\": image_url})  # Phải truyền dictionary với key 'url'\n\n# # 4. Xử lý kết quả (output là dictionary với key 'caption')\n# print(\"Generated caption:\", result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:19:04.948049Z","iopub.execute_input":"2025-04-17T09:19:04.948563Z","iopub.status.idle":"2025-04-17T09:20:31.707996Z","shell.execute_reply.started":"2025-04-17T09:19:04.948542Z","shell.execute_reply":"2025-04-17T09:20:31.707176Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79394f8fe9c44dbbb156a2f11dde1a37"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at /tmp/tmpserasi80/artifacts/BartPho_ViT_GPT2_LoRA_ICG_final were not used when initializing VisionEncoderDecoderModel: ['decoder.base_model.model.transformer.h.0.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.0.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.0.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.0.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.0.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.0.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.0.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.0.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.0.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.0.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.0.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.0.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.0.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.0.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.0.ln_1.bias', 'decoder.base_model.model.transformer.h.0.ln_1.weight', 'decoder.base_model.model.transformer.h.0.ln_2.bias', 'decoder.base_model.model.transformer.h.0.ln_2.weight', 'decoder.base_model.model.transformer.h.0.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.0.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.0.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.0.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.0.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.0.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.1.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.1.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.1.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.1.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.1.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.1.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.1.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.1.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.1.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.1.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.1.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.1.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.1.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.1.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.1.ln_1.bias', 'decoder.base_model.model.transformer.h.1.ln_1.weight', 'decoder.base_model.model.transformer.h.1.ln_2.bias', 'decoder.base_model.model.transformer.h.1.ln_2.weight', 'decoder.base_model.model.transformer.h.1.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.1.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.1.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.1.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.1.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.1.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.10.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.10.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.10.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.10.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.10.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.10.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.10.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.10.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.10.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.10.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.10.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.10.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.10.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.10.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.10.ln_1.bias', 'decoder.base_model.model.transformer.h.10.ln_1.weight', 'decoder.base_model.model.transformer.h.10.ln_2.bias', 'decoder.base_model.model.transformer.h.10.ln_2.weight', 'decoder.base_model.model.transformer.h.10.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.10.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.10.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.10.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.10.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.10.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.11.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.11.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.11.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.11.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.11.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.11.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.11.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.11.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.11.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.11.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.11.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.11.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.11.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.11.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.11.ln_1.bias', 'decoder.base_model.model.transformer.h.11.ln_1.weight', 'decoder.base_model.model.transformer.h.11.ln_2.bias', 'decoder.base_model.model.transformer.h.11.ln_2.weight', 'decoder.base_model.model.transformer.h.11.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.11.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.11.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.11.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.11.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.11.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.2.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.2.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.2.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.2.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.2.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.2.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.2.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.2.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.2.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.2.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.2.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.2.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.2.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.2.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.2.ln_1.bias', 'decoder.base_model.model.transformer.h.2.ln_1.weight', 'decoder.base_model.model.transformer.h.2.ln_2.bias', 'decoder.base_model.model.transformer.h.2.ln_2.weight', 'decoder.base_model.model.transformer.h.2.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.2.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.2.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.2.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.2.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.2.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.3.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.3.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.3.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.3.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.3.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.3.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.3.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.3.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.3.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.3.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.3.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.3.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.3.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.3.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.3.ln_1.bias', 'decoder.base_model.model.transformer.h.3.ln_1.weight', 'decoder.base_model.model.transformer.h.3.ln_2.bias', 'decoder.base_model.model.transformer.h.3.ln_2.weight', 'decoder.base_model.model.transformer.h.3.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.3.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.3.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.3.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.3.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.3.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.4.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.4.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.4.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.4.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.4.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.4.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.4.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.4.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.4.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.4.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.4.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.4.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.4.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.4.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.4.ln_1.bias', 'decoder.base_model.model.transformer.h.4.ln_1.weight', 'decoder.base_model.model.transformer.h.4.ln_2.bias', 'decoder.base_model.model.transformer.h.4.ln_2.weight', 'decoder.base_model.model.transformer.h.4.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.4.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.4.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.4.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.4.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.4.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.5.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.5.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.5.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.5.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.5.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.5.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.5.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.5.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.5.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.5.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.5.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.5.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.5.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.5.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.5.ln_1.bias', 'decoder.base_model.model.transformer.h.5.ln_1.weight', 'decoder.base_model.model.transformer.h.5.ln_2.bias', 'decoder.base_model.model.transformer.h.5.ln_2.weight', 'decoder.base_model.model.transformer.h.5.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.5.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.5.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.5.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.5.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.5.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.6.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.6.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.6.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.6.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.6.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.6.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.6.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.6.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.6.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.6.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.6.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.6.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.6.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.6.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.6.ln_1.bias', 'decoder.base_model.model.transformer.h.6.ln_1.weight', 'decoder.base_model.model.transformer.h.6.ln_2.bias', 'decoder.base_model.model.transformer.h.6.ln_2.weight', 'decoder.base_model.model.transformer.h.6.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.6.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.6.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.6.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.6.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.6.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.7.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.7.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.7.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.7.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.7.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.7.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.7.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.7.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.7.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.7.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.7.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.7.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.7.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.7.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.7.ln_1.bias', 'decoder.base_model.model.transformer.h.7.ln_1.weight', 'decoder.base_model.model.transformer.h.7.ln_2.bias', 'decoder.base_model.model.transformer.h.7.ln_2.weight', 'decoder.base_model.model.transformer.h.7.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.7.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.7.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.7.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.7.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.7.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.8.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.8.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.8.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.8.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.8.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.8.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.8.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.8.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.8.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.8.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.8.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.8.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.8.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.8.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.8.ln_1.bias', 'decoder.base_model.model.transformer.h.8.ln_1.weight', 'decoder.base_model.model.transformer.h.8.ln_2.bias', 'decoder.base_model.model.transformer.h.8.ln_2.weight', 'decoder.base_model.model.transformer.h.8.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.8.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.8.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.8.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.8.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.8.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.9.attn.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.9.attn.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.9.attn.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.9.attn.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.9.crossattention.c_attn.base_layer.bias', 'decoder.base_model.model.transformer.h.9.crossattention.c_attn.base_layer.weight', 'decoder.base_model.model.transformer.h.9.crossattention.c_attn.lora_A.default.weight', 'decoder.base_model.model.transformer.h.9.crossattention.c_attn.lora_B.default.weight', 'decoder.base_model.model.transformer.h.9.crossattention.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.9.crossattention.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.9.crossattention.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.9.crossattention.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.h.9.crossattention.q_attn.bias', 'decoder.base_model.model.transformer.h.9.crossattention.q_attn.weight', 'decoder.base_model.model.transformer.h.9.ln_1.bias', 'decoder.base_model.model.transformer.h.9.ln_1.weight', 'decoder.base_model.model.transformer.h.9.ln_2.bias', 'decoder.base_model.model.transformer.h.9.ln_2.weight', 'decoder.base_model.model.transformer.h.9.ln_cross_attn.bias', 'decoder.base_model.model.transformer.h.9.ln_cross_attn.weight', 'decoder.base_model.model.transformer.h.9.mlp.c_fc.bias', 'decoder.base_model.model.transformer.h.9.mlp.c_fc.weight', 'decoder.base_model.model.transformer.h.9.mlp.c_proj.base_layer.bias', 'decoder.base_model.model.transformer.h.9.mlp.c_proj.base_layer.weight', 'decoder.base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight', 'decoder.base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight', 'decoder.base_model.model.transformer.ln_f.bias', 'decoder.base_model.model.transformer.ln_f.weight', 'decoder.base_model.model.transformer.wpe.weight', 'decoder.base_model.model.transformer.wte.weight']\n- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at /tmp/tmpserasi80/artifacts/BartPho_ViT_GPT2_LoRA_ICG_final and are newly initialized: ['decoder.lm_head.weight', 'decoder.transformer.h.0.attn.c_attn.bias', 'decoder.transformer.h.0.attn.c_attn.weight', 'decoder.transformer.h.0.attn.c_proj.bias', 'decoder.transformer.h.0.attn.c_proj.weight', 'decoder.transformer.h.0.crossattention.c_attn.bias', 'decoder.transformer.h.0.crossattention.c_attn.weight', 'decoder.transformer.h.0.crossattention.c_proj.bias', 'decoder.transformer.h.0.crossattention.c_proj.weight', 'decoder.transformer.h.0.crossattention.q_attn.bias', 'decoder.transformer.h.0.crossattention.q_attn.weight', 'decoder.transformer.h.0.ln_1.bias', 'decoder.transformer.h.0.ln_1.weight', 'decoder.transformer.h.0.ln_2.bias', 'decoder.transformer.h.0.ln_2.weight', 'decoder.transformer.h.0.ln_cross_attn.bias', 'decoder.transformer.h.0.ln_cross_attn.weight', 'decoder.transformer.h.0.mlp.c_fc.bias', 'decoder.transformer.h.0.mlp.c_fc.weight', 'decoder.transformer.h.0.mlp.c_proj.bias', 'decoder.transformer.h.0.mlp.c_proj.weight', 'decoder.transformer.h.1.attn.c_attn.bias', 'decoder.transformer.h.1.attn.c_attn.weight', 'decoder.transformer.h.1.attn.c_proj.bias', 'decoder.transformer.h.1.attn.c_proj.weight', 'decoder.transformer.h.1.crossattention.c_attn.bias', 'decoder.transformer.h.1.crossattention.c_attn.weight', 'decoder.transformer.h.1.crossattention.c_proj.bias', 'decoder.transformer.h.1.crossattention.c_proj.weight', 'decoder.transformer.h.1.crossattention.q_attn.bias', 'decoder.transformer.h.1.crossattention.q_attn.weight', 'decoder.transformer.h.1.ln_1.bias', 'decoder.transformer.h.1.ln_1.weight', 'decoder.transformer.h.1.ln_2.bias', 'decoder.transformer.h.1.ln_2.weight', 'decoder.transformer.h.1.ln_cross_attn.bias', 'decoder.transformer.h.1.ln_cross_attn.weight', 'decoder.transformer.h.1.mlp.c_fc.bias', 'decoder.transformer.h.1.mlp.c_fc.weight', 'decoder.transformer.h.1.mlp.c_proj.bias', 'decoder.transformer.h.1.mlp.c_proj.weight', 'decoder.transformer.h.10.attn.c_attn.bias', 'decoder.transformer.h.10.attn.c_attn.weight', 'decoder.transformer.h.10.attn.c_proj.bias', 'decoder.transformer.h.10.attn.c_proj.weight', 'decoder.transformer.h.10.crossattention.c_attn.bias', 'decoder.transformer.h.10.crossattention.c_attn.weight', 'decoder.transformer.h.10.crossattention.c_proj.bias', 'decoder.transformer.h.10.crossattention.c_proj.weight', 'decoder.transformer.h.10.crossattention.q_attn.bias', 'decoder.transformer.h.10.crossattention.q_attn.weight', 'decoder.transformer.h.10.ln_1.bias', 'decoder.transformer.h.10.ln_1.weight', 'decoder.transformer.h.10.ln_2.bias', 'decoder.transformer.h.10.ln_2.weight', 'decoder.transformer.h.10.ln_cross_attn.bias', 'decoder.transformer.h.10.ln_cross_attn.weight', 'decoder.transformer.h.10.mlp.c_fc.bias', 'decoder.transformer.h.10.mlp.c_fc.weight', 'decoder.transformer.h.10.mlp.c_proj.bias', 'decoder.transformer.h.10.mlp.c_proj.weight', 'decoder.transformer.h.11.attn.c_attn.bias', 'decoder.transformer.h.11.attn.c_attn.weight', 'decoder.transformer.h.11.attn.c_proj.bias', 'decoder.transformer.h.11.attn.c_proj.weight', 'decoder.transformer.h.11.crossattention.c_attn.bias', 'decoder.transformer.h.11.crossattention.c_attn.weight', 'decoder.transformer.h.11.crossattention.c_proj.bias', 'decoder.transformer.h.11.crossattention.c_proj.weight', 'decoder.transformer.h.11.crossattention.q_attn.bias', 'decoder.transformer.h.11.crossattention.q_attn.weight', 'decoder.transformer.h.11.ln_1.bias', 'decoder.transformer.h.11.ln_1.weight', 'decoder.transformer.h.11.ln_2.bias', 'decoder.transformer.h.11.ln_2.weight', 'decoder.transformer.h.11.ln_cross_attn.bias', 'decoder.transformer.h.11.ln_cross_attn.weight', 'decoder.transformer.h.11.mlp.c_fc.bias', 'decoder.transformer.h.11.mlp.c_fc.weight', 'decoder.transformer.h.11.mlp.c_proj.bias', 'decoder.transformer.h.11.mlp.c_proj.weight', 'decoder.transformer.h.2.attn.c_attn.bias', 'decoder.transformer.h.2.attn.c_attn.weight', 'decoder.transformer.h.2.attn.c_proj.bias', 'decoder.transformer.h.2.attn.c_proj.weight', 'decoder.transformer.h.2.crossattention.c_attn.bias', 'decoder.transformer.h.2.crossattention.c_attn.weight', 'decoder.transformer.h.2.crossattention.c_proj.bias', 'decoder.transformer.h.2.crossattention.c_proj.weight', 'decoder.transformer.h.2.crossattention.q_attn.bias', 'decoder.transformer.h.2.crossattention.q_attn.weight', 'decoder.transformer.h.2.ln_1.bias', 'decoder.transformer.h.2.ln_1.weight', 'decoder.transformer.h.2.ln_2.bias', 'decoder.transformer.h.2.ln_2.weight', 'decoder.transformer.h.2.ln_cross_attn.bias', 'decoder.transformer.h.2.ln_cross_attn.weight', 'decoder.transformer.h.2.mlp.c_fc.bias', 'decoder.transformer.h.2.mlp.c_fc.weight', 'decoder.transformer.h.2.mlp.c_proj.bias', 'decoder.transformer.h.2.mlp.c_proj.weight', 'decoder.transformer.h.3.attn.c_attn.bias', 'decoder.transformer.h.3.attn.c_attn.weight', 'decoder.transformer.h.3.attn.c_proj.bias', 'decoder.transformer.h.3.attn.c_proj.weight', 'decoder.transformer.h.3.crossattention.c_attn.bias', 'decoder.transformer.h.3.crossattention.c_attn.weight', 'decoder.transformer.h.3.crossattention.c_proj.bias', 'decoder.transformer.h.3.crossattention.c_proj.weight', 'decoder.transformer.h.3.crossattention.q_attn.bias', 'decoder.transformer.h.3.crossattention.q_attn.weight', 'decoder.transformer.h.3.ln_1.bias', 'decoder.transformer.h.3.ln_1.weight', 'decoder.transformer.h.3.ln_2.bias', 'decoder.transformer.h.3.ln_2.weight', 'decoder.transformer.h.3.ln_cross_attn.bias', 'decoder.transformer.h.3.ln_cross_attn.weight', 'decoder.transformer.h.3.mlp.c_fc.bias', 'decoder.transformer.h.3.mlp.c_fc.weight', 'decoder.transformer.h.3.mlp.c_proj.bias', 'decoder.transformer.h.3.mlp.c_proj.weight', 'decoder.transformer.h.4.attn.c_attn.bias', 'decoder.transformer.h.4.attn.c_attn.weight', 'decoder.transformer.h.4.attn.c_proj.bias', 'decoder.transformer.h.4.attn.c_proj.weight', 'decoder.transformer.h.4.crossattention.c_attn.bias', 'decoder.transformer.h.4.crossattention.c_attn.weight', 'decoder.transformer.h.4.crossattention.c_proj.bias', 'decoder.transformer.h.4.crossattention.c_proj.weight', 'decoder.transformer.h.4.crossattention.q_attn.bias', 'decoder.transformer.h.4.crossattention.q_attn.weight', 'decoder.transformer.h.4.ln_1.bias', 'decoder.transformer.h.4.ln_1.weight', 'decoder.transformer.h.4.ln_2.bias', 'decoder.transformer.h.4.ln_2.weight', 'decoder.transformer.h.4.ln_cross_attn.bias', 'decoder.transformer.h.4.ln_cross_attn.weight', 'decoder.transformer.h.4.mlp.c_fc.bias', 'decoder.transformer.h.4.mlp.c_fc.weight', 'decoder.transformer.h.4.mlp.c_proj.bias', 'decoder.transformer.h.4.mlp.c_proj.weight', 'decoder.transformer.h.5.attn.c_attn.bias', 'decoder.transformer.h.5.attn.c_attn.weight', 'decoder.transformer.h.5.attn.c_proj.bias', 'decoder.transformer.h.5.attn.c_proj.weight', 'decoder.transformer.h.5.crossattention.c_attn.bias', 'decoder.transformer.h.5.crossattention.c_attn.weight', 'decoder.transformer.h.5.crossattention.c_proj.bias', 'decoder.transformer.h.5.crossattention.c_proj.weight', 'decoder.transformer.h.5.crossattention.q_attn.bias', 'decoder.transformer.h.5.crossattention.q_attn.weight', 'decoder.transformer.h.5.ln_1.bias', 'decoder.transformer.h.5.ln_1.weight', 'decoder.transformer.h.5.ln_2.bias', 'decoder.transformer.h.5.ln_2.weight', 'decoder.transformer.h.5.ln_cross_attn.bias', 'decoder.transformer.h.5.ln_cross_attn.weight', 'decoder.transformer.h.5.mlp.c_fc.bias', 'decoder.transformer.h.5.mlp.c_fc.weight', 'decoder.transformer.h.5.mlp.c_proj.bias', 'decoder.transformer.h.5.mlp.c_proj.weight', 'decoder.transformer.h.6.attn.c_attn.bias', 'decoder.transformer.h.6.attn.c_attn.weight', 'decoder.transformer.h.6.attn.c_proj.bias', 'decoder.transformer.h.6.attn.c_proj.weight', 'decoder.transformer.h.6.crossattention.c_attn.bias', 'decoder.transformer.h.6.crossattention.c_attn.weight', 'decoder.transformer.h.6.crossattention.c_proj.bias', 'decoder.transformer.h.6.crossattention.c_proj.weight', 'decoder.transformer.h.6.crossattention.q_attn.bias', 'decoder.transformer.h.6.crossattention.q_attn.weight', 'decoder.transformer.h.6.ln_1.bias', 'decoder.transformer.h.6.ln_1.weight', 'decoder.transformer.h.6.ln_2.bias', 'decoder.transformer.h.6.ln_2.weight', 'decoder.transformer.h.6.ln_cross_attn.bias', 'decoder.transformer.h.6.ln_cross_attn.weight', 'decoder.transformer.h.6.mlp.c_fc.bias', 'decoder.transformer.h.6.mlp.c_fc.weight', 'decoder.transformer.h.6.mlp.c_proj.bias', 'decoder.transformer.h.6.mlp.c_proj.weight', 'decoder.transformer.h.7.attn.c_attn.bias', 'decoder.transformer.h.7.attn.c_attn.weight', 'decoder.transformer.h.7.attn.c_proj.bias', 'decoder.transformer.h.7.attn.c_proj.weight', 'decoder.transformer.h.7.crossattention.c_attn.bias', 'decoder.transformer.h.7.crossattention.c_attn.weight', 'decoder.transformer.h.7.crossattention.c_proj.bias', 'decoder.transformer.h.7.crossattention.c_proj.weight', 'decoder.transformer.h.7.crossattention.q_attn.bias', 'decoder.transformer.h.7.crossattention.q_attn.weight', 'decoder.transformer.h.7.ln_1.bias', 'decoder.transformer.h.7.ln_1.weight', 'decoder.transformer.h.7.ln_2.bias', 'decoder.transformer.h.7.ln_2.weight', 'decoder.transformer.h.7.ln_cross_attn.bias', 'decoder.transformer.h.7.ln_cross_attn.weight', 'decoder.transformer.h.7.mlp.c_fc.bias', 'decoder.transformer.h.7.mlp.c_fc.weight', 'decoder.transformer.h.7.mlp.c_proj.bias', 'decoder.transformer.h.7.mlp.c_proj.weight', 'decoder.transformer.h.8.attn.c_attn.bias', 'decoder.transformer.h.8.attn.c_attn.weight', 'decoder.transformer.h.8.attn.c_proj.bias', 'decoder.transformer.h.8.attn.c_proj.weight', 'decoder.transformer.h.8.crossattention.c_attn.bias', 'decoder.transformer.h.8.crossattention.c_attn.weight', 'decoder.transformer.h.8.crossattention.c_proj.bias', 'decoder.transformer.h.8.crossattention.c_proj.weight', 'decoder.transformer.h.8.crossattention.q_attn.bias', 'decoder.transformer.h.8.crossattention.q_attn.weight', 'decoder.transformer.h.8.ln_1.bias', 'decoder.transformer.h.8.ln_1.weight', 'decoder.transformer.h.8.ln_2.bias', 'decoder.transformer.h.8.ln_2.weight', 'decoder.transformer.h.8.ln_cross_attn.bias', 'decoder.transformer.h.8.ln_cross_attn.weight', 'decoder.transformer.h.8.mlp.c_fc.bias', 'decoder.transformer.h.8.mlp.c_fc.weight', 'decoder.transformer.h.8.mlp.c_proj.bias', 'decoder.transformer.h.8.mlp.c_proj.weight', 'decoder.transformer.h.9.attn.c_attn.bias', 'decoder.transformer.h.9.attn.c_attn.weight', 'decoder.transformer.h.9.attn.c_proj.bias', 'decoder.transformer.h.9.attn.c_proj.weight', 'decoder.transformer.h.9.crossattention.c_attn.bias', 'decoder.transformer.h.9.crossattention.c_attn.weight', 'decoder.transformer.h.9.crossattention.c_proj.bias', 'decoder.transformer.h.9.crossattention.c_proj.weight', 'decoder.transformer.h.9.crossattention.q_attn.bias', 'decoder.transformer.h.9.crossattention.q_attn.weight', 'decoder.transformer.h.9.ln_1.bias', 'decoder.transformer.h.9.ln_1.weight', 'decoder.transformer.h.9.ln_2.bias', 'decoder.transformer.h.9.ln_2.weight', 'decoder.transformer.h.9.ln_cross_attn.bias', 'decoder.transformer.h.9.ln_cross_attn.weight', 'decoder.transformer.h.9.mlp.c_fc.bias', 'decoder.transformer.h.9.mlp.c_fc.weight', 'decoder.transformer.h.9.mlp.c_proj.bias', 'decoder.transformer.h.9.mlp.c_proj.weight', 'decoder.transformer.ln_f.bias', 'decoder.transformer.ln_f.weight', 'decoder.transformer.wpe.weight', 'decoder.transformer.wte.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2025/04/17 09:20:27 WARNING mlflow.pyfunc.model: The underlying model does not support passing additional parameters to the predict function. `params` {'input_types': 'string', 'output_types': 'string'} will be ignored.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"[ERROR] during prediction: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nGenerated caption: ['']\n","output_type":"stream"},{"name":"stderr","text":"../aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"# from mlflow.tracking import MlflowClient\n\n# client = MlflowClient()\n# versions = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n# for version in versions:\n#     client.delete_model_version(name=\"BartPho_ViT_GPT2_LoRA_ICG\", version=version)\n\n# client.delete_registered_model(name=\"BartPho_ViT_GPT2_LoRA_ICG\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:24:00.081367Z","iopub.execute_input":"2025-04-17T09:24:00.081991Z","iopub.status.idle":"2025-04-17T09:24:01.649927Z","shell.execute_reply.started":"2025-04-17T09:24:00.081965Z","shell.execute_reply":"2025-04-17T09:24:01.649388Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}