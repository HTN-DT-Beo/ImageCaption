{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11334502,"sourceType":"datasetVersion","datasetId":6723117},{"sourceId":11337142,"sourceType":"datasetVersion","datasetId":7085417,"isSourceIdPinned":true},{"sourceId":331865,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":277580,"modelId":298468},{"sourceId":374794,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":307843,"modelId":328293}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow pyvi minio -q\n!pip install hf_xet -q\n\nimport mlflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:36:23.598453Z","iopub.execute_input":"2025-05-26T15:36:23.598634Z","iopub.status.idle":"2025-05-26T15:36:47.281552Z","shell.execute_reply.started":"2025-05-26T15:36:23.598617Z","shell.execute_reply":"2025-05-26T15:36:47.280917Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m720.5/720.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"mlflow.set_tracking_uri(\"http://36.50.135.226:7893/\")\n\n# from mlflow.tracking import MlflowClient\n\n# client = MlflowClient()\n# client.restore_experiment(experiment_id=\"2\")\n\nmlflow.set_experiment(experiment_id=\"10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:36:47.283644Z","iopub.execute_input":"2025-05-26T15:36:47.284109Z","iopub.status.idle":"2025-05-26T15:36:47.756687Z","shell.execute_reply.started":"2025-05-26T15:36:47.284089Z","shell.execute_reply":"2025-05-26T15:36:47.755401Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='s3://mlflow/6', creation_time=1745743711248, experiment_id='10', last_update_time=1745743711248, lifecycle_stage='active', name='ImageCaption_TPC37k_BartPho-ViT-GPT2_LoRALayerFT', tags={}>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\n\nos.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://36.50.135.226:9000\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:36:47.757764Z","iopub.execute_input":"2025-05-26T15:36:47.758738Z","iopub.status.idle":"2025-05-26T15:36:47.763624Z","shell.execute_reply.started":"2025-05-26T15:36:47.758705Z","shell.execute_reply":"2025-05-26T15:36:47.762647Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# MinIO","metadata":{}},{"cell_type":"code","source":"from minio import Minio\nfrom minio.error import S3Error\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:36:47.764566Z","iopub.execute_input":"2025-05-26T15:36:47.765243Z","iopub.status.idle":"2025-05-26T15:36:48.288263Z","shell.execute_reply.started":"2025-05-26T15:36:47.765213Z","shell.execute_reply":"2025-05-26T15:36:48.287610Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nimport transformers\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor\nimport torch\nfrom PIL import Image\nimport torch.nn as nn\nimport cv2\nimport torchvision\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom collections import OrderedDict\nfrom transformers import GPT2Config, GPT2LMHeadModel\nimport mlflow\nfrom mlflow.models import infer_signature\nimport mlflow.pytorch\nimport re\nfrom pyvi import ViTokenizer\nfrom torch.optim import AdamW\nfrom datetime import datetime\nimport json\nimport pickle\nfrom io import BytesIO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:36:48.289121Z","iopub.execute_input":"2025-05-26T15:36:48.289321Z","iopub.status.idle":"2025-05-26T15:37:30.453045Z","shell.execute_reply.started":"2025-05-26T15:36:48.289304Z","shell.execute_reply":"2025-05-26T15:37:30.452367Z"}},"outputs":[{"name":"stderr","text":"2025-05-26 15:37:10.723213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748273831.138258      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748273831.277461      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 1. Chuáº©n bá»‹ file thÆ° viá»‡n","metadata":{}},{"cell_type":"code","source":"pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:37:30.453889Z","iopub.execute_input":"2025-05-26T15:37:30.454435Z","iopub.status.idle":"2025-05-26T15:37:32.368899Z","shell.execute_reply.started":"2025-05-26T15:37:30.454404Z","shell.execute_reply":"2025-05-26T15:37:32.367704Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"with open(\"requirements.txt\") as f:\n    pip_reqs = [line.strip() for line in f if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:37:32.372168Z","iopub.execute_input":"2025-05-26T15:37:32.372446Z","iopub.status.idle":"2025-05-26T15:37:32.377904Z","shell.execute_reply.started":"2025-05-26T15:37:32.372420Z","shell.execute_reply":"2025-05-26T15:37:32.377259Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 2. Load data","metadata":{}},{"cell_type":"markdown","source":"## API Data","metadata":{}},{"cell_type":"code","source":"# 1. Gá»­i GET request Ä‘áº¿n API metadata\nurl = \"http://36.50.135.226/api/v1/metadata/encoded-data\"\nheaders = {\"accept\": \"application/json\"}\n\nresponse = requests.get(url, headers=headers)\n\n# 2. Kiá»ƒm tra pháº£n há»“i\nif response.status_code == 200:\n    data = response.json()\n    object_keys = data.get(\"object_keys\", [])\n\n    def extract_timestamp(key):\n        match = re.search(r\"encoded_data_(\\d{14})\\.pkl\", key)\n        if match:\n            return datetime.strptime(match.group(1), \"%Y%m%d%H%M%S\")\n        return None\n\n    if object_keys:\n        base_url = \"http://160.191.244.13:9000/lakehouse/\"\n        all_pkl_data = []  # Biáº¿n chá»©a toÃ n bá»™ dá»¯ liá»‡u\n\n        for key in sorted(object_keys, key=extract_timestamp):\n            file_url = base_url + key\n            file_response = requests.get(file_url)\n\n            if file_response.status_code == 200:\n                try:\n                    file_bytes = BytesIO(file_response.content)\n                    pkl_data = pickle.load(file_bytes)\n\n                    # Gá»™p dá»¯ liá»‡u\n                    if isinstance(pkl_data, list):\n                        all_pkl_data.extend(pkl_data)\n                    elif isinstance(pkl_data, dict):\n                        all_pkl_data.append(pkl_data)\n                    else:\n                        all_pkl_data.append(pkl_data)\n\n                    print(f\"âœ… ÄÃ£ xá»­ lÃ½: {key}\")\n                except Exception as e:\n                    print(f\"âš ï¸ Lá»—i Ä‘á»c file {key}: {e}\")\n            else:\n                print(f\"âŒ KhÃ´ng thá»ƒ táº£i file: {key}\")\n        \n        # âœ… In ra tá»•ng sá»‘ pháº§n tá»­ Ä‘Ã£ gá»™p\n        print(f\"\\nğŸ“¦ Tá»•ng sá»‘ Ä‘á»‘i tÆ°á»£ng Ä‘Ã£ gá»™p: {len(all_pkl_data)}\")\n    else:\n        print(\"KhÃ´ng cÃ³ object_keys nÃ o trong dá»¯ liá»‡u.\")\nelse:\n    print(\"Lá»—i khi gá»i API:\", response.status_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:37:44.968313Z","iopub.execute_input":"2025-05-26T15:37:44.968615Z","iopub.status.idle":"2025-05-26T15:54:11.988974Z","shell.execute_reply.started":"2025-05-26T15:37:44.968584Z","shell.execute_reply":"2025-05-26T15:54:11.988159Z"}},"outputs":[{"name":"stdout","text":"âœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145536.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145620.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145657.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145708.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145719.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145731.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145744.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145755.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145809.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145821.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145832.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145846.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145859.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145913.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145925.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145938.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145952.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150005.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150016.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150027.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150039.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150051.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150104.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150115.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150130.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150143.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150156.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150209.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150223.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150236.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150250.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150305.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150319.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150331.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150343.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150356.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150408.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150419.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150431.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150443.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150456.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150510.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150522.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150536.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150549.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150602.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150617.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150630.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150643.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150655.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150708.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150719.pkl\n\nğŸ“¦ Tá»•ng sá»‘ Ä‘á»‘i tÆ°á»£ng Ä‘Ã£ gá»™p: 2576\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(len(all_pkl_data))\nprint(all_pkl_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:54:11.989753Z","iopub.execute_input":"2025-05-26T15:54:11.990012Z","iopub.status.idle":"2025-05-26T15:54:11.998143Z","shell.execute_reply.started":"2025-05-26T15:54:11.989993Z","shell.execute_reply":"2025-05-26T15:54:11.997158Z"}},"outputs":[{"name":"stdout","text":"2576\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/6e0a007b66b69f7c9aacb9b46606f0ed.jpg', 'pixel_values': tensor([[[[-0.9059, -0.7725, -0.7333,  ...,  0.0275, -0.0039, -0.0118],\n          [-0.8196, -0.6471, -0.5843,  ...,  0.0196,  0.0039, -0.0196],\n          [-0.6392, -0.6706, -0.6549,  ..., -0.0118, -0.0275, -0.0431],\n          ...,\n          [-0.2627, -0.2471, -0.2549,  ...,  0.1216,  0.2627,  0.2471],\n          [-0.2549, -0.2314, -0.2392,  ...,  0.4275,  0.4745,  0.3804],\n          [-0.2549, -0.2314, -0.2235,  ..., -0.0431, -0.0353, -0.0353]],\n\n         [[-0.8902, -0.7490, -0.7098,  ..., -0.3176, -0.3412, -0.3647],\n          [-0.8039, -0.6235, -0.5608,  ..., -0.3412, -0.3569, -0.3804],\n          [-0.6157, -0.6549, -0.6314,  ..., -0.3569, -0.3647, -0.3882],\n          ...,\n          [-0.4745, -0.4745, -0.4824,  ..., -0.0667,  0.0745,  0.0745],\n          [-0.4824, -0.4824, -0.4980,  ...,  0.2863,  0.3412,  0.2627],\n          [-0.4745, -0.4902, -0.4902,  ..., -0.2157, -0.2000, -0.2078]],\n\n         [[-0.9765, -0.8745, -0.8745,  ..., -0.7647, -0.7725, -0.7725],\n          [-0.8824, -0.7569, -0.7333,  ..., -0.7647, -0.7647, -0.7882],\n          [-0.7176, -0.7882, -0.8039,  ..., -0.7569, -0.7647, -0.7961],\n          ...,\n          [-0.6941, -0.6784, -0.6706,  ..., -0.1686,  0.0039,  0.0039],\n          [-0.6941, -0.6627, -0.6627,  ...,  0.2784,  0.3569,  0.2627],\n          [-0.6863, -0.6627, -0.6392,  ..., -0.2941, -0.2941, -0.3412]]]]), 'input_ids': tensor([[   0, 7069,  140, 1867,  109,    4,  715,  448,  330,    5,  105, 2786,\n           52,  132,    5,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import random\n\n# Giáº£ sá»­ all_pkl_data lÃ  má»™t list chá»©a toÃ n bá»™ dá»¯ liá»‡u\nprint(f\"Tá»•ng sá»‘ dá»¯ liá»‡u: {len(all_pkl_data)}\")  # 2576\n\n# 1. Shuffle Ä‘á»ƒ ngáº«u nhiÃªn hÃ³a thá»© tá»± (náº¿u cáº§n)\nrandom.shuffle(all_pkl_data)\n\n# 2. TÃ­nh sá»‘ lÆ°á»£ng pháº§n tá»­ cho táº­p train\ntrain_size = int(0.8 * len(all_pkl_data))  # 80%\n\n# 3. Chia dá»¯ liá»‡u\ntrain_pkl_data = all_pkl_data[:train_size]\ntest_pkl_data = all_pkl_data[train_size:]\n\n# 4. In kiá»ƒm tra\nprint(f\"Sá»‘ lÆ°á»£ng train: {len(train_pkl_data)}\")  # ~2060\nprint(f\"Sá»‘ lÆ°á»£ng test: {len(test_pkl_data)}\")    # ~516\n\n# âœ… In máº«u má»™t pháº§n tá»­\nprint(\"ğŸ“„ Máº«u train:\")\nprint(train_pkl_data[0])\nprint(\"ğŸ“„ Máº«u test:\")\nprint(test_pkl_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T15:59:57.425827Z","iopub.execute_input":"2025-05-26T15:59:57.426610Z","iopub.status.idle":"2025-05-26T15:59:57.441558Z","shell.execute_reply.started":"2025-05-26T15:59:57.426588Z","shell.execute_reply":"2025-05-26T15:59:57.440895Z"}},"outputs":[{"name":"stdout","text":"Tá»•ng sá»‘ dá»¯ liá»‡u: 2576\nSá»‘ lÆ°á»£ng train: 2060\nSá»‘ lÆ°á»£ng test: 516\nğŸ“„ Máº«u train:\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/d62f86bc8aa3b964bb121d956af2f057_0.jpg', 'pixel_values': tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          ...,\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.]],\n\n         [[-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          ...,\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.]],\n\n         [[-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          ...,\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.],\n          [-1., -1., -1.,  ..., -1., -1., -1.]]]]), 'input_ids': tensor([[   0,  425,   13,  108,  415,  448, 7649,   15,   36,  981,    5, 1571,\n           10, 1214, 1784,    5,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nğŸ“„ Máº«u test:\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/c647f4c03511dd23caa90772fe80f906_1.jpg', 'pixel_values': tensor([[[[-0.5686, -0.4824, -0.5922,  ..., -0.4431, -0.2784, -0.1608],\n          [-0.5373, -0.5451, -0.6235,  ..., -0.3882, -0.4196, -0.4353],\n          [-0.4431, -0.5922, -0.6392,  ..., -0.3804, -0.3961, -0.2549],\n          ...,\n          [ 0.1686,  0.1137,  0.0824,  ...,  0.0980,  0.1843,  0.1843],\n          [ 0.2549,  0.3176,  0.3255,  ...,  0.0667,  0.1294,  0.1294],\n          [ 0.5059,  0.6157,  0.5686,  ...,  0.0275,  0.0667,  0.0588]],\n\n         [[-0.4196, -0.2863, -0.3961,  ..., -0.3725, -0.1843, -0.0431],\n          [-0.3882, -0.3569, -0.4275,  ..., -0.2784, -0.3176, -0.3333],\n          [-0.3020, -0.4039, -0.4510,  ..., -0.2706, -0.3098, -0.1843],\n          ...,\n          [-0.0510, -0.0824, -0.0824,  ...,  0.0196,  0.1059,  0.1059],\n          [ 0.0745,  0.1529,  0.1843,  ..., -0.0039,  0.0588,  0.0510],\n          [ 0.3725,  0.4902,  0.4510,  ..., -0.0353, -0.0039, -0.0196]],\n\n         [[-0.6392, -0.5059, -0.5922,  ..., -0.5451, -0.3882, -0.3020],\n          [-0.6000, -0.5765, -0.6314,  ..., -0.5451, -0.5451, -0.5608],\n          [-0.4824, -0.6078, -0.6471,  ..., -0.5451, -0.5294, -0.3804],\n          ...,\n          [-0.2000, -0.2471, -0.2627,  ..., -0.1608, -0.0824, -0.0902],\n          [-0.0588, -0.0039,  0.0118,  ..., -0.1765, -0.1373, -0.1451],\n          [ 0.2471,  0.3255,  0.2627,  ..., -0.2078, -0.1922, -0.2157]]]]), 'input_ids': tensor([[   0, 7069,  140, 1867, 2911,    4,  715,  448,  330,    5,   18,  452,\n         1263,  121,  981,   12,   58, 1500,    5,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from datetime import datetime\nimport pickle\n\nprint(type(test_pkl_data[0]))\n\n# Láº¥y ngÃ y hiá»‡n táº¡i theo Ä‘á»‹nh dáº¡ng dd_mm_yy\ntoday_str = datetime.now().strftime(\"%d_%m_%y\")\n\n# Táº¡o tÃªn file cÃ³ chá»©a ngÃ y\ntest_pkl_filename = f\"{today_str}_test_pkl_data.pkl\"\n\n# LÆ°u file\nwith open(test_pkl_filename, \"wb\") as f:\n    pickle.dump(test_pkl_data, f)\n\nprint(f\"âœ… ÄÃ£ lÆ°u file test: '{test_pkl_filename}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T16:01:59.578586Z","iopub.execute_input":"2025-05-26T16:01:59.578948Z","iopub.status.idle":"2025-05-26T16:02:00.626339Z","shell.execute_reply.started":"2025-05-26T16:01:59.578924Z","shell.execute_reply":"2025-05-26T16:02:00.625119Z"}},"outputs":[{"name":"stdout","text":"<class 'dict'>\nâœ… ÄÃ£ lÆ°u file test: '26_05_25_test_pkl_data.pkl'\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Colected Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef load_data(data_path=\"/kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv\"):\n    try:\n        df = pd.read_csv(data_path)\n        df = df[['original_url','local_path', 'search_query', 'short_caption']].rename(columns={\n            'original_url': 'original_url',\n            'local_path': 'url',\n            'search_query': 'search_query',\n            'short_caption': 'caption'\n        })\n        print(f\"ÄÃ£ táº£i CSV tá»«: {data_path} (KÃ­ch thÆ°á»›c: {df.shape})\")\n        return df\n    except FileNotFoundError:\n        print(f\"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file CSV táº¡i {data_path}\")\n        raise\n    except Exception as e:\n        print(f\"Lá»—i khi Ä‘á»c CSV: {e}\")\n        raise\n\n# Load dá»¯ liá»‡u\ndf = load_data()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.019570Z","iopub.execute_input":"2025-05-26T09:02:09.019825Z","iopub.status.idle":"2025-05-26T09:02:09.324624Z","shell.execute_reply.started":"2025-05-26T09:02:09.019786Z","shell.execute_reply":"2025-05-26T09:02:09.323859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Split data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# HÃ m chia dá»¯ liá»‡u giá»¯ nguyÃªn\ndef split_data(df, stratify_col='search_query', test_size=0.1, val_size=0.1, random_state=42):\n    unique_urls = df.drop_duplicates('original_url')\n\n    sss_1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_val_idx, test_idx in sss_1.split(unique_urls, unique_urls[stratify_col]):\n        train_val_urls = unique_urls.iloc[train_val_idx]['original_url']\n        test_urls = unique_urls.iloc[test_idx]['original_url']\n\n    df_train_val = df[df['original_url'].isin(train_val_urls)]\n    df_test = df[df['original_url'].isin(test_urls)]\n\n    unique_train_val_urls = df_train_val.drop_duplicates('original_url')\n    val_ratio = val_size / (1 - test_size)\n    sss_2 = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n    for train_idx, val_idx in sss_2.split(unique_train_val_urls, unique_train_val_urls[stratify_col]):\n        train_urls = unique_train_val_urls.iloc[train_idx]['original_url']\n        val_urls = unique_train_val_urls.iloc[val_idx]['original_url']\n\n    df_train = df_train_val[df_train_val['original_url'].isin(train_urls)]\n    df_val = df_train_val[df_train_val['original_url'].isin(val_urls)]\n\n    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.325361Z","iopub.execute_input":"2025-05-26T09:02:09.325566Z","iopub.status.idle":"2025-05-26T09:02:09.332105Z","shell.execute_reply.started":"2025-05-26T09:02:09.325548Z","shell.execute_reply":"2025-05-26T09:02:09.331368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_split_save(df: pd.DataFrame): \n    # Split\n    train_df, val_df, test_df = split_data(df, stratify_col='search_query')\n    \n    # Kiá»ƒm tra\n    print(\"Train size:\", len(train_df))\n    print(\"Val size:\", len(val_df))\n    print(\"Test size:\", len(test_df))\n    print(train_df['search_query'].value_counts(normalize=True))\n    print(val_df['search_query'].value_counts(normalize=True))\n    print(test_df['search_query'].value_counts(normalize=True))\n\n    # Save file\n    # Reset index\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    test_df = test_df.reset_index(drop=True)\n    \n    # Kiá»ƒm tra sá»‘ lÆ°á»£ng máº«u sau khi chia\n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n    \n    # LÆ°u cÃ¡c DataFrame vÃ o JSON\n    train_df.to_json(\"train.json\", orient='records', indent=4, force_ascii=False)\n    val_df.to_json(\"val.json\", orient='records', indent=4, force_ascii=False)\n    test_df.to_json(\"test.json\", orient='records', indent=4, force_ascii=False)\n    \n    print(\"ÄÃ£ lÆ°u train.json, val.json, test.json\")\n    return train_df, val_df, test_df, df\n\n# train_df, val_df, test_df, df = load_split_save(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.332873Z","iopub.execute_input":"2025-05-26T09:02:09.333105Z","iopub.status.idle":"2025-05-26T09:02:09.353283Z","shell.execute_reply.started":"2025-05-26T09:02:09.333081Z","shell.execute_reply":"2025-05-26T09:02:09.352681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Concat 2 data frame (colected + userAPI)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Giáº£ sá»­ df lÃ  DataFrame chá»©a cá»™t áº£nh hoáº·c thÃ´ng tin áº£nh\n# BÆ°á»›c 1: Chia train vÃ  táº¡m (val + test)\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n\n# BÆ°á»›c 2: Chia tiáº¿p temp thÃ nh val vÃ  test\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n\nprint(f\"Sá»‘ lÆ°á»£ng áº£nh trong train: {len(train_df)}\")\nprint(f\"Sá»‘ lÆ°á»£ng áº£nh trong val: {len(val_df)}\")\nprint(f\"Sá»‘ lÆ°á»£ng áº£nh trong test: {len(test_df)}\")\n\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\ndf = df.copy().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.353969Z","iopub.execute_input":"2025-05-26T09:02:09.354155Z","iopub.status.idle":"2025-05-26T09:02:09.388520Z","shell.execute_reply.started":"2025-05-26T09:02:09.354120Z","shell.execute_reply":"2025-05-26T09:02:09.387878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Load feature extractor","metadata":{}},{"cell_type":"code","source":"feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n\ndef load_image(local_path, dataset_base_path=\"/kaggle/input/traffic-pictures-captioning/\"):\n    \"\"\"\n    Load áº£nh tá»« local_path trong dataset trÃªn Kaggle.\n    \n    Parameters:\n    - local_path (str): ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i cá»§a áº£nh (tá»« cá»™t 'url' trong DataFrame)\n    - dataset_base_path (str): ÄÆ°á»ng dáº«n gá»‘c Ä‘áº¿n dataset\n    \n    Returns:\n    - image_rgb (numpy.ndarray): áº¢nh á»Ÿ Ä‘á»‹nh dáº¡ng RGB, hoáº·c None náº¿u lá»—i\n    \"\"\"\n    try:\n        # Chuáº©n hÃ³a Ä‘Æ°á»ng dáº«n áº£nh\n        local_path = local_path.lstrip('./')  # Loáº¡i bá» './' náº¿u cÃ³\n        full_image_path = os.path.join(dataset_base_path, local_path)\n        \n        # Äá»c áº£nh báº±ng OpenCV\n        image = cv2.imread(full_image_path)\n        if image is None:\n            print(f\"Lá»—i: KhÃ´ng thá»ƒ Ä‘á»c áº£nh tá»« {full_image_path}\")\n            return None\n        \n        # Chuyá»ƒn tá»« BGR sang RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image_rgb\n    \n    except Exception as e:\n        print(f\"Lá»—i khi xá»­ lÃ½ áº£nh {full_image_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.394346Z","iopub.execute_input":"2025-05-26T09:02:09.395350Z","iopub.status.idle":"2025-05-26T09:02:09.524823Z","shell.execute_reply.started":"2025-05-26T09:02:09.395332Z","shell.execute_reply":"2025-05-26T09:02:09.524093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Load Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer cá»§a BartPho\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n\n# Kiá»ƒm tra vocab size\nvocab_size = tokenizer.vocab_size\nprint(f\"VOCAB SIZE: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.529979Z","iopub.execute_input":"2025-05-26T09:02:09.530207Z","iopub.status.idle":"2025-05-26T09:02:09.951611Z","shell.execute_reply.started":"2025-05-26T09:02:09.530191Z","shell.execute_reply":"2025-05-26T09:02:09.950958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In ra cÃ¡c special tokens\nprint(\"Special Tokens:\", tokenizer.special_tokens_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.952440Z","iopub.execute_input":"2025-05-26T09:02:09.953296Z","iopub.status.idle":"2025-05-26T09:02:09.957062Z","shell.execute_reply.started":"2025-05-26T09:02:09.953274Z","shell.execute_reply":"2025-05-26T09:02:09.956354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. CÃ¡c hÃ m tiá»n xá»­ lÃ½ caption","metadata":{}},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    return re.sub(r\"[^\\w\\s,!?.]\", \"\", text).strip()\n\ndef to_lowercase(text: str) -> str:\n    return text.lower()\n\ndef join_vietnamese_compounds(text: str) -> str:\n    return ViTokenizer.tokenize(text)\n\ndef caption_preprocess(text: str) -> str:\n    text = clean_text(text)\n    text = to_lowercase(text)\n    text = join_vietnamese_compounds(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.957668Z","iopub.execute_input":"2025-05-26T09:02:09.957905Z","iopub.status.idle":"2025-05-26T09:02:09.976433Z","shell.execute_reply.started":"2025-05-26T09:02:09.957880Z","shell.execute_reply":"2025-05-26T09:02:09.975744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = caption_preprocess(\"Kiá»ƒm tra phÃ¢n tÃ¡ch tá»«\")\n\n# TÃ¡ch token\ntokens = tokenizer.tokenize(text)\n\n# Chuyá»ƒn token thÃ nh ID\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\ntext = tokenizer.decode(token_ids, skip_special_tokens=True)\n# In káº¿t quáº£\nprint(\"List Word (Tokenized):\", tokens)\nprint(\"List Token ID:\", token_ids)\nprint(\"Text:\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.977081Z","iopub.execute_input":"2025-05-26T09:02:09.977297Z","iopub.status.idle":"2025-05-26T09:02:09.993089Z","shell.execute_reply.started":"2025-05-26T09:02:09.977281Z","shell.execute_reply":"2025-05-26T09:02:09.992494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HÃ m tÃ­nh sá»‘ tá»« cá»§a má»™t caption\ndef count_words(caption):\n    preprocessed_caption = caption_preprocess(caption)\n    tokens = tokenizer.tokenize(preprocessed_caption)\n    return len(tokens)\n\n# Ãp dá»¥ng hÃ m cho toÃ n bá»™ dataframe\ndf['word_count'] = df['caption'].apply(count_words)\n\n# Láº¥y Ä‘á»™ dÃ i lá»›n nháº¥t\nmax_length = df['word_count'].max()\n\n# In ra\nprint(f\"Äá»™ dÃ i caption dÃ i nháº¥t lÃ : {max_length} tá»«\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:09.993868Z","iopub.execute_input":"2025-05-26T09:02:09.994079Z","iopub.status.idle":"2025-05-26T09:02:32.036684Z","shell.execute_reply.started":"2025-05-26T09:02:09.994063Z","shell.execute_reply":"2025-05-26T09:02:32.035858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Tiá»n xá»­ lÃ½ input model","metadata":{}},{"cell_type":"code","source":"def process_data(image_url, caption):\n    try:\n        img_array = load_image(image_url)\n        if img_array is None:\n            return None\n        \n        pixel_values = feature_extractor(img_array, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        caption = caption_preprocess(caption)\n        tokenized_caption = tokenizer(caption, padding=\"max_length\", max_length=max_length, truncation=True)\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": torch.tensor(tokenized_caption[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(tokenized_caption[\"attention_mask\"])\n        }\n    except Exception as e:\n        print(f\"Error processing data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.040154Z","iopub.execute_input":"2025-05-26T09:02:32.040386Z","iopub.status.idle":"2025-05-26T09:02:32.045564Z","shell.execute_reply.started":"2025-05-26T09:02:32.040368Z","shell.execute_reply":"2025-05-26T09:02:32.044768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Táº¡o táº­p dá»¯ liá»‡u huáº¥n luyá»‡n","metadata":{}},{"cell_type":"code","source":"class ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions):\n        self.image_paths = image_paths\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        data = process_data(self.image_paths[idx], self.captions[idx])\n        if data is None:\n            return self.__getitem__((idx + 1) % len(self))\n        return data\n\ndef custom_collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    return {\n        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch])\n    }\n\ntrain_dataset = ImageCaptionDataset(train_df[\"url\"], train_df[\"caption\"])\nval_dataset = ImageCaptionDataset(val_df[\"url\"], val_df[\"caption\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.046314Z","iopub.execute_input":"2025-05-26T09:02:32.046523Z","iopub.status.idle":"2025-05-26T09:02:32.068635Z","shell.execute_reply.started":"2025-05-26T09:02:32.046507Z","shell.execute_reply":"2025-05-26T09:02:32.067750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_pkl_data))\nprint(train_pkl_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.069672Z","iopub.execute_input":"2025-05-26T09:02:32.070321Z","iopub.status.idle":"2025-05-26T09:02:32.090524Z","shell.execute_reply.started":"2025-05-26T09:02:32.070292Z","shell.execute_reply":"2025-05-26T09:02:32.089861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataset))\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.091416Z","iopub.execute_input":"2025-05-26T09:02:32.091731Z","iopub.status.idle":"2025-05-26T09:02:32.118557Z","shell.execute_reply.started":"2025-05-26T09:02:32.091713Z","shell.execute_reply":"2025-05-26T09:02:32.117975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Class táº¡o Dataset tá»« list cÃ¡c dict\nclass DatasetFromList(Dataset):\n    def __init__(self, data_list):\n        self.data_list = data_list\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, idx):\n        return self.data_list[idx]\n\n# HÃ m táº¡o DatasetFromList tá»« list dict\ndef dataset_from_list(data_list):\n    return DatasetFromList(data_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.119270Z","iopub.execute_input":"2025-05-26T09:02:32.119503Z","iopub.status.idle":"2025-05-26T09:02:32.124145Z","shell.execute_reply.started":"2025-05-26T09:02:32.119475Z","shell.execute_reply":"2025-05-26T09:02:32.123518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_data = []\n\nfor item in train_pkl_data:\n    if \"image_url\" in item:\n        del item[\"image_url\"]\n\n    if \"pixel_values\" in item:\n        item[\"pixel_values\"] = item[\"pixel_values\"].squeeze(0)\n    \n    # Xá»­ lÃ½ input_ids vÃ  attention_mask: squeeze rá»“i pad/cáº¯t\n    if \"input_ids\" in item and \"attention_mask\" in item:\n        input_ids = item[\"input_ids\"].squeeze(0)\n        attention_mask = item[\"attention_mask\"].squeeze(0)\n\n        # Chuyá»ƒn tensor sang list Ä‘á»ƒ dá»… xá»­ lÃ½\n        input_ids_list = input_ids.tolist()\n        attention_mask_list = attention_mask.tolist()\n\n        # Pad hoáº·c truncate cho input_ids\n        if len(input_ids_list) < max_length:\n            pad_length = max_length - len(input_ids_list)\n            input_ids_list += [1] * pad_length          # pad vá»›i 1 (token id padding)\n            attention_mask_list += [0] * pad_length     # pad mask báº±ng 0\n        else:\n            input_ids_list = input_ids_list[:max_length]\n            attention_mask_list = attention_mask_list[:max_length]\n\n        # Chuyá»ƒn láº¡i thÃ nh tensor\n        import torch\n        item[\"input_ids\"] = torch.tensor(input_ids_list)\n        item[\"attention_mask\"] = torch.tensor(attention_mask_list)\n\n    new_data.append(item)\n\n\ncleaned_dataset = dataset_from_list(new_data)\nprint(cleaned_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.124765Z","iopub.execute_input":"2025-05-26T09:02:32.124969Z","iopub.status.idle":"2025-05-26T09:02:32.148055Z","shell.execute_reply.started":"2025-05-26T09:02:32.124953Z","shell.execute_reply":"2025-05-26T09:02:32.147455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GhÃ©p dataset cho dá»¯ liá»‡u cÅ© vÃ  dá»¯ liá»‡u má»›i tá»« API","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\n\ntrain_dataset = ConcatDataset([cleaned_dataset, train_dataset])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.148959Z","iopub.execute_input":"2025-05-26T09:02:32.149775Z","iopub.status.idle":"2025-05-26T09:02:32.156783Z","shell.execute_reply.started":"2025-05-26T09:02:32.149747Z","shell.execute_reply":"2025-05-26T09:02:32.155986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataset))\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:02:32.157573Z","iopub.execute_input":"2025-05-26T09:02:32.157773Z","iopub.status.idle":"2025-05-26T09:02:32.176337Z","shell.execute_reply.started":"2025-05-26T09:02:32.157756Z","shell.execute_reply":"2025-05-26T09:02:32.175686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Cáº¥u hÃ¬nh LoRA","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom peft import PeftModel\n\n# Cáº¥u hÃ¬nh LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],  # CÃ¡c lá»›p trong GPT2\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:15:59.567851Z","iopub.execute_input":"2025-05-26T08:15:59.568096Z","iopub.status.idle":"2025-05-26T08:15:59.881716Z","shell.execute_reply.started":"2025-05-26T08:15:59.568074Z","shell.execute_reply":"2025-05-26T08:15:59.881153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Äá»‹nh nghÄ©a mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\ndel vit_model.classifier\nvit_model.vit.pooler = torch.nn.Sequential(OrderedDict([\n    ('dense', torch.nn.Linear(in_features=768, out_features=768, bias=True)),\n    ('activation', torch.nn.Tanh())\n]))\n\nconfig = GPT2Config.from_pretrained(\"gpt2\")\nconfig.add_cross_attention = True\nconfig.vocab_size = vocab_size\n\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config, ignore_mismatched_sizes=True)\ngpt2_model.resize_token_embeddings(config.vocab_size)\n\n# Ãp dá»¥ng LoRA vÃ o GPT2\ngpt2_model = get_peft_model(gpt2_model, lora_config)\ngpt2_model.print_trainable_parameters()  # kiá»ƒm tra sá»‘ lÆ°á»£ng tham sá»‘ cáº§n huáº¥n luyá»‡n\n\nmodel = VisionEncoderDecoderModel(encoder=vit_model.vit, decoder=gpt2_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:15:59.882439Z","iopub.execute_input":"2025-05-26T08:15:59.882875Z","iopub.status.idle":"2025-05-26T08:16:07.036888Z","shell.execute_reply.started":"2025-05-26T08:15:59.882850Z","shell.execute_reply":"2025-05-26T08:16:07.036246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11. Äá»‹nh nghÄ©a lá»›p mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"class LoRACaptionWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    def load_context(self, context):\n        try:\n            # ÄÆ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n cÃ¡c thÆ° má»¥c con trong artifact\n            model_path = context.artifacts[\"model_dir\"]\n            tokenizer_path = os.path.join(model_path, \"tokenizer\")\n            feature_extractor_path = os.path.join(model_path, \"feature_extractor\")\n\n            \n            # Load cÃ¡c thÃ nh pháº§n vá»›i Ä‘Æ°á»ng dáº«n chÃ­nh xÃ¡c\n            self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            self.feature_extractor = ViTImageProcessor.from_pretrained(feature_extractor_path)\n            \n            # Äáº·t model vÃ o cháº¿ Ä‘á»™ eval\n            self.model.eval()\n        except Exception as e:\n            raise ValueError(f\"Error loading model: {str(e)}\")\n\n    def predict(self, context, model_input):\n        try:\n            # Xá»­ lÃ½ Ä‘áº§u vÃ o\n            if isinstance(model_input, dict):\n                image_url = model_input[\"url\"][0] if \"url\" in model_input else model_input[\"image_path\"][0]\n            else:\n                image_url = model_input.iloc[0][\"url\"] if \"url\" in model_input.columns else model_input.iloc[0][\"image_path\"]\n            \n            image = self.load_image(image_url)\n            if image is None:\n                return [\"\"]\n                \n            # Tiá»n xá»­ lÃ½ áº£nh vÃ  táº¡o caption\n            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n            \n            output_ids = self.model.generate(\n                pixel_values,\n                max_length=45,\n                min_length=20,\n                num_beams=4,\n                do_sample=True,\n                temperature=0.8,\n                top_k=20,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                repetition_penalty=2.0,\n                early_stopping=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                decoder_start_token_id=self.tokenizer.bos_token_id\n            )\n            \n            caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            return [caption.replace(\"_\", \" \").strip()]\n            \n        except Exception as e:\n            print(f\"[ERROR] during prediction: {e}\")\n            return [\"\"]\n   \n    def load_image(self, image_url):\n        try:\n            import requests\n            import numpy as np\n            from PIL import Image\n            from io import BytesIO\n            \n            response = requests.get(image_url, timeout=10)\n            if response.status_code != 200:\n                return None\n                \n            image = Image.open(BytesIO(response.content))\n            return image.convert(\"RGB\") if image else None\n            \n        except Exception as e:\n            print(f\"[ERROR] loading image: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:16:07.037641Z","iopub.execute_input":"2025-05-26T08:16:07.037886Z","iopub.status.idle":"2025-05-26T08:16:07.117309Z","shell.execute_reply.started":"2025-05-26T08:16:07.037865Z","shell.execute_reply":"2025-05-26T08:16:07.116710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 12. Huáº¥n luyá»‡n mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ndef evaluate_model(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            encoder_outputs = model.encoder(pixel_values=pixel_values)\n            outputs = model.decoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                encoder_hidden_states=encoder_outputs.last_hidden_state,\n                labels=input_ids\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n    return total_loss / len(val_dataloader)\n\ndef train_model(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr=5e-5, epochs=1, epoch_count=1, model_name='BartPho_ViT_GPT2_LoRA_ICG', data_part=\"Part_1\", df_log=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Optimizer chá»‰ update cÃ¡c tham sá»‘ cÃ³ requires_grad=True (LoRA adapter)\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Thá»‘ng kÃª tham sá»‘ trainable\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n\n    run_name = model_name + \"_\" + data_part\n\n    with mlflow.start_run(run_name=run_name):\n        if df_log is not None:\n            dataset = mlflow.data.from_pandas(df_log, targets=\"caption\")\n            mlflow.log_input(dataset, context=\"training\")\n\n        # Logging cÃ¡c siÃªu tham sá»‘\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"epoch_count\", epoch_count)\n        mlflow.log_param(\"data_part\", data_part)\n        mlflow.log_param(\"learning_rate\", lr)\n        mlflow.log_param(\"train_data_size\", len(train_dataloader.dataset))\n        mlflow.log_param(\"val_data_size\", len(val_dataloader.dataset))\n        mlflow.log_param(\"trainable_params\", trainable_params)\n        mlflow.log_param(\"total_params\", total_params)\n        mlflow.log_param(\"trainable_ratio\", round(trainable_params / total_params, 4))\n        mlflow.log_param(\"model_name\", model_name)\n\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch in train_dataloader:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n\n                encoder_outputs = model.encoder(pixel_values=pixel_values)\n                outputs = model.decoder(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_outputs.last_hidden_state,\n                    labels=input_ids\n                )\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_loss = total_loss / len(train_dataloader)\n            val_loss = evaluate_model(model, val_dataloader, device)\n\n            # Log loss theo epoch\n            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n            print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n            # LÆ°u mÃ´ hÃ¬nh sau má»—i epoch\n            epoch_save_path = f\"/kaggle/working/{model_name}_epoch_{epoch+1}\"\n            model.save_pretrained(epoch_save_path)\n            tokenizer.save_pretrained(os.path.join(epoch_save_path, \"tokenizer\"))\n            feature_extractor.save_pretrained(os.path.join(epoch_save_path, \"feature_extractor\"))\n            \n            # Log toÃ n bá»™ thÆ° má»¥c nhÆ° má»™t artifact\n            mlflow.log_artifacts(epoch_save_path, artifact_path=f\"epoch_{epoch+1}\")\n\n        # LÆ°u model cuá»‘i cÃ¹ng riÃªng biá»‡t\n        final_model_path = f\"/kaggle/working/{model_name}_final\"\n        model.save_pretrained(final_model_path)\n        tokenizer.save_pretrained(os.path.join(final_model_path, \"tokenizer\"))\n        feature_extractor.save_pretrained(os.path.join(final_model_path, \"feature_extractor\"))\n        mlflow.log_artifacts(final_model_path, artifact_path=\"final_model\")\n\n        # Log model dÆ°á»›i dáº¡ng pyfunc (náº¿u cáº§n)\n        # Äáº£m báº£o báº¡n Ä‘Ã£ Ä‘á»‹nh nghÄ©a class ImageCaptionModel trÆ°á»›c\n\n        # Infer signature\n        input_example = pd.DataFrame({\"url\": [\"http://example.com/test.jpg\"]})\n        output_example = pd.DataFrame({\"caption\": [\"MÃ´ táº£ áº£nh\"]})\n        \n        # 2. Táº¡o model signature\n        signature = infer_signature(\n            input_example,\n            output_example,\n            params={\"input_types\": \"string\", \"output_types\": \"string\"}  # Chá»‰ Ä‘á»‹nh kiá»ƒu dá»¯ liá»‡u\n        )\n        mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=LoRACaptionWrapper(),\n            artifacts={\"model_dir\": final_model_path},  # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a model, tokenizer vÃ  feature_extractor\n            registered_model_name=model_name,\n            signature=signature,\n            # input_example=input_example,\n            pip_requirements=pip_reqs\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:16:07.118074Z","iopub.execute_input":"2025-05-26T08:16:07.118730Z","iopub.status.idle":"2025-05-26T08:16:07.660264Z","shell.execute_reply.started":"2025-05-26T08:16:07.118701Z","shell.execute_reply":"2025-05-26T08:16:07.659723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Táº¡o k Subset cho train, val\ntrain_subset = Subset(train_dataset, range(len(train_dataset)))  # Láº¥y train_df.shape[0] dá»¯ liá»‡u Ä‘áº§u tiÃªn tá»« train_dataset\nval_subset = Subset(val_dataset, range(val_df.shape[0]))     # Láº¥y 4620 dá»¯ liá»‡u Ä‘áº§u tiÃªn tá»« val_dataset\n\n# Táº¡o DataLoader cho cÃ¡c subset\ntrain_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\nval_dataloader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:19:17.530669Z","iopub.execute_input":"2025-05-26T08:19:17.531368Z","iopub.status.idle":"2025-05-26T08:19:17.535627Z","shell.execute_reply.started":"2025-05-26T08:19:17.531341Z","shell.execute_reply":"2025-05-26T08:19:17.534894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13. Load model pretrain","metadata":{}},{"cell_type":"code","source":"# ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a cÃ¡c tá»‡p mÃ´ hÃ¬nh\nmodel_path = '/kaggle/input/phovit-gptcap/pytorch/default/15'\n\n# Táº£i mÃ´ hÃ¬nh VisionEncoderDecoder\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path, ignore_mismatched_sizes=True)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:19:21.450843Z","iopub.execute_input":"2025-05-26T08:19:21.451115Z","iopub.status.idle":"2025-05-26T08:19:22.252678Z","shell.execute_reply.started":"2025-05-26T08:19:21.451095Z","shell.execute_reply":"2025-05-26T08:19:22.252091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 14. Huáº¥n luyá»‡n","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\n# Láº¥y ngÃ y hiá»‡n táº¡i theo format dd-mm-yyyy\ntoday_str = datetime.now().strftime(\"%d-%m-%Y\")\nmodel_name = f'BartPho_ViT_GPT2_LoRA_ICG {today_str} Incremental'\n\ntrain_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    tokenizer,\n    feature_extractor,\n    lr=5e-5,\n    epochs=2,\n    epoch_count=17,\n    model_name=model_name,\n    data_part=\"FULL\",\n    df_log=df\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T08:20:21.053843Z","iopub.execute_input":"2025-05-26T08:20:21.054118Z","iopub.status.idle":"2025-05-26T08:20:29.532916Z","shell.execute_reply.started":"2025-05-26T08:20:21.054099Z","shell.execute_reply":"2025-05-26T08:20:29.531726Z"}},"outputs":[],"execution_count":null}]}