{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11334502,"sourceType":"datasetVersion","datasetId":6723117},{"sourceId":11337142,"sourceType":"datasetVersion","datasetId":7085417,"isSourceIdPinned":true},{"sourceId":331865,"sourceType":"modelInstanceVersion","modelInstanceId":277580,"modelId":298468},{"sourceId":374794,"sourceType":"modelInstanceVersion","modelInstanceId":307843,"modelId":328293}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow pyvi minio -q\n!pip install hf_xet -q\n\nimport mlflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:09.725198Z","iopub.execute_input":"2025-05-27T15:13:09.725427Z","iopub.status.idle":"2025-05-27T15:13:31.885051Z","shell.execute_reply.started":"2025-05-27T15:13:09.725408Z","shell.execute_reply":"2025-05-27T15:13:31.884087Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m722.9/722.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"mlflow.set_tracking_uri(\"http://36.50.135.226:7893/\")\n\n# from mlflow.tracking import MlflowClient\n\n# client = MlflowClient()\n# client.restore_experiment(experiment_id=\"2\")\n\nmlflow.set_experiment(experiment_id=\"10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:31.886112Z","iopub.execute_input":"2025-05-27T15:13:31.887305Z","iopub.status.idle":"2025-05-27T15:13:32.402294Z","shell.execute_reply.started":"2025-05-27T15:13:31.887271Z","shell.execute_reply":"2025-05-27T15:13:32.401511Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='s3://mlflow/6', creation_time=1745743711248, experiment_id='10', last_update_time=1745743711248, lifecycle_stage='active', name='ImageCaption_TPC37k_BartPho-ViT-GPT2_LoRALayerFT', tags={}>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\n\nos.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://36.50.135.226:9000\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:32.404542Z","iopub.execute_input":"2025-05-27T15:13:32.404895Z","iopub.status.idle":"2025-05-27T15:13:32.409022Z","shell.execute_reply.started":"2025-05-27T15:13:32.404871Z","shell.execute_reply":"2025-05-27T15:13:32.408188Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# MinIO","metadata":{}},{"cell_type":"code","source":"from minio import Minio\nfrom minio.error import S3Error\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:32.409793Z","iopub.execute_input":"2025-05-27T15:13:32.410007Z","iopub.status.idle":"2025-05-27T15:13:32.850864Z","shell.execute_reply.started":"2025-05-27T15:13:32.409989Z","shell.execute_reply":"2025-05-27T15:13:32.849949Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nimport transformers\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor\nimport torch\nfrom PIL import Image\nimport torch.nn as nn\nimport cv2\nimport torchvision\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom collections import OrderedDict\nfrom transformers import GPT2Config, GPT2LMHeadModel\nimport mlflow\nfrom mlflow.models import infer_signature\nimport mlflow.pytorch\nimport re\nfrom pyvi import ViTokenizer\nfrom torch.optim import AdamW\nfrom datetime import datetime\nimport json\nimport pickle\nfrom io import BytesIO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:32.851848Z","iopub.execute_input":"2025-05-27T15:13:32.852195Z","iopub.status.idle":"2025-05-27T15:14:05.118848Z","shell.execute_reply.started":"2025-05-27T15:13:32.852166Z","shell.execute_reply":"2025-05-27T15:14:05.118056Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 15:13:48.716264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748358828.967395      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748358829.035919      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 1. Chuáº©n bá»‹ file thÆ° viá»‡n","metadata":{}},{"cell_type":"code","source":"pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:14:05.119791Z","iopub.execute_input":"2025-05-27T15:14:05.120395Z","iopub.status.idle":"2025-05-27T15:14:07.142380Z","shell.execute_reply.started":"2025-05-27T15:14:05.120371Z","shell.execute_reply":"2025-05-27T15:14:07.141117Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"with open(\"requirements.txt\") as f:\n    pip_reqs = [line.strip() for line in f if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:14:07.143460Z","iopub.execute_input":"2025-05-27T15:14:07.143814Z","iopub.status.idle":"2025-05-27T15:14:07.150205Z","shell.execute_reply.started":"2025-05-27T15:14:07.143776Z","shell.execute_reply":"2025-05-27T15:14:07.149111Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 2. Load data","metadata":{}},{"cell_type":"markdown","source":"## API Data","metadata":{}},{"cell_type":"code","source":"# 1. Gá»­i GET request Ä‘áº¿n API metadata\nurl = \"http://36.50.135.226/api/v1/metadata/encoded-data\"\nheaders = {\"accept\": \"application/json\"}\nresponse = requests.get(url, headers=headers)\n\n# 2. Kiá»ƒm tra pháº£n há»“i\nif response.status_code == 200:\n    data = response.json()\n    object_keys = data.get(\"object_keys\", [])\n    def extract_timestamp(key):\n        match = re.search(r\"encoded_data_(\\d{14})\\.pkl\", key)\n        if match:\n            return datetime.strptime(match.group(1), \"%Y%m%d%H%M%S\")\n        return None\n    if object_keys:\n        base_url = \"http://160.191.244.13:9000/lakehouse/\"\n        all_pkl_data = []  # Biáº¿n chá»©a toÃ n bá»™ dá»¯ liá»‡u\n        for key in sorted(object_keys, key=extract_timestamp):\n            file_url = base_url + key\n            file_response = requests.get(file_url)\n            if file_response.status_code == 200:\n                try:\n                    file_bytes = BytesIO(file_response.content)\n                    pkl_data = pickle.load(file_bytes)\n                    # Gá»™p dá»¯ liá»‡u\n                    if isinstance(pkl_data, list):\n                        all_pkl_data.extend(pkl_data)\n                    elif isinstance(pkl_data, dict):\n                        all_pkl_data.append(pkl_data)\n                    else:\n                        all_pkl_data.append(pkl_data)\n                    print(f\"âœ… ÄÃ£ xá»­ lÃ½: {key}\")\n                except Exception as e:\n                    print(f\"âš ï¸ Lá»—i Ä‘á»c file {key}: {e}\")\n            else:\n                print(f\"âŒ KhÃ´ng thá»ƒ táº£i file: {key}\")\n        # âœ… In ra tá»•ng sá»‘ pháº§n tá»­ Ä‘Ã£ gá»™p\n        print(f\"\\nğŸ“¦ Tá»•ng sá»‘ Ä‘á»‘i tÆ°á»£ng Ä‘Ã£ gá»™p: {len(all_pkl_data)}\")\n    else:\n        print(\"KhÃ´ng cÃ³ object_keys nÃ o trong dá»¯ liá»‡u.\")\nelse:\n    print(\"Lá»—i khi gá»i API:\", response.status_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:14:07.151250Z","iopub.execute_input":"2025-05-27T15:14:07.151590Z","iopub.status.idle":"2025-05-27T15:32:20.643835Z","shell.execute_reply.started":"2025-05-27T15:14:07.151562Z","shell.execute_reply":"2025-05-27T15:32:20.642827Z"}},"outputs":[{"name":"stdout","text":"âœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145536.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145620.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145657.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145708.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145719.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145731.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145744.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145755.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145809.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145821.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145832.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145846.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145859.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145913.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145925.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145938.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524145952.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150005.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150016.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150027.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150039.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150051.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150104.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150115.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150130.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150143.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150156.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150209.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150223.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150236.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150250.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150305.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150319.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150331.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150343.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150356.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150408.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150419.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150431.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150443.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150456.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150510.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150522.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150536.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150549.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150602.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150617.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150630.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150643.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150655.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150708.pkl\nâœ… ÄÃ£ xá»­ lÃ½: imcp/encoded-data/2025-05-24/encoded_data_20250524150719.pkl\n\nğŸ“¦ Tá»•ng sá»‘ Ä‘á»‘i tÆ°á»£ng Ä‘Ã£ gá»™p: 2576\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(len(all_pkl_data))\nprint(all_pkl_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:32:20.646883Z","iopub.execute_input":"2025-05-27T15:32:20.647183Z","iopub.status.idle":"2025-05-27T15:32:20.703912Z","shell.execute_reply.started":"2025-05-27T15:32:20.647161Z","shell.execute_reply":"2025-05-27T15:32:20.703078Z"}},"outputs":[{"name":"stdout","text":"2576\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/6e0a007b66b69f7c9aacb9b46606f0ed.jpg', 'pixel_values': tensor([[[[-0.9059, -0.7725, -0.7333,  ...,  0.0275, -0.0039, -0.0118],\n          [-0.8196, -0.6471, -0.5843,  ...,  0.0196,  0.0039, -0.0196],\n          [-0.6392, -0.6706, -0.6549,  ..., -0.0118, -0.0275, -0.0431],\n          ...,\n          [-0.2627, -0.2471, -0.2549,  ...,  0.1216,  0.2627,  0.2471],\n          [-0.2549, -0.2314, -0.2392,  ...,  0.4275,  0.4745,  0.3804],\n          [-0.2549, -0.2314, -0.2235,  ..., -0.0431, -0.0353, -0.0353]],\n\n         [[-0.8902, -0.7490, -0.7098,  ..., -0.3176, -0.3412, -0.3647],\n          [-0.8039, -0.6235, -0.5608,  ..., -0.3412, -0.3569, -0.3804],\n          [-0.6157, -0.6549, -0.6314,  ..., -0.3569, -0.3647, -0.3882],\n          ...,\n          [-0.4745, -0.4745, -0.4824,  ..., -0.0667,  0.0745,  0.0745],\n          [-0.4824, -0.4824, -0.4980,  ...,  0.2863,  0.3412,  0.2627],\n          [-0.4745, -0.4902, -0.4902,  ..., -0.2157, -0.2000, -0.2078]],\n\n         [[-0.9765, -0.8745, -0.8745,  ..., -0.7647, -0.7725, -0.7725],\n          [-0.8824, -0.7569, -0.7333,  ..., -0.7647, -0.7647, -0.7882],\n          [-0.7176, -0.7882, -0.8039,  ..., -0.7569, -0.7647, -0.7961],\n          ...,\n          [-0.6941, -0.6784, -0.6706,  ..., -0.1686,  0.0039,  0.0039],\n          [-0.6941, -0.6627, -0.6627,  ...,  0.2784,  0.3569,  0.2627],\n          [-0.6863, -0.6627, -0.6392,  ..., -0.2941, -0.2941, -0.3412]]]]), 'input_ids': tensor([[   0, 7069,  140, 1867,  109,    4,  715,  448,  330,    5,  105, 2786,\n           52,  132,    5,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import random\n\n# Giáº£ sá»­ all_pkl_data lÃ  má»™t list chá»©a toÃ n bá»™ dá»¯ liá»‡u\nprint(f\"Tá»•ng sá»‘ dá»¯ liá»‡u: {len(all_pkl_data)}\")  # 2576\n\n# 1. Shuffle Ä‘á»ƒ ngáº«u nhiÃªn hÃ³a thá»© tá»± (náº¿u cáº§n)\nrandom.shuffle(all_pkl_data)\n\n# 2. TÃ­nh sá»‘ lÆ°á»£ng pháº§n tá»­ cho táº­p train\ntrain_size = int(0.8 * len(all_pkl_data))  # 80%\n\n# 3. Chia dá»¯ liá»‡u\ntrain_pkl_data = all_pkl_data[:train_size]\ntest_pkl_data = all_pkl_data[train_size:]\n\n# 4. In kiá»ƒm tra\nprint(f\"Sá»‘ lÆ°á»£ng train: {len(train_pkl_data)}\")  # ~2060\nprint(f\"Sá»‘ lÆ°á»£ng test: {len(test_pkl_data)}\")    # ~516\n\n# âœ… In máº«u má»™t pháº§n tá»­\nprint(\"ğŸ“„ Máº«u train:\")\nprint(train_pkl_data[0])\nprint(\"ğŸ“„ Máº«u test:\")\nprint(test_pkl_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:32:20.704982Z","iopub.execute_input":"2025-05-27T15:32:20.705246Z","iopub.status.idle":"2025-05-27T15:32:20.719915Z","shell.execute_reply.started":"2025-05-27T15:32:20.705226Z","shell.execute_reply":"2025-05-27T15:32:20.718744Z"}},"outputs":[{"name":"stdout","text":"Tá»•ng sá»‘ dá»¯ liá»‡u: 2576\nSá»‘ lÆ°á»£ng train: 2060\nSá»‘ lÆ°á»£ng test: 516\nğŸ“„ Máº«u train:\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/7edd6547cc8e80c4d02f5f73fab60105.jpg', 'pixel_values': tensor([[[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n          [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n          [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n          ...,\n          [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n          [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n          [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n         [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n          [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n          [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n          ...,\n          [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n          [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n          [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n         [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n          [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n          [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n          ...,\n          [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n          [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n          [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]]), 'input_ids': tensor([[    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n           515,   105,    52,  2227,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nğŸ“„ Máº«u test:\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/d4e042f7a85d9082120dcd4e3de58a83_0.jpg', 'pixel_values': tensor([[[[-0.3647, -0.3804, -0.3961,  ..., -0.0431, -0.0431, -0.0275],\n          [-0.3490, -0.3725, -0.3647,  ..., -0.0196, -0.0431, -0.0196],\n          [-0.3333, -0.3490, -0.3569,  ..., -0.0196, -0.0431, -0.0039],\n          ...,\n          [ 0.2314,  0.1843,  0.0510,  ...,  0.3098,  0.3333,  0.2863],\n          [ 0.2314,  0.0902, -0.0667,  ...,  0.3176,  0.3647,  0.2157],\n          [ 0.3098,  0.3255,  0.2549,  ...,  0.3804,  0.3098,  0.3333]],\n\n         [[-0.5216, -0.5373, -0.5451,  ..., -0.0510, -0.0588, -0.0353],\n          [-0.5059, -0.5294, -0.5137,  ..., -0.0353, -0.0510, -0.0196],\n          [-0.4902, -0.5059, -0.5137,  ..., -0.0196, -0.0353,  0.0118],\n          ...,\n          [ 0.1608,  0.1137, -0.0275,  ...,  0.3176,  0.3490,  0.3098],\n          [ 0.1294, -0.0118, -0.1765,  ...,  0.3255,  0.3725,  0.2235],\n          [ 0.1843,  0.2000,  0.1294,  ...,  0.3961,  0.3176,  0.3412]],\n\n         [[-0.5765, -0.6000, -0.6235,  ..., -0.1216, -0.0510, -0.0039],\n          [-0.5608, -0.6000, -0.5922,  ..., -0.0980, -0.0902, -0.0431],\n          [-0.5529, -0.5765, -0.5922,  ..., -0.0745, -0.1216, -0.0902],\n          ...,\n          [ 0.0667,  0.0196, -0.1216,  ...,  0.2627,  0.3098,  0.2706],\n          [ 0.0118, -0.1059, -0.2471,  ...,  0.2784,  0.3333,  0.2000],\n          [ 0.0667,  0.1137,  0.0745,  ...,  0.3412,  0.2784,  0.3176]]]]), 'input_ids': tensor([[    0,   679,   103,   140,  1867,     4,   715,  1263,   330,     5,\n            18,   452,  1036, 16993,     4,    57,   981,    12,    58,  1867,\n             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from datetime import datetime\nimport pickle\n\nprint(type(test_pkl_data[0]))\n\n# Láº¥y ngÃ y hiá»‡n táº¡i theo Ä‘á»‹nh dáº¡ng dd_mm_yy\ntoday_str = datetime.now().strftime(\"%d_%m_%y\")\n\n# Táº¡o tÃªn file cÃ³ chá»©a ngÃ y\ntest_pkl_filename = f\"{today_str}_test_pkl_data.pkl\"\n\n# LÆ°u file\nwith open(test_pkl_filename, \"wb\") as f:\n    pickle.dump(test_pkl_data, f)\n\nprint(f\"âœ… ÄÃ£ lÆ°u file test: '{test_pkl_filename}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:32:20.720890Z","iopub.execute_input":"2025-05-27T15:32:20.721238Z","iopub.status.idle":"2025-05-27T15:32:21.772892Z","shell.execute_reply.started":"2025-05-27T15:32:20.721213Z","shell.execute_reply":"2025-05-27T15:32:21.771927Z"}},"outputs":[{"name":"stdout","text":"<class 'dict'>\nâœ… ÄÃ£ lÆ°u file test: '27_05_25_test_pkl_data.pkl'\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Colected Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef load_data(data_path=\"/kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv\"):\n    try:\n        df = pd.read_csv(data_path)\n        df = df[['original_url','local_path', 'search_query', 'short_caption']].rename(columns={\n            'original_url': 'original_url',\n            'local_path': 'url',\n            'search_query': 'search_query',\n            'short_caption': 'caption'\n        })\n        print(f\"ÄÃ£ táº£i CSV tá»«: {data_path} (KÃ­ch thÆ°á»›c: {df.shape})\")\n        return df\n    except FileNotFoundError:\n        print(f\"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file CSV táº¡i {data_path}\")\n        raise\n    except Exception as e:\n        print(f\"Lá»—i khi Ä‘á»c CSV: {e}\")\n        raise\n\n# Load dá»¯ liá»‡u\ndf = load_data()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:01.426920Z","iopub.execute_input":"2025-05-27T15:45:01.427283Z","iopub.status.idle":"2025-05-27T15:45:01.741470Z","shell.execute_reply.started":"2025-05-27T15:45:01.427260Z","shell.execute_reply":"2025-05-27T15:45:01.740712Z"}},"outputs":[{"name":"stdout","text":"ÄÃ£ táº£i CSV tá»«: /kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv (KÃ­ch thÆ°á»›c: (37056, 4))\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# 3. Split data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# HÃ m chia dá»¯ liá»‡u giá»¯ nguyÃªn\ndef split_data(df, stratify_col='search_query', test_size=0.1, val_size=0.1, random_state=42):\n    unique_urls = df.drop_duplicates('original_url')\n\n    sss_1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_val_idx, test_idx in sss_1.split(unique_urls, unique_urls[stratify_col]):\n        train_val_urls = unique_urls.iloc[train_val_idx]['original_url']\n        test_urls = unique_urls.iloc[test_idx]['original_url']\n\n    df_train_val = df[df['original_url'].isin(train_val_urls)]\n    df_test = df[df['original_url'].isin(test_urls)]\n\n    unique_train_val_urls = df_train_val.drop_duplicates('original_url')\n    val_ratio = val_size / (1 - test_size)\n    sss_2 = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n    for train_idx, val_idx in sss_2.split(unique_train_val_urls, unique_train_val_urls[stratify_col]):\n        train_urls = unique_train_val_urls.iloc[train_idx]['original_url']\n        val_urls = unique_train_val_urls.iloc[val_idx]['original_url']\n\n    df_train = df_train_val[df_train_val['original_url'].isin(train_urls)]\n    df_val = df_train_val[df_train_val['original_url'].isin(val_urls)]\n\n    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:01.855280Z","iopub.execute_input":"2025-05-27T15:45:01.855594Z","iopub.status.idle":"2025-05-27T15:45:01.863422Z","shell.execute_reply.started":"2025-05-27T15:45:01.855573Z","shell.execute_reply":"2025-05-27T15:45:01.862532Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def load_split_save(df: pd.DataFrame): \n    # Split\n    train_df, val_df, test_df = split_data(df, stratify_col='search_query')\n    \n    # Kiá»ƒm tra\n    print(\"Train size:\", len(train_df))\n    print(\"Val size:\", len(val_df))\n    print(\"Test size:\", len(test_df))\n    print(train_df['search_query'].value_counts(normalize=True))\n    print(val_df['search_query'].value_counts(normalize=True))\n    print(test_df['search_query'].value_counts(normalize=True))\n\n    # Save file\n    # Reset index\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    test_df = test_df.reset_index(drop=True)\n    \n    # Kiá»ƒm tra sá»‘ lÆ°á»£ng máº«u sau khi chia\n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n    \n    # LÆ°u cÃ¡c DataFrame vÃ o JSON\n    train_df.to_json(\"train.json\", orient='records', indent=4, force_ascii=False)\n    val_df.to_json(\"val.json\", orient='records', indent=4, force_ascii=False)\n    test_df.to_json(\"test.json\", orient='records', indent=4, force_ascii=False)\n    \n    print(\"ÄÃ£ lÆ°u train.json, val.json, test.json\")\n    return train_df, val_df, test_df, df\n\ntrain_df, val_df, test_df, df = load_split_save(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:02.045160Z","iopub.execute_input":"2025-05-27T15:45:02.045494Z","iopub.status.idle":"2025-05-27T15:45:02.294375Z","shell.execute_reply.started":"2025-05-27T15:45:02.045471Z","shell.execute_reply":"2025-05-27T15:45:02.293455Z"}},"outputs":[{"name":"stdout","text":"Train size: 29640\nVal size: 3708\nTest size: 3708\nsearch_query\nvá»‰a hÃ¨ Ä‘Æ°á»ng phá»‘ viá»‡t nam               0.031039\nÄ‘Æ°á»ng phá»‘ sau bÃ£o                       0.023212\nngÆ°á»i Ä‘á»£i xe bus táº¡i tráº¡m               0.023077\nnáº¯ng nÃ³ng Ä‘Æ°á»ng phá»‘ viá»‡t nam            0.022807\nngÆ°á»i Ä‘i bá»™ táº¡i ngÃ£ tÆ°                  0.022267\n                                          ...   \nchÆ°á»›ng ngáº¡i váº­t trÃªn Ä‘Æ°á»ng Ä‘i bá»™        0.004049\ncÃ´ng trÃ¬nh Ä‘Ã o Ä‘Æ°á»ng                    0.003914\nváº¡ch dÃ nh cho ngÆ°á»i khiáº¿m thá»‹           0.002834\nbiá»ƒn bÃ¡o chá»¯ ná»•i cho ngÆ°á»i khiáº¿m thá»‹    0.001754\nlá»‘i Ä‘i dÃ nh cho ngÆ°á»i khuyáº¿t táº­t        0.001350\nName: proportion, Length: 65, dtype: float64\nsearch_query\nvá»‰a hÃ¨ Ä‘Æ°á»ng phá»‘ viá»‡t nam               0.031284\nÄ‘Æ°á»ng phá»‘ sau bÃ£o                       0.023732\nngÆ°á»i Ä‘i bá»™ táº¡i ngÃ£ tÆ°                  0.022654\nnáº¯ng nÃ³ng Ä‘Æ°á»ng phá»‘ viá»‡t nam            0.022654\nngÆ°á»i Ä‘á»£i xe bus táº¡i tráº¡m               0.022654\n                                          ...   \nchÆ°á»›ng ngáº¡i váº­t trÃªn Ä‘Æ°á»ng Ä‘i bá»™        0.004315\ncÃ´ng trÃ¬nh Ä‘Ã o Ä‘Æ°á»ng                    0.003236\nváº¡ch dÃ nh cho ngÆ°á»i khiáº¿m thá»‹           0.003236\nbiá»ƒn bÃ¡o chá»¯ ná»•i cho ngÆ°á»i khiáº¿m thá»‹    0.002157\nlá»‘i Ä‘i dÃ nh cho ngÆ°á»i khuyáº¿t táº­t        0.001079\nName: proportion, Length: 65, dtype: float64\nsearch_query\nvá»‰a hÃ¨ Ä‘Æ°á»ng phá»‘ viá»‡t nam               0.031284\nÄ‘Æ°á»ng phá»‘ sau bÃ£o                       0.023732\nngÆ°á»i Ä‘i bá»™ táº¡i ngÃ£ tÆ°                  0.022654\nnáº¯ng nÃ³ng Ä‘Æ°á»ng phá»‘ viá»‡t nam            0.022654\nngÆ°á»i Ä‘á»£i xe bus táº¡i tráº¡m               0.022654\n                                          ...   \nchÆ°á»›ng ngáº¡i váº­t trÃªn Ä‘Æ°á»ng Ä‘i bá»™        0.004315\ncÃ´ng trÃ¬nh Ä‘Ã o Ä‘Æ°á»ng                    0.003236\nváº¡ch dÃ nh cho ngÆ°á»i khiáº¿m thá»‹           0.003236\nbiá»ƒn bÃ¡o chá»¯ ná»•i cho ngÆ°á»i khiáº¿m thá»‹    0.002157\nlá»‘i Ä‘i dÃ nh cho ngÆ°á»i khuyáº¿t táº­t        0.001079\nName: proportion, Length: 65, dtype: float64\nTrain: 29640, Val: 3708, Test: 3708\nÄÃ£ lÆ°u train.json, val.json, test.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Giáº£ sá»­ báº¡n muá»‘n stratify theo cá»™t 'search_query'\ndf_part1, df_part2 = train_test_split(\n    train_df,\n    test_size=len(all_pkl_data)*3,\n    stratify=train_df['search_query'],\n    random_state=42  # Äáº£m báº£o káº¿t quáº£ reproducible\n)\n\nprint(\"Pháº§n 1:\", df_part1.shape)\nprint(\"Pháº§n 2:\", df_part2.shape)\ntrain_df = df_part2.copy()\n\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\ndf = df.copy().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:02.295503Z","iopub.execute_input":"2025-05-27T15:45:02.295744Z","iopub.status.idle":"2025-05-27T15:45:02.342191Z","shell.execute_reply.started":"2025-05-27T15:45:02.295727Z","shell.execute_reply":"2025-05-27T15:45:02.341404Z"}},"outputs":[{"name":"stdout","text":"Pháº§n 1: (21912, 4)\nPháº§n 2: (7728, 4)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# 4. Load feature extractor","metadata":{}},{"cell_type":"code","source":"feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n\ndef load_image(local_path, dataset_base_path=\"/kaggle/input/traffic-pictures-captioning/\"):\n    \"\"\"\n    Load áº£nh tá»« local_path trong dataset trÃªn Kaggle.\n    \n    Parameters:\n    - local_path (str): ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i cá»§a áº£nh (tá»« cá»™t 'url' trong DataFrame)\n    - dataset_base_path (str): ÄÆ°á»ng dáº«n gá»‘c Ä‘áº¿n dataset\n    \n    Returns:\n    - image_rgb (numpy.ndarray): áº¢nh á»Ÿ Ä‘á»‹nh dáº¡ng RGB, hoáº·c None náº¿u lá»—i\n    \"\"\"\n    try:\n        # Chuáº©n hÃ³a Ä‘Æ°á»ng dáº«n áº£nh\n        local_path = local_path.lstrip('./')  # Loáº¡i bá» './' náº¿u cÃ³\n        full_image_path = os.path.join(dataset_base_path, local_path)\n        \n        # Äá»c áº£nh báº±ng OpenCV\n        image = cv2.imread(full_image_path)\n        if image is None:\n            print(f\"Lá»—i: KhÃ´ng thá»ƒ Ä‘á»c áº£nh tá»« {full_image_path}\")\n            return None\n        \n        # Chuyá»ƒn tá»« BGR sang RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image_rgb\n    \n    except Exception as e:\n        print(f\"Lá»—i khi xá»­ lÃ½ áº£nh {full_image_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:02.601355Z","iopub.execute_input":"2025-05-27T15:45:02.601666Z","iopub.status.idle":"2025-05-27T15:45:02.790527Z","shell.execute_reply.started":"2025-05-27T15:45:02.601644Z","shell.execute_reply":"2025-05-27T15:45:02.789472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdffd6c35124d38af3412274f4ed471"}},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# 5. Load Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer cá»§a BartPho\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n\n# Kiá»ƒm tra vocab size\nvocab_size = tokenizer.vocab_size\nprint(f\"VOCAB SIZE: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:18.227019Z","iopub.execute_input":"2025-05-27T15:45:18.227604Z","iopub.status.idle":"2025-05-27T15:45:20.000991Z","shell.execute_reply.started":"2025-05-27T15:45:18.227580Z","shell.execute_reply":"2025-05-27T15:45:20.000061Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/897 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf7aa29483c4479c8b7eef78a73bca18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7ec007bf00435299b6401bec887dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f30bab348324c8592b959c94a87cc0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576a2a3ed7374d07bc1b56e5d475040f"}},"metadata":{}},{"name":"stdout","text":"VOCAB SIZE: 64000\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# In ra cÃ¡c special tokens\nprint(\"Special Tokens:\", tokenizer.special_tokens_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:22.319248Z","iopub.execute_input":"2025-05-27T15:45:22.319849Z","iopub.status.idle":"2025-05-27T15:45:22.324558Z","shell.execute_reply.started":"2025-05-27T15:45:22.319823Z","shell.execute_reply":"2025-05-27T15:45:22.323716Z"}},"outputs":[{"name":"stdout","text":"Special Tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# 6. CÃ¡c hÃ m tiá»n xá»­ lÃ½ caption","metadata":{}},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    return re.sub(r\"[^\\w\\s,!?.]\", \"\", text).strip()\n\ndef to_lowercase(text: str) -> str:\n    return text.lower()\n\ndef join_vietnamese_compounds(text: str) -> str:\n    return ViTokenizer.tokenize(text)\n\ndef caption_preprocess(text: str) -> str:\n    text = clean_text(text)\n    text = to_lowercase(text)\n    text = join_vietnamese_compounds(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:22.863397Z","iopub.execute_input":"2025-05-27T15:45:22.863740Z","iopub.status.idle":"2025-05-27T15:45:22.869622Z","shell.execute_reply.started":"2025-05-27T15:45:22.863685Z","shell.execute_reply":"2025-05-27T15:45:22.868539Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"text = caption_preprocess(\"Kiá»ƒm tra phÃ¢n tÃ¡ch tá»«\")\n\n# TÃ¡ch token\ntokens = tokenizer.tokenize(text)\n\n# Chuyá»ƒn token thÃ nh ID\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\ntext = tokenizer.decode(token_ids, skip_special_tokens=True)\n# In káº¿t quáº£\nprint(\"List Word (Tokenized):\", tokens)\nprint(\"List Token ID:\", token_ids)\nprint(\"Text:\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:23.323278Z","iopub.execute_input":"2025-05-27T15:45:23.323574Z","iopub.status.idle":"2025-05-27T15:45:23.332196Z","shell.execute_reply.started":"2025-05-27T15:45:23.323555Z","shell.execute_reply":"2025-05-27T15:45:23.331361Z"}},"outputs":[{"name":"stdout","text":"List Word (Tokenized): ['kiá»ƒm_tra', 'phÃ¢n_tÃ¡ch', 'tá»«']\nList Token ID: [342, 31166, 39]\nText: kiá»ƒm_tra phÃ¢n_tÃ¡ch tá»«\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# HÃ m tÃ­nh sá»‘ tá»« cá»§a má»™t caption\ndef count_words(caption):\n    preprocessed_caption = caption_preprocess(caption)\n    tokens = tokenizer.tokenize(preprocessed_caption)\n    return len(tokens)\n\n# Ãp dá»¥ng hÃ m cho toÃ n bá»™ dataframe\ndf['word_count'] = df['caption'].apply(count_words)\n\n# Láº¥y Ä‘á»™ dÃ i lá»›n nháº¥t\nmax_length = df['word_count'].max()\n\n# In ra\nprint(f\"Äá»™ dÃ i caption dÃ i nháº¥t lÃ : {max_length} tá»«\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:25.771370Z","iopub.execute_input":"2025-05-27T15:45:25.771654Z","iopub.status.idle":"2025-05-27T15:45:54.387566Z","shell.execute_reply.started":"2025-05-27T15:45:25.771636Z","shell.execute_reply":"2025-05-27T15:45:54.386572Z"}},"outputs":[{"name":"stdout","text":"Äá»™ dÃ i caption dÃ i nháº¥t lÃ : 71 tá»«\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# 7. Tiá»n xá»­ lÃ½ input model","metadata":{}},{"cell_type":"code","source":"def process_data(image_url, caption):\n    try:\n        img_array = load_image(image_url)\n        if img_array is None:\n            return None\n        \n        pixel_values = feature_extractor(img_array, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        caption = caption_preprocess(caption)\n        tokenized_caption = tokenizer(caption, padding=\"max_length\", max_length=max_length, truncation=True)\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": torch.tensor(tokenized_caption[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(tokenized_caption[\"attention_mask\"])\n        }\n    except Exception as e:\n        print(f\"Error processing data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.388887Z","iopub.execute_input":"2025-05-27T15:45:54.389157Z","iopub.status.idle":"2025-05-27T15:45:54.395032Z","shell.execute_reply.started":"2025-05-27T15:45:54.389131Z","shell.execute_reply":"2025-05-27T15:45:54.394179Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# 8. Táº¡o táº­p dá»¯ liá»‡u huáº¥n luyá»‡n","metadata":{}},{"cell_type":"code","source":"class ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions):\n        self.image_paths = image_paths\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        data = process_data(self.image_paths[idx], self.captions[idx])\n        if data is None:\n            return self.__getitem__((idx + 1) % len(self))\n        return data\n\ndef custom_collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    return {\n        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch])\n    }\n\ntrain_dataset = ImageCaptionDataset(train_df[\"url\"], train_df[\"caption\"])\nval_dataset = ImageCaptionDataset(val_df[\"url\"], val_df[\"caption\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.395804Z","iopub.execute_input":"2025-05-27T15:45:54.396109Z","iopub.status.idle":"2025-05-27T15:45:54.420212Z","shell.execute_reply.started":"2025-05-27T15:45:54.396090Z","shell.execute_reply":"2025-05-27T15:45:54.419246Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"print(len(train_pkl_data))\nprint(train_pkl_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.422200Z","iopub.execute_input":"2025-05-27T15:45:54.422478Z","iopub.status.idle":"2025-05-27T15:45:54.445188Z","shell.execute_reply.started":"2025-05-27T15:45:54.422458Z","shell.execute_reply":"2025-05-27T15:45:54.444200Z"}},"outputs":[{"name":"stdout","text":"2060\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/7edd6547cc8e80c4d02f5f73fab60105.jpg', 'pixel_values': tensor([[[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n          [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n          [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n          ...,\n          [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n          [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n          [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n         [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n          [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n          [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n          ...,\n          [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n          [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n          [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n         [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n          [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n          [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n          ...,\n          [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n          [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n          [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]]), 'input_ids': tensor([[    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n           515,   105,    52,  2227,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(len(train_dataset))\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.446174Z","iopub.execute_input":"2025-05-27T15:45:54.446529Z","iopub.status.idle":"2025-05-27T15:45:54.524742Z","shell.execute_reply.started":"2025-05-27T15:45:54.446502Z","shell.execute_reply":"2025-05-27T15:45:54.523898Z"}},"outputs":[{"name":"stdout","text":"7728\n{'pixel_values': tensor([[[ 0.4510,  0.4588,  0.4667,  ..., -0.7725, -0.6941, -0.7647],\n         [ 0.4588,  0.4667,  0.4667,  ..., -0.6627, -0.6392, -0.7804],\n         [ 0.4667,  0.4667,  0.4745,  ..., -0.6235, -0.7020, -0.8588],\n         ...,\n         [-0.1373, -0.1529, -0.1686,  ..., -0.6471, -0.6157, -0.6000],\n         [-0.1765, -0.1529, -0.1529,  ..., -0.5451, -0.5373, -0.5765],\n         [-0.1765, -0.1529, -0.1608,  ..., -0.3569, -0.3176, -0.3647]],\n\n        [[ 0.5137,  0.5216,  0.5294,  ..., -0.7333, -0.6549, -0.7255],\n         [ 0.5216,  0.5294,  0.5294,  ..., -0.6235, -0.6000, -0.7412],\n         [ 0.5294,  0.5294,  0.5373,  ..., -0.5843, -0.6627, -0.8196],\n         ...,\n         [-0.1294, -0.1451, -0.1608,  ..., -0.6235, -0.5922, -0.5765],\n         [-0.1686, -0.1451, -0.1451,  ..., -0.5059, -0.4980, -0.5373],\n         [-0.1686, -0.1451, -0.1529,  ..., -0.3098, -0.2706, -0.3255]],\n\n        [[ 0.5294,  0.5373,  0.5451,  ..., -0.7098, -0.6314, -0.7020],\n         [ 0.5373,  0.5451,  0.5451,  ..., -0.6000, -0.5765, -0.7176],\n         [ 0.5451,  0.5451,  0.5529,  ..., -0.5608, -0.6392, -0.7961],\n         ...,\n         [-0.0980, -0.1137, -0.1294,  ..., -0.6157, -0.5922, -0.5765],\n         [-0.1373, -0.1137, -0.1137,  ..., -0.4980, -0.4980, -0.5294],\n         [-0.1373, -0.1137, -0.1216,  ..., -0.2941, -0.2549, -0.3020]]]), 'input_ids': tensor([   0,  448, 7416,   91,  368,  888, 4968,  109,    5, 8994,   17,  297,\n           5, 1690, 2633,  216,   71,    5,  105, 2786,    6,  820,  145,   41,\n           5,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Class táº¡o Dataset tá»« list cÃ¡c dict\nclass DatasetFromList(Dataset):\n    def __init__(self, data_list):\n        self.data_list = data_list\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, idx):\n        return self.data_list[idx]\n\n# HÃ m táº¡o DatasetFromList tá»« list dict\ndef dataset_from_list(data_list):\n    return DatasetFromList(data_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.525597Z","iopub.execute_input":"2025-05-27T15:45:54.525853Z","iopub.status.idle":"2025-05-27T15:45:54.531095Z","shell.execute_reply.started":"2025-05-27T15:45:54.525835Z","shell.execute_reply":"2025-05-27T15:45:54.530288Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"new_data = []\n\nfor item in train_pkl_data:\n    if \"image_url\" in item:\n        del item[\"image_url\"]\n\n    if \"pixel_values\" in item:\n        item[\"pixel_values\"] = item[\"pixel_values\"].squeeze(0)\n    \n    # Xá»­ lÃ½ input_ids vÃ  attention_mask: squeeze rá»“i pad/cáº¯t\n    if \"input_ids\" in item and \"attention_mask\" in item:\n        input_ids = item[\"input_ids\"].squeeze(0)\n        attention_mask = item[\"attention_mask\"].squeeze(0)\n\n        # Chuyá»ƒn tensor sang list Ä‘á»ƒ dá»… xá»­ lÃ½\n        input_ids_list = input_ids.tolist()\n        attention_mask_list = attention_mask.tolist()\n\n        # Pad hoáº·c truncate cho input_ids\n        if len(input_ids_list) < max_length:\n            pad_length = max_length - len(input_ids_list)\n            input_ids_list += [1] * pad_length          # pad vá»›i 1 (token id padding)\n            attention_mask_list += [0] * pad_length     # pad mask báº±ng 0\n        else:\n            input_ids_list = input_ids_list[:max_length]\n            attention_mask_list = attention_mask_list[:max_length]\n\n        # Chuyá»ƒn láº¡i thÃ nh tensor\n        import torch\n        item[\"input_ids\"] = torch.tensor(input_ids_list)\n        item[\"attention_mask\"] = torch.tensor(attention_mask_list)\n\n    new_data.append(item)\n\n\ncleaned_dataset = dataset_from_list(new_data)\nprint(cleaned_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.532105Z","iopub.execute_input":"2025-05-27T15:45:54.532397Z","iopub.status.idle":"2025-05-27T15:45:54.670708Z","shell.execute_reply.started":"2025-05-27T15:45:54.532372Z","shell.execute_reply":"2025-05-27T15:45:54.669856Z"}},"outputs":[{"name":"stdout","text":"{'pixel_values': tensor([[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n         [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n         [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n         ...,\n         [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n         [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n         [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n        [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n         [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n         [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n         ...,\n         [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n         [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n         [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n        [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n         [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n         [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n         ...,\n         [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n         [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n         [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]), 'input_ids': tensor([    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n          515,   105,    52,  2227,     5,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"GhÃ©p dataset cho dá»¯ liá»‡u cÅ© vÃ  dá»¯ liá»‡u má»›i tá»« API","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\n\ntrain_dataset = ConcatDataset([cleaned_dataset, train_dataset])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.671515Z","iopub.execute_input":"2025-05-27T15:45:54.671738Z","iopub.status.idle":"2025-05-27T15:45:54.676771Z","shell.execute_reply.started":"2025-05-27T15:45:54.671720Z","shell.execute_reply":"2025-05-27T15:45:54.675751Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(len(train_dataset))\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.677645Z","iopub.execute_input":"2025-05-27T15:45:54.677952Z","iopub.status.idle":"2025-05-27T15:45:54.698751Z","shell.execute_reply.started":"2025-05-27T15:45:54.677925Z","shell.execute_reply":"2025-05-27T15:45:54.697863Z"}},"outputs":[{"name":"stdout","text":"9788\n{'pixel_values': tensor([[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n         [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n         [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n         ...,\n         [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n         [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n         [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n        [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n         [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n         [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n         ...,\n         [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n         [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n         [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n        [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n         [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n         [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n         ...,\n         [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n         [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n         [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]), 'input_ids': tensor([    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n          515,   105,    52,  2227,     5,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# 9. Cáº¥u hÃ¬nh LoRA","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom peft import PeftModel\n\n# Cáº¥u hÃ¬nh LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],  # CÃ¡c lá»›p trong GPT2\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:31.246506Z","iopub.execute_input":"2025-05-27T15:02:31.246782Z","iopub.status.idle":"2025-05-27T15:02:31.756331Z","shell.execute_reply.started":"2025-05-27T15:02:31.246763Z","shell.execute_reply":"2025-05-27T15:02:31.755473Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"# 10. Äá»‹nh nghÄ©a mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\ndel vit_model.classifier\nvit_model.vit.pooler = torch.nn.Sequential(OrderedDict([\n    ('dense', torch.nn.Linear(in_features=768, out_features=768, bias=True)),\n    ('activation', torch.nn.Tanh())\n]))\n\nconfig = GPT2Config.from_pretrained(\"gpt2\")\nconfig.add_cross_attention = True\nconfig.vocab_size = vocab_size\n\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config, ignore_mismatched_sizes=True)\ngpt2_model.resize_token_embeddings(config.vocab_size)\n\n# Ãp dá»¥ng LoRA vÃ o GPT2\ngpt2_model = get_peft_model(gpt2_model, lora_config)\ngpt2_model.print_trainable_parameters()  # kiá»ƒm tra sá»‘ lÆ°á»£ng tham sá»‘ cáº§n huáº¥n luyá»‡n\n\nmodel = VisionEncoderDecoderModel(encoder=vit_model.vit, decoder=gpt2_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:31.757235Z","iopub.execute_input":"2025-05-27T15:02:31.757480Z","iopub.status.idle":"2025-05-27T15:02:40.773661Z","shell.execute_reply.started":"2025-05-27T15:02:31.757461Z","shell.execute_reply":"2025-05-27T15:02:40.772765Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dc24359cb274bb0ba07b812455f7a43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299bf316cdf24bf0aa392654336d3a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec49cfa60f64f9dba752b2dbbc81be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd40acd3e824c6c8ab20528659a1384"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized because the shapes did not match:\n- transformer.wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([64000, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10748bea4b384952a620837455f5bfc1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,179,648 || all params: 164,540,928 || trainable%: 0.7169\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# 11. Äá»‹nh nghÄ©a lá»›p mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"class LoRACaptionWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    def load_context(self, context):\n        try:\n            # ÄÆ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n cÃ¡c thÆ° má»¥c con trong artifact\n            model_path = context.artifacts[\"model_dir\"]\n            tokenizer_path = os.path.join(model_path, \"tokenizer\")\n            feature_extractor_path = os.path.join(model_path, \"feature_extractor\")\n\n            \n            # Load cÃ¡c thÃ nh pháº§n vá»›i Ä‘Æ°á»ng dáº«n chÃ­nh xÃ¡c\n            self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            self.feature_extractor = ViTImageProcessor.from_pretrained(feature_extractor_path)\n            \n            # Äáº·t model vÃ o cháº¿ Ä‘á»™ eval\n            self.model.eval()\n        except Exception as e:\n            raise ValueError(f\"Error loading model: {str(e)}\")\n\n    def predict(self, context, model_input):\n        try:\n            # Xá»­ lÃ½ Ä‘áº§u vÃ o\n            if isinstance(model_input, dict):\n                image_url = model_input[\"url\"][0] if \"url\" in model_input else model_input[\"image_path\"][0]\n            else:\n                image_url = model_input.iloc[0][\"url\"] if \"url\" in model_input.columns else model_input.iloc[0][\"image_path\"]\n            \n            image = self.load_image(image_url)\n            if image is None:\n                return [\"\"]\n                \n            # Tiá»n xá»­ lÃ½ áº£nh vÃ  táº¡o caption\n            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n            \n            output_ids = self.model.generate(\n                pixel_values,\n                max_length=45,\n                min_length=20,\n                num_beams=4,\n                do_sample=True,\n                temperature=0.8,\n                top_k=20,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                repetition_penalty=2.0,\n                early_stopping=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                decoder_start_token_id=self.tokenizer.bos_token_id\n            )\n            \n            caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            return [caption.replace(\"_\", \" \").strip()]\n            \n        except Exception as e:\n            print(f\"[ERROR] during prediction: {e}\")\n            return [\"\"]\n   \n    def load_image(self, image_url):\n        try:\n            import requests\n            import numpy as np\n            from PIL import Image\n            from io import BytesIO\n            \n            response = requests.get(image_url, timeout=10)\n            if response.status_code != 200:\n                return None\n                \n            image = Image.open(BytesIO(response.content))\n            return image.convert(\"RGB\") if image else None\n            \n        except Exception as e:\n            print(f\"[ERROR] loading image: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.774637Z","iopub.execute_input":"2025-05-27T15:02:40.774955Z","iopub.status.idle":"2025-05-27T15:02:40.896513Z","shell.execute_reply.started":"2025-05-27T15:02:40.774923Z","shell.execute_reply":"2025-05-27T15:02:40.895442Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n  color_warning(\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# 12. Huáº¥n luyá»‡n mÃ´ hÃ¬nh","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ndef evaluate_model(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            encoder_outputs = model.encoder(pixel_values=pixel_values)\n            outputs = model.decoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                encoder_hidden_states=encoder_outputs.last_hidden_state,\n                labels=input_ids\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n    return total_loss / len(val_dataloader)\n\ndef train_model(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr=5e-5, epochs=1, epoch_count=1, model_name='BartPho_ViT_GPT2_LoRA_ICG', data_part=\"Part_1\", df_log=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Optimizer chá»‰ update cÃ¡c tham sá»‘ cÃ³ requires_grad=True (LoRA adapter)\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Thá»‘ng kÃª tham sá»‘ trainable\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n\n    # Láº¥y ngÃ y hiá»‡n táº¡i theo format dd-mm-yyyy\n    today_str = datetime.now().strftime(\"%d-%m-%Y\")\n    run_name = f'{model_name}_{today_str}_{data_part}'\n\n    with mlflow.start_run(run_name=run_name):\n        if df_log is not None:\n            dataset = mlflow.data.from_pandas(df_log, targets=\"caption\")\n            mlflow.log_input(dataset, context=\"training\")\n\n        # Logging cÃ¡c siÃªu tham sá»‘\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"epoch_count\", epoch_count)\n        mlflow.log_param(\"data_part\", data_part)\n        mlflow.log_param(\"learning_rate\", lr)\n        mlflow.log_param(\"train_data_size\", len(train_dataloader.dataset))\n        mlflow.log_param(\"val_data_size\", len(val_dataloader.dataset))\n        mlflow.log_param(\"trainable_params\", trainable_params)\n        mlflow.log_param(\"total_params\", total_params)\n        mlflow.log_param(\"trainable_ratio\", round(trainable_params / total_params, 4))\n        mlflow.log_param(\"model_name\", model_name)\n\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch in train_dataloader:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n\n                encoder_outputs = model.encoder(pixel_values=pixel_values)\n                outputs = model.decoder(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_outputs.last_hidden_state,\n                    labels=input_ids\n                )\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_loss = total_loss / len(train_dataloader)\n            val_loss = evaluate_model(model, val_dataloader, device)\n\n            # Log loss theo epoch\n            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n            print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n            # LÆ°u mÃ´ hÃ¬nh sau má»—i epoch\n            epoch_save_path = f\"/kaggle/working/{model_name}_epoch_{epoch+1}\"\n            model.save_pretrained(epoch_save_path)\n            tokenizer.save_pretrained(os.path.join(epoch_save_path, \"tokenizer\"))\n            feature_extractor.save_pretrained(os.path.join(epoch_save_path, \"feature_extractor\"))\n            \n            # Log toÃ n bá»™ thÆ° má»¥c nhÆ° má»™t artifact\n            mlflow.log_artifacts(epoch_save_path, artifact_path=f\"epoch_{epoch+1}\")\n\n        # LÆ°u model cuá»‘i cÃ¹ng riÃªng biá»‡t\n        final_model_path = f\"/kaggle/working/{model_name}_final\"\n        model.save_pretrained(final_model_path)\n        tokenizer.save_pretrained(os.path.join(final_model_path, \"tokenizer\"))\n        feature_extractor.save_pretrained(os.path.join(final_model_path, \"feature_extractor\"))\n        mlflow.log_artifacts(final_model_path, artifact_path=\"final_model\")\n\n        # Log model dÆ°á»›i dáº¡ng pyfunc (náº¿u cáº§n)\n        # Äáº£m báº£o báº¡n Ä‘Ã£ Ä‘á»‹nh nghÄ©a class ImageCaptionModel trÆ°á»›c\n\n        # Infer signature\n        input_example = pd.DataFrame({\"url\": [\"http://example.com/test.jpg\"]})\n        output_example = pd.DataFrame({\"caption\": [\"MÃ´ táº£ áº£nh\"]})\n        \n        # 2. Táº¡o model signature\n        signature = infer_signature(\n            input_example,\n            output_example,\n            params={\"input_types\": \"string\", \"output_types\": \"string\"}  # Chá»‰ Ä‘á»‹nh kiá»ƒu dá»¯ liá»‡u\n        )\n        mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=LoRACaptionWrapper(),\n            artifacts={\"model_dir\": final_model_path},  # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a model, tokenizer vÃ  feature_extractor\n            registered_model_name=model_name,\n            signature=signature,\n            # input_example=input_example,\n            pip_requirements=pip_reqs\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.897571Z","iopub.execute_input":"2025-05-27T15:02:40.897837Z","iopub.status.idle":"2025-05-27T15:02:40.926123Z","shell.execute_reply.started":"2025-05-27T15:02:40.897817Z","shell.execute_reply":"2025-05-27T15:02:40.924956Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Táº¡o k Subset cho train, val\ntrain_subset = Subset(train_dataset, range(len(train_dataset)))  # Láº¥y train_df.shape[0] dá»¯ liá»‡u Ä‘áº§u tiÃªn tá»« train_dataset\nval_subset = Subset(val_dataset, range(val_df.shape[0]))     # Láº¥y 4620 dá»¯ liá»‡u Ä‘áº§u tiÃªn tá»« val_dataset\n\n# Táº¡o DataLoader cho cÃ¡c subset\ntrain_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\nval_dataloader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.927156Z","iopub.execute_input":"2025-05-27T15:02:40.927422Z","iopub.status.idle":"2025-05-27T15:02:40.939827Z","shell.execute_reply.started":"2025-05-27T15:02:40.927401Z","shell.execute_reply":"2025-05-27T15:02:40.938835Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# 13. Load model pretrain","metadata":{}},{"cell_type":"code","source":"# ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a cÃ¡c tá»‡p mÃ´ hÃ¬nh\nmodel_path = '/kaggle/input/phovit-gptcap/pytorch/default/15'\n\n# Táº£i mÃ´ hÃ¬nh VisionEncoderDecoder\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path, ignore_mismatched_sizes=True)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.940841Z","iopub.execute_input":"2025-05-27T15:02:40.941206Z","iopub.status.idle":"2025-05-27T15:02:46.819404Z","shell.execute_reply.started":"2025-05-27T15:02:40.941182Z","shell.execute_reply":"2025-05-27T15:02:46.818326Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# 14. Huáº¥n luyá»‡n","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\ntrain_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    tokenizer,\n    feature_extractor,\n    lr=5e-5,\n    epochs=2,\n    epoch_count=17,\n    model_name=\"BartPho_ViT_GPT2_LoRA_ICG_Incremental\",\n    data_part=\"FULL\",\n    df_log=df\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:46.820508Z","iopub.execute_input":"2025-05-27T15:02:46.820814Z","iopub.status.idle":"2025-05-27T15:03:35.521995Z","shell.execute_reply.started":"2025-05-27T15:02:46.820788Z","shell.execute_reply":"2025-05-27T15:03:35.520364Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"ğŸƒ View run BartPho_ViT_GPT2_LoRA_ICG Incremental_27-05-2025_FULL at: http://36.50.135.226:7893/#/experiments/10/runs/6439b7c1809f469f924ebca8c6be1057\nğŸ§ª View experiment at: http://36.50.135.226:7893/#/experiments/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/821225077.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr, epochs, epoch_count, model_name, data_part, df_log)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    783\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/842534181.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/821225077.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr, epochs, epoch_count, model_name, data_part, df_log)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{model_name}_{today_str}_{data_part}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf_log\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"caption\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive_run_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFINISHED\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mend_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mend_run\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mlast_active_run_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0m_last_active_run_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_active_run_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_terminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_active_run_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlast_active_run_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_id_to_system_metrics_monitor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0msystem_metrics_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_id_to_system_metrics_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_active_run_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mset_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m   3353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3354\u001b[0m         \"\"\"\n\u001b[0;32m-> 3355\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_terminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mset_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshut_down_async_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         self.store.update_run_info(\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mrun_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36mupdate_run_info\u001b[0;34m(self, run_id, run_status, end_time, run_name)\u001b[0m\n\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUpdateRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36m_call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_METHOD_TO_INFO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_host_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     def search_experiments(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_rest_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0mresponse_to_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0mjs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_to_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_parse_as_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             base_msg = (\n","\u001b[0;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: The run 6439b7c1809f469f924ebca8c6be1057 must be in the 'active' state. Current state is deleted."],"ename":"RestException","evalue":"INVALID_PARAMETER_VALUE: The run 6439b7c1809f469f924ebca8c6be1057 must be in the 'active' state. Current state is deleted.","output_type":"error"}],"execution_count":58}]}