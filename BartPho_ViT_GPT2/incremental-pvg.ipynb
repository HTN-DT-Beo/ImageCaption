{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11334502,"sourceType":"datasetVersion","datasetId":6723117},{"sourceId":11337142,"sourceType":"datasetVersion","datasetId":7085417,"isSourceIdPinned":true},{"sourceId":331865,"sourceType":"modelInstanceVersion","modelInstanceId":277580,"modelId":298468},{"sourceId":374794,"sourceType":"modelInstanceVersion","modelInstanceId":307843,"modelId":328293}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow pyvi minio -q\n!pip install hf_xet -q\n\nimport mlflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:09.725198Z","iopub.execute_input":"2025-05-27T15:13:09.725427Z","iopub.status.idle":"2025-05-27T15:13:31.885051Z","shell.execute_reply.started":"2025-05-27T15:13:09.725408Z","shell.execute_reply":"2025-05-27T15:13:31.884087Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.9/722.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"mlflow.set_tracking_uri(\"http://36.50.135.226:7893/\")\n\n# from mlflow.tracking import MlflowClient\n\n# client = MlflowClient()\n# client.restore_experiment(experiment_id=\"2\")\n\nmlflow.set_experiment(experiment_id=\"10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:31.886112Z","iopub.execute_input":"2025-05-27T15:13:31.887305Z","iopub.status.idle":"2025-05-27T15:13:32.402294Z","shell.execute_reply.started":"2025-05-27T15:13:31.887271Z","shell.execute_reply":"2025-05-27T15:13:32.401511Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='s3://mlflow/6', creation_time=1745743711248, experiment_id='10', last_update_time=1745743711248, lifecycle_stage='active', name='ImageCaption_TPC37k_BartPho-ViT-GPT2_LoRALayerFT', tags={}>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\n\nos.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://36.50.135.226:9000\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:32.404542Z","iopub.execute_input":"2025-05-27T15:13:32.404895Z","iopub.status.idle":"2025-05-27T15:13:32.409022Z","shell.execute_reply.started":"2025-05-27T15:13:32.404871Z","shell.execute_reply":"2025-05-27T15:13:32.408188Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# MinIO","metadata":{}},{"cell_type":"code","source":"from minio import Minio\nfrom minio.error import S3Error\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:32.409793Z","iopub.execute_input":"2025-05-27T15:13:32.410007Z","iopub.status.idle":"2025-05-27T15:13:32.850864Z","shell.execute_reply.started":"2025-05-27T15:13:32.409989Z","shell.execute_reply":"2025-05-27T15:13:32.849949Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\nimport transformers\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor\nimport torch\nfrom PIL import Image\nimport torch.nn as nn\nimport cv2\nimport torchvision\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom collections import OrderedDict\nfrom transformers import GPT2Config, GPT2LMHeadModel\nimport mlflow\nfrom mlflow.models import infer_signature\nimport mlflow.pytorch\nimport re\nfrom pyvi import ViTokenizer\nfrom torch.optim import AdamW\nfrom datetime import datetime\nimport json\nimport pickle\nfrom io import BytesIO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:13:32.851848Z","iopub.execute_input":"2025-05-27T15:13:32.852195Z","iopub.status.idle":"2025-05-27T15:14:05.118848Z","shell.execute_reply.started":"2025-05-27T15:13:32.852166Z","shell.execute_reply":"2025-05-27T15:14:05.118056Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 15:13:48.716264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748358828.967395      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748358829.035919      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 1. Chuẩn bị file thư viện","metadata":{}},{"cell_type":"code","source":"pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:14:05.119791Z","iopub.execute_input":"2025-05-27T15:14:05.120395Z","iopub.status.idle":"2025-05-27T15:14:07.142380Z","shell.execute_reply.started":"2025-05-27T15:14:05.120371Z","shell.execute_reply":"2025-05-27T15:14:07.141117Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"with open(\"requirements.txt\") as f:\n    pip_reqs = [line.strip() for line in f if line.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:14:07.143460Z","iopub.execute_input":"2025-05-27T15:14:07.143814Z","iopub.status.idle":"2025-05-27T15:14:07.150205Z","shell.execute_reply.started":"2025-05-27T15:14:07.143776Z","shell.execute_reply":"2025-05-27T15:14:07.149111Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 2. Load data","metadata":{}},{"cell_type":"markdown","source":"## API Data","metadata":{}},{"cell_type":"code","source":"# 1. Gửi GET request đến API metadata\nurl = \"http://36.50.135.226/api/v1/metadata/encoded-data\"\nheaders = {\"accept\": \"application/json\"}\nresponse = requests.get(url, headers=headers)\n\n# 2. Kiểm tra phản hồi\nif response.status_code == 200:\n    data = response.json()\n    object_keys = data.get(\"object_keys\", [])\n    def extract_timestamp(key):\n        match = re.search(r\"encoded_data_(\\d{14})\\.pkl\", key)\n        if match:\n            return datetime.strptime(match.group(1), \"%Y%m%d%H%M%S\")\n        return None\n    if object_keys:\n        base_url = \"http://160.191.244.13:9000/lakehouse/\"\n        all_pkl_data = []  # Biến chứa toàn bộ dữ liệu\n        for key in sorted(object_keys, key=extract_timestamp):\n            file_url = base_url + key\n            file_response = requests.get(file_url)\n            if file_response.status_code == 200:\n                try:\n                    file_bytes = BytesIO(file_response.content)\n                    pkl_data = pickle.load(file_bytes)\n                    # Gộp dữ liệu\n                    if isinstance(pkl_data, list):\n                        all_pkl_data.extend(pkl_data)\n                    elif isinstance(pkl_data, dict):\n                        all_pkl_data.append(pkl_data)\n                    else:\n                        all_pkl_data.append(pkl_data)\n                    print(f\"✅ Đã xử lý: {key}\")\n                except Exception as e:\n                    print(f\"⚠️ Lỗi đọc file {key}: {e}\")\n            else:\n                print(f\"❌ Không thể tải file: {key}\")\n        # ✅ In ra tổng số phần tử đã gộp\n        print(f\"\\n📦 Tổng số đối tượng đã gộp: {len(all_pkl_data)}\")\n    else:\n        print(\"Không có object_keys nào trong dữ liệu.\")\nelse:\n    print(\"Lỗi khi gọi API:\", response.status_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:14:07.151250Z","iopub.execute_input":"2025-05-27T15:14:07.151590Z","iopub.status.idle":"2025-05-27T15:32:20.643835Z","shell.execute_reply.started":"2025-05-27T15:14:07.151562Z","shell.execute_reply":"2025-05-27T15:32:20.642827Z"}},"outputs":[{"name":"stdout","text":"✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145536.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145620.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145657.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145708.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145719.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145731.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145744.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145755.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145809.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145821.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145832.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145846.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145859.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145913.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145925.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145938.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524145952.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150005.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150016.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150027.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150039.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150051.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150104.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150115.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150130.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150143.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150156.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150209.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150223.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150236.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150250.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150305.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150319.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150331.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150343.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150356.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150408.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150419.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150431.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150443.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150456.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150510.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150522.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150536.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150549.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150602.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150617.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150630.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150643.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150655.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150708.pkl\n✅ Đã xử lý: imcp/encoded-data/2025-05-24/encoded_data_20250524150719.pkl\n\n📦 Tổng số đối tượng đã gộp: 2576\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(len(all_pkl_data))\nprint(all_pkl_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:32:20.646883Z","iopub.execute_input":"2025-05-27T15:32:20.647183Z","iopub.status.idle":"2025-05-27T15:32:20.703912Z","shell.execute_reply.started":"2025-05-27T15:32:20.647161Z","shell.execute_reply":"2025-05-27T15:32:20.703078Z"}},"outputs":[{"name":"stdout","text":"2576\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/6e0a007b66b69f7c9aacb9b46606f0ed.jpg', 'pixel_values': tensor([[[[-0.9059, -0.7725, -0.7333,  ...,  0.0275, -0.0039, -0.0118],\n          [-0.8196, -0.6471, -0.5843,  ...,  0.0196,  0.0039, -0.0196],\n          [-0.6392, -0.6706, -0.6549,  ..., -0.0118, -0.0275, -0.0431],\n          ...,\n          [-0.2627, -0.2471, -0.2549,  ...,  0.1216,  0.2627,  0.2471],\n          [-0.2549, -0.2314, -0.2392,  ...,  0.4275,  0.4745,  0.3804],\n          [-0.2549, -0.2314, -0.2235,  ..., -0.0431, -0.0353, -0.0353]],\n\n         [[-0.8902, -0.7490, -0.7098,  ..., -0.3176, -0.3412, -0.3647],\n          [-0.8039, -0.6235, -0.5608,  ..., -0.3412, -0.3569, -0.3804],\n          [-0.6157, -0.6549, -0.6314,  ..., -0.3569, -0.3647, -0.3882],\n          ...,\n          [-0.4745, -0.4745, -0.4824,  ..., -0.0667,  0.0745,  0.0745],\n          [-0.4824, -0.4824, -0.4980,  ...,  0.2863,  0.3412,  0.2627],\n          [-0.4745, -0.4902, -0.4902,  ..., -0.2157, -0.2000, -0.2078]],\n\n         [[-0.9765, -0.8745, -0.8745,  ..., -0.7647, -0.7725, -0.7725],\n          [-0.8824, -0.7569, -0.7333,  ..., -0.7647, -0.7647, -0.7882],\n          [-0.7176, -0.7882, -0.8039,  ..., -0.7569, -0.7647, -0.7961],\n          ...,\n          [-0.6941, -0.6784, -0.6706,  ..., -0.1686,  0.0039,  0.0039],\n          [-0.6941, -0.6627, -0.6627,  ...,  0.2784,  0.3569,  0.2627],\n          [-0.6863, -0.6627, -0.6392,  ..., -0.2941, -0.2941, -0.3412]]]]), 'input_ids': tensor([[   0, 7069,  140, 1867,  109,    4,  715,  448,  330,    5,  105, 2786,\n           52,  132,    5,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import random\n\n# Giả sử all_pkl_data là một list chứa toàn bộ dữ liệu\nprint(f\"Tổng số dữ liệu: {len(all_pkl_data)}\")  # 2576\n\n# 1. Shuffle để ngẫu nhiên hóa thứ tự (nếu cần)\nrandom.shuffle(all_pkl_data)\n\n# 2. Tính số lượng phần tử cho tập train\ntrain_size = int(0.8 * len(all_pkl_data))  # 80%\n\n# 3. Chia dữ liệu\ntrain_pkl_data = all_pkl_data[:train_size]\ntest_pkl_data = all_pkl_data[train_size:]\n\n# 4. In kiểm tra\nprint(f\"Số lượng train: {len(train_pkl_data)}\")  # ~2060\nprint(f\"Số lượng test: {len(test_pkl_data)}\")    # ~516\n\n# ✅ In mẫu một phần tử\nprint(\"📄 Mẫu train:\")\nprint(train_pkl_data[0])\nprint(\"📄 Mẫu test:\")\nprint(test_pkl_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:32:20.704982Z","iopub.execute_input":"2025-05-27T15:32:20.705246Z","iopub.status.idle":"2025-05-27T15:32:20.719915Z","shell.execute_reply.started":"2025-05-27T15:32:20.705226Z","shell.execute_reply":"2025-05-27T15:32:20.718744Z"}},"outputs":[{"name":"stdout","text":"Tổng số dữ liệu: 2576\nSố lượng train: 2060\nSố lượng test: 516\n📄 Mẫu train:\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/7edd6547cc8e80c4d02f5f73fab60105.jpg', 'pixel_values': tensor([[[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n          [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n          [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n          ...,\n          [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n          [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n          [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n         [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n          [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n          [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n          ...,\n          [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n          [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n          [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n         [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n          [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n          [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n          ...,\n          [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n          [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n          [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]]), 'input_ids': tensor([[    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n           515,   105,    52,  2227,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n📄 Mẫu test:\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/d4e042f7a85d9082120dcd4e3de58a83_0.jpg', 'pixel_values': tensor([[[[-0.3647, -0.3804, -0.3961,  ..., -0.0431, -0.0431, -0.0275],\n          [-0.3490, -0.3725, -0.3647,  ..., -0.0196, -0.0431, -0.0196],\n          [-0.3333, -0.3490, -0.3569,  ..., -0.0196, -0.0431, -0.0039],\n          ...,\n          [ 0.2314,  0.1843,  0.0510,  ...,  0.3098,  0.3333,  0.2863],\n          [ 0.2314,  0.0902, -0.0667,  ...,  0.3176,  0.3647,  0.2157],\n          [ 0.3098,  0.3255,  0.2549,  ...,  0.3804,  0.3098,  0.3333]],\n\n         [[-0.5216, -0.5373, -0.5451,  ..., -0.0510, -0.0588, -0.0353],\n          [-0.5059, -0.5294, -0.5137,  ..., -0.0353, -0.0510, -0.0196],\n          [-0.4902, -0.5059, -0.5137,  ..., -0.0196, -0.0353,  0.0118],\n          ...,\n          [ 0.1608,  0.1137, -0.0275,  ...,  0.3176,  0.3490,  0.3098],\n          [ 0.1294, -0.0118, -0.1765,  ...,  0.3255,  0.3725,  0.2235],\n          [ 0.1843,  0.2000,  0.1294,  ...,  0.3961,  0.3176,  0.3412]],\n\n         [[-0.5765, -0.6000, -0.6235,  ..., -0.1216, -0.0510, -0.0039],\n          [-0.5608, -0.6000, -0.5922,  ..., -0.0980, -0.0902, -0.0431],\n          [-0.5529, -0.5765, -0.5922,  ..., -0.0745, -0.1216, -0.0902],\n          ...,\n          [ 0.0667,  0.0196, -0.1216,  ...,  0.2627,  0.3098,  0.2706],\n          [ 0.0118, -0.1059, -0.2471,  ...,  0.2784,  0.3333,  0.2000],\n          [ 0.0667,  0.1137,  0.0745,  ...,  0.3412,  0.2784,  0.3176]]]]), 'input_ids': tensor([[    0,   679,   103,   140,  1867,     4,   715,  1263,   330,     5,\n            18,   452,  1036, 16993,     4,    57,   981,    12,    58,  1867,\n             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from datetime import datetime\nimport pickle\n\nprint(type(test_pkl_data[0]))\n\n# Lấy ngày hiện tại theo định dạng dd_mm_yy\ntoday_str = datetime.now().strftime(\"%d_%m_%y\")\n\n# Tạo tên file có chứa ngày\ntest_pkl_filename = f\"{today_str}_test_pkl_data.pkl\"\n\n# Lưu file\nwith open(test_pkl_filename, \"wb\") as f:\n    pickle.dump(test_pkl_data, f)\n\nprint(f\"✅ Đã lưu file test: '{test_pkl_filename}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:32:20.720890Z","iopub.execute_input":"2025-05-27T15:32:20.721238Z","iopub.status.idle":"2025-05-27T15:32:21.772892Z","shell.execute_reply.started":"2025-05-27T15:32:20.721213Z","shell.execute_reply":"2025-05-27T15:32:21.771927Z"}},"outputs":[{"name":"stdout","text":"<class 'dict'>\n✅ Đã lưu file test: '27_05_25_test_pkl_data.pkl'\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Colected Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef load_data(data_path=\"/kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv\"):\n    try:\n        df = pd.read_csv(data_path)\n        df = df[['original_url','local_path', 'search_query', 'short_caption']].rename(columns={\n            'original_url': 'original_url',\n            'local_path': 'url',\n            'search_query': 'search_query',\n            'short_caption': 'caption'\n        })\n        print(f\"Đã tải CSV từ: {data_path} (Kích thước: {df.shape})\")\n        return df\n    except FileNotFoundError:\n        print(f\"Lỗi: Không tìm thấy file CSV tại {data_path}\")\n        raise\n    except Exception as e:\n        print(f\"Lỗi khi đọc CSV: {e}\")\n        raise\n\n# Load dữ liệu\ndf = load_data()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:01.426920Z","iopub.execute_input":"2025-05-27T15:45:01.427283Z","iopub.status.idle":"2025-05-27T15:45:01.741470Z","shell.execute_reply.started":"2025-05-27T15:45:01.427260Z","shell.execute_reply":"2025-05-27T15:45:01.740712Z"}},"outputs":[{"name":"stdout","text":"Đã tải CSV từ: /kaggle/input/traffic-pictures-captioning/augmented/captions_augmented.csv (Kích thước: (37056, 4))\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# 3. Split data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Hàm chia dữ liệu giữ nguyên\ndef split_data(df, stratify_col='search_query', test_size=0.1, val_size=0.1, random_state=42):\n    unique_urls = df.drop_duplicates('original_url')\n\n    sss_1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    for train_val_idx, test_idx in sss_1.split(unique_urls, unique_urls[stratify_col]):\n        train_val_urls = unique_urls.iloc[train_val_idx]['original_url']\n        test_urls = unique_urls.iloc[test_idx]['original_url']\n\n    df_train_val = df[df['original_url'].isin(train_val_urls)]\n    df_test = df[df['original_url'].isin(test_urls)]\n\n    unique_train_val_urls = df_train_val.drop_duplicates('original_url')\n    val_ratio = val_size / (1 - test_size)\n    sss_2 = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=random_state)\n    for train_idx, val_idx in sss_2.split(unique_train_val_urls, unique_train_val_urls[stratify_col]):\n        train_urls = unique_train_val_urls.iloc[train_idx]['original_url']\n        val_urls = unique_train_val_urls.iloc[val_idx]['original_url']\n\n    df_train = df_train_val[df_train_val['original_url'].isin(train_urls)]\n    df_val = df_train_val[df_train_val['original_url'].isin(val_urls)]\n\n    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:01.855280Z","iopub.execute_input":"2025-05-27T15:45:01.855594Z","iopub.status.idle":"2025-05-27T15:45:01.863422Z","shell.execute_reply.started":"2025-05-27T15:45:01.855573Z","shell.execute_reply":"2025-05-27T15:45:01.862532Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def load_split_save(df: pd.DataFrame): \n    # Split\n    train_df, val_df, test_df = split_data(df, stratify_col='search_query')\n    \n    # Kiểm tra\n    print(\"Train size:\", len(train_df))\n    print(\"Val size:\", len(val_df))\n    print(\"Test size:\", len(test_df))\n    print(train_df['search_query'].value_counts(normalize=True))\n    print(val_df['search_query'].value_counts(normalize=True))\n    print(test_df['search_query'].value_counts(normalize=True))\n\n    # Save file\n    # Reset index\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    test_df = test_df.reset_index(drop=True)\n    \n    # Kiểm tra số lượng mẫu sau khi chia\n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n    \n    # Lưu các DataFrame vào JSON\n    train_df.to_json(\"train.json\", orient='records', indent=4, force_ascii=False)\n    val_df.to_json(\"val.json\", orient='records', indent=4, force_ascii=False)\n    test_df.to_json(\"test.json\", orient='records', indent=4, force_ascii=False)\n    \n    print(\"Đã lưu train.json, val.json, test.json\")\n    return train_df, val_df, test_df, df\n\ntrain_df, val_df, test_df, df = load_split_save(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:02.045160Z","iopub.execute_input":"2025-05-27T15:45:02.045494Z","iopub.status.idle":"2025-05-27T15:45:02.294375Z","shell.execute_reply.started":"2025-05-27T15:45:02.045471Z","shell.execute_reply":"2025-05-27T15:45:02.293455Z"}},"outputs":[{"name":"stdout","text":"Train size: 29640\nVal size: 3708\nTest size: 3708\nsearch_query\nvỉa hè đường phố việt nam               0.031039\nđường phố sau bão                       0.023212\nngười đợi xe bus tại trạm               0.023077\nnắng nóng đường phố việt nam            0.022807\nngười đi bộ tại ngã tư                  0.022267\n                                          ...   \nchướng ngại vật trên đường đi bộ        0.004049\ncông trình đào đường                    0.003914\nvạch dành cho người khiếm thị           0.002834\nbiển báo chữ nổi cho người khiếm thị    0.001754\nlối đi dành cho người khuyết tật        0.001350\nName: proportion, Length: 65, dtype: float64\nsearch_query\nvỉa hè đường phố việt nam               0.031284\nđường phố sau bão                       0.023732\nngười đi bộ tại ngã tư                  0.022654\nnắng nóng đường phố việt nam            0.022654\nngười đợi xe bus tại trạm               0.022654\n                                          ...   \nchướng ngại vật trên đường đi bộ        0.004315\ncông trình đào đường                    0.003236\nvạch dành cho người khiếm thị           0.003236\nbiển báo chữ nổi cho người khiếm thị    0.002157\nlối đi dành cho người khuyết tật        0.001079\nName: proportion, Length: 65, dtype: float64\nsearch_query\nvỉa hè đường phố việt nam               0.031284\nđường phố sau bão                       0.023732\nngười đi bộ tại ngã tư                  0.022654\nnắng nóng đường phố việt nam            0.022654\nngười đợi xe bus tại trạm               0.022654\n                                          ...   \nchướng ngại vật trên đường đi bộ        0.004315\ncông trình đào đường                    0.003236\nvạch dành cho người khiếm thị           0.003236\nbiển báo chữ nổi cho người khiếm thị    0.002157\nlối đi dành cho người khuyết tật        0.001079\nName: proportion, Length: 65, dtype: float64\nTrain: 29640, Val: 3708, Test: 3708\nĐã lưu train.json, val.json, test.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Giả sử bạn muốn stratify theo cột 'search_query'\ndf_part1, df_part2 = train_test_split(\n    train_df,\n    test_size=len(all_pkl_data)*3,\n    stratify=train_df['search_query'],\n    random_state=42  # Đảm bảo kết quả reproducible\n)\n\nprint(\"Phần 1:\", df_part1.shape)\nprint(\"Phần 2:\", df_part2.shape)\ntrain_df = df_part2.copy()\n\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\ndf = df.copy().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:02.295503Z","iopub.execute_input":"2025-05-27T15:45:02.295744Z","iopub.status.idle":"2025-05-27T15:45:02.342191Z","shell.execute_reply.started":"2025-05-27T15:45:02.295727Z","shell.execute_reply":"2025-05-27T15:45:02.341404Z"}},"outputs":[{"name":"stdout","text":"Phần 1: (21912, 4)\nPhần 2: (7728, 4)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# 4. Load feature extractor","metadata":{}},{"cell_type":"code","source":"feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n\ndef load_image(local_path, dataset_base_path=\"/kaggle/input/traffic-pictures-captioning/\"):\n    \"\"\"\n    Load ảnh từ local_path trong dataset trên Kaggle.\n    \n    Parameters:\n    - local_path (str): Đường dẫn tương đối của ảnh (từ cột 'url' trong DataFrame)\n    - dataset_base_path (str): Đường dẫn gốc đến dataset\n    \n    Returns:\n    - image_rgb (numpy.ndarray): Ảnh ở định dạng RGB, hoặc None nếu lỗi\n    \"\"\"\n    try:\n        # Chuẩn hóa đường dẫn ảnh\n        local_path = local_path.lstrip('./')  # Loại bỏ './' nếu có\n        full_image_path = os.path.join(dataset_base_path, local_path)\n        \n        # Đọc ảnh bằng OpenCV\n        image = cv2.imread(full_image_path)\n        if image is None:\n            print(f\"Lỗi: Không thể đọc ảnh từ {full_image_path}\")\n            return None\n        \n        # Chuyển từ BGR sang RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image_rgb\n    \n    except Exception as e:\n        print(f\"Lỗi khi xử lý ảnh {full_image_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:02.601355Z","iopub.execute_input":"2025-05-27T15:45:02.601666Z","iopub.status.idle":"2025-05-27T15:45:02.790527Z","shell.execute_reply.started":"2025-05-27T15:45:02.601644Z","shell.execute_reply":"2025-05-27T15:45:02.789472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdffd6c35124d38af3412274f4ed471"}},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# 5. Load Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer của BartPho\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n\n# Kiểm tra vocab size\nvocab_size = tokenizer.vocab_size\nprint(f\"VOCAB SIZE: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:18.227019Z","iopub.execute_input":"2025-05-27T15:45:18.227604Z","iopub.status.idle":"2025-05-27T15:45:20.000991Z","shell.execute_reply.started":"2025-05-27T15:45:18.227580Z","shell.execute_reply":"2025-05-27T15:45:20.000061Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/897 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf7aa29483c4479c8b7eef78a73bca18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7ec007bf00435299b6401bec887dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f30bab348324c8592b959c94a87cc0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576a2a3ed7374d07bc1b56e5d475040f"}},"metadata":{}},{"name":"stdout","text":"VOCAB SIZE: 64000\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# In ra các special tokens\nprint(\"Special Tokens:\", tokenizer.special_tokens_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:22.319248Z","iopub.execute_input":"2025-05-27T15:45:22.319849Z","iopub.status.idle":"2025-05-27T15:45:22.324558Z","shell.execute_reply.started":"2025-05-27T15:45:22.319823Z","shell.execute_reply":"2025-05-27T15:45:22.323716Z"}},"outputs":[{"name":"stdout","text":"Special Tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# 6. Các hàm tiền xử lý caption","metadata":{}},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    return re.sub(r\"[^\\w\\s,!?.]\", \"\", text).strip()\n\ndef to_lowercase(text: str) -> str:\n    return text.lower()\n\ndef join_vietnamese_compounds(text: str) -> str:\n    return ViTokenizer.tokenize(text)\n\ndef caption_preprocess(text: str) -> str:\n    text = clean_text(text)\n    text = to_lowercase(text)\n    text = join_vietnamese_compounds(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:22.863397Z","iopub.execute_input":"2025-05-27T15:45:22.863740Z","iopub.status.idle":"2025-05-27T15:45:22.869622Z","shell.execute_reply.started":"2025-05-27T15:45:22.863685Z","shell.execute_reply":"2025-05-27T15:45:22.868539Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"text = caption_preprocess(\"Kiểm tra phân tách từ\")\n\n# Tách token\ntokens = tokenizer.tokenize(text)\n\n# Chuyển token thành ID\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\ntext = tokenizer.decode(token_ids, skip_special_tokens=True)\n# In kết quả\nprint(\"List Word (Tokenized):\", tokens)\nprint(\"List Token ID:\", token_ids)\nprint(\"Text:\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:23.323278Z","iopub.execute_input":"2025-05-27T15:45:23.323574Z","iopub.status.idle":"2025-05-27T15:45:23.332196Z","shell.execute_reply.started":"2025-05-27T15:45:23.323555Z","shell.execute_reply":"2025-05-27T15:45:23.331361Z"}},"outputs":[{"name":"stdout","text":"List Word (Tokenized): ['kiểm_tra', 'phân_tách', 'từ']\nList Token ID: [342, 31166, 39]\nText: kiểm_tra phân_tách từ\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Hàm tính số từ của một caption\ndef count_words(caption):\n    preprocessed_caption = caption_preprocess(caption)\n    tokens = tokenizer.tokenize(preprocessed_caption)\n    return len(tokens)\n\n# Áp dụng hàm cho toàn bộ dataframe\ndf['word_count'] = df['caption'].apply(count_words)\n\n# Lấy độ dài lớn nhất\nmax_length = df['word_count'].max()\n\n# In ra\nprint(f\"Độ dài caption dài nhất là: {max_length} từ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:25.771370Z","iopub.execute_input":"2025-05-27T15:45:25.771654Z","iopub.status.idle":"2025-05-27T15:45:54.387566Z","shell.execute_reply.started":"2025-05-27T15:45:25.771636Z","shell.execute_reply":"2025-05-27T15:45:54.386572Z"}},"outputs":[{"name":"stdout","text":"Độ dài caption dài nhất là: 71 từ\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# 7. Tiền xử lý input model","metadata":{}},{"cell_type":"code","source":"def process_data(image_url, caption):\n    try:\n        img_array = load_image(image_url)\n        if img_array is None:\n            return None\n        \n        pixel_values = feature_extractor(img_array, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        caption = caption_preprocess(caption)\n        tokenized_caption = tokenizer(caption, padding=\"max_length\", max_length=max_length, truncation=True)\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": torch.tensor(tokenized_caption[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(tokenized_caption[\"attention_mask\"])\n        }\n    except Exception as e:\n        print(f\"Error processing data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.388887Z","iopub.execute_input":"2025-05-27T15:45:54.389157Z","iopub.status.idle":"2025-05-27T15:45:54.395032Z","shell.execute_reply.started":"2025-05-27T15:45:54.389131Z","shell.execute_reply":"2025-05-27T15:45:54.394179Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# 8. Tạo tập dữ liệu huấn luyện","metadata":{}},{"cell_type":"code","source":"class ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions):\n        self.image_paths = image_paths\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        data = process_data(self.image_paths[idx], self.captions[idx])\n        if data is None:\n            return self.__getitem__((idx + 1) % len(self))\n        return data\n\ndef custom_collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    return {\n        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch])\n    }\n\ntrain_dataset = ImageCaptionDataset(train_df[\"url\"], train_df[\"caption\"])\nval_dataset = ImageCaptionDataset(val_df[\"url\"], val_df[\"caption\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.395804Z","iopub.execute_input":"2025-05-27T15:45:54.396109Z","iopub.status.idle":"2025-05-27T15:45:54.420212Z","shell.execute_reply.started":"2025-05-27T15:45:54.396090Z","shell.execute_reply":"2025-05-27T15:45:54.419246Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"print(len(train_pkl_data))\nprint(train_pkl_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.422200Z","iopub.execute_input":"2025-05-27T15:45:54.422478Z","iopub.status.idle":"2025-05-27T15:45:54.445188Z","shell.execute_reply.started":"2025-05-27T15:45:54.422458Z","shell.execute_reply":"2025-05-27T15:45:54.444200Z"}},"outputs":[{"name":"stdout","text":"2060\n{'image_url': 'http://160.191.244.13:9000/lakehouse/imcp/augmented-data/2025-05-24/7edd6547cc8e80c4d02f5f73fab60105.jpg', 'pixel_values': tensor([[[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n          [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n          [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n          ...,\n          [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n          [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n          [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n         [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n          [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n          [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n          ...,\n          [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n          [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n          [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n         [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n          [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n          [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n          ...,\n          [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n          [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n          [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]]), 'input_ids': tensor([[    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n           515,   105,    52,  2227,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(len(train_dataset))\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.446174Z","iopub.execute_input":"2025-05-27T15:45:54.446529Z","iopub.status.idle":"2025-05-27T15:45:54.524742Z","shell.execute_reply.started":"2025-05-27T15:45:54.446502Z","shell.execute_reply":"2025-05-27T15:45:54.523898Z"}},"outputs":[{"name":"stdout","text":"7728\n{'pixel_values': tensor([[[ 0.4510,  0.4588,  0.4667,  ..., -0.7725, -0.6941, -0.7647],\n         [ 0.4588,  0.4667,  0.4667,  ..., -0.6627, -0.6392, -0.7804],\n         [ 0.4667,  0.4667,  0.4745,  ..., -0.6235, -0.7020, -0.8588],\n         ...,\n         [-0.1373, -0.1529, -0.1686,  ..., -0.6471, -0.6157, -0.6000],\n         [-0.1765, -0.1529, -0.1529,  ..., -0.5451, -0.5373, -0.5765],\n         [-0.1765, -0.1529, -0.1608,  ..., -0.3569, -0.3176, -0.3647]],\n\n        [[ 0.5137,  0.5216,  0.5294,  ..., -0.7333, -0.6549, -0.7255],\n         [ 0.5216,  0.5294,  0.5294,  ..., -0.6235, -0.6000, -0.7412],\n         [ 0.5294,  0.5294,  0.5373,  ..., -0.5843, -0.6627, -0.8196],\n         ...,\n         [-0.1294, -0.1451, -0.1608,  ..., -0.6235, -0.5922, -0.5765],\n         [-0.1686, -0.1451, -0.1451,  ..., -0.5059, -0.4980, -0.5373],\n         [-0.1686, -0.1451, -0.1529,  ..., -0.3098, -0.2706, -0.3255]],\n\n        [[ 0.5294,  0.5373,  0.5451,  ..., -0.7098, -0.6314, -0.7020],\n         [ 0.5373,  0.5451,  0.5451,  ..., -0.6000, -0.5765, -0.7176],\n         [ 0.5451,  0.5451,  0.5529,  ..., -0.5608, -0.6392, -0.7961],\n         ...,\n         [-0.0980, -0.1137, -0.1294,  ..., -0.6157, -0.5922, -0.5765],\n         [-0.1373, -0.1137, -0.1137,  ..., -0.4980, -0.4980, -0.5294],\n         [-0.1373, -0.1137, -0.1216,  ..., -0.2941, -0.2549, -0.3020]]]), 'input_ids': tensor([   0,  448, 7416,   91,  368,  888, 4968,  109,    5, 8994,   17,  297,\n           5, 1690, 2633,  216,   71,    5,  105, 2786,    6,  820,  145,   41,\n           5,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Class tạo Dataset từ list các dict\nclass DatasetFromList(Dataset):\n    def __init__(self, data_list):\n        self.data_list = data_list\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, idx):\n        return self.data_list[idx]\n\n# Hàm tạo DatasetFromList từ list dict\ndef dataset_from_list(data_list):\n    return DatasetFromList(data_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.525597Z","iopub.execute_input":"2025-05-27T15:45:54.525853Z","iopub.status.idle":"2025-05-27T15:45:54.531095Z","shell.execute_reply.started":"2025-05-27T15:45:54.525835Z","shell.execute_reply":"2025-05-27T15:45:54.530288Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"new_data = []\n\nfor item in train_pkl_data:\n    if \"image_url\" in item:\n        del item[\"image_url\"]\n\n    if \"pixel_values\" in item:\n        item[\"pixel_values\"] = item[\"pixel_values\"].squeeze(0)\n    \n    # Xử lý input_ids và attention_mask: squeeze rồi pad/cắt\n    if \"input_ids\" in item and \"attention_mask\" in item:\n        input_ids = item[\"input_ids\"].squeeze(0)\n        attention_mask = item[\"attention_mask\"].squeeze(0)\n\n        # Chuyển tensor sang list để dễ xử lý\n        input_ids_list = input_ids.tolist()\n        attention_mask_list = attention_mask.tolist()\n\n        # Pad hoặc truncate cho input_ids\n        if len(input_ids_list) < max_length:\n            pad_length = max_length - len(input_ids_list)\n            input_ids_list += [1] * pad_length          # pad với 1 (token id padding)\n            attention_mask_list += [0] * pad_length     # pad mask bằng 0\n        else:\n            input_ids_list = input_ids_list[:max_length]\n            attention_mask_list = attention_mask_list[:max_length]\n\n        # Chuyển lại thành tensor\n        import torch\n        item[\"input_ids\"] = torch.tensor(input_ids_list)\n        item[\"attention_mask\"] = torch.tensor(attention_mask_list)\n\n    new_data.append(item)\n\n\ncleaned_dataset = dataset_from_list(new_data)\nprint(cleaned_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.532105Z","iopub.execute_input":"2025-05-27T15:45:54.532397Z","iopub.status.idle":"2025-05-27T15:45:54.670708Z","shell.execute_reply.started":"2025-05-27T15:45:54.532372Z","shell.execute_reply":"2025-05-27T15:45:54.669856Z"}},"outputs":[{"name":"stdout","text":"{'pixel_values': tensor([[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n         [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n         [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n         ...,\n         [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n         [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n         [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n        [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n         [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n         [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n         ...,\n         [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n         [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n         [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n        [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n         [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n         [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n         ...,\n         [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n         [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n         [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]), 'input_ids': tensor([    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n          515,   105,    52,  2227,     5,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"Ghép dataset cho dữ liệu cũ và dữ liệu mới từ API","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\n\ntrain_dataset = ConcatDataset([cleaned_dataset, train_dataset])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.671515Z","iopub.execute_input":"2025-05-27T15:45:54.671738Z","iopub.status.idle":"2025-05-27T15:45:54.676771Z","shell.execute_reply.started":"2025-05-27T15:45:54.671720Z","shell.execute_reply":"2025-05-27T15:45:54.675751Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(len(train_dataset))\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:45:54.677645Z","iopub.execute_input":"2025-05-27T15:45:54.677952Z","iopub.status.idle":"2025-05-27T15:45:54.698751Z","shell.execute_reply.started":"2025-05-27T15:45:54.677925Z","shell.execute_reply":"2025-05-27T15:45:54.697863Z"}},"outputs":[{"name":"stdout","text":"9788\n{'pixel_values': tensor([[[-0.7804, -0.7804, -0.7961,  ..., -0.7647, -0.7647, -0.7647],\n         [-0.7804, -0.7725, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n         [-0.7804, -0.7569, -0.7569,  ..., -0.7725, -0.7804, -0.7569],\n         ...,\n         [ 0.5137,  0.5765,  0.5686,  ...,  0.4745,  0.3961,  0.4824],\n         [ 0.5765,  0.5608,  0.5294,  ...,  0.5216,  0.4431,  0.4667],\n         [ 0.5608,  0.5451,  0.5451,  ...,  0.5922,  0.4980,  0.4588]],\n\n        [[-0.3882, -0.3725, -0.3647,  ..., -0.3176, -0.3569, -0.3882],\n         [-0.3725, -0.3647, -0.3569,  ..., -0.3098, -0.3412, -0.3725],\n         [-0.3490, -0.3255, -0.3255,  ..., -0.2941, -0.3255, -0.3490],\n         ...,\n         [ 0.3882,  0.4510,  0.4431,  ...,  0.4118,  0.3333,  0.4275],\n         [ 0.4588,  0.4431,  0.3961,  ...,  0.4667,  0.3961,  0.4196],\n         [ 0.4431,  0.4275,  0.4118,  ...,  0.5451,  0.4510,  0.4118]],\n\n        [[ 0.1059,  0.1216,  0.1373,  ...,  0.1765,  0.1216,  0.0667],\n         [ 0.1216,  0.1294,  0.1451,  ...,  0.1843,  0.1529,  0.0980],\n         [ 0.1294,  0.1608,  0.1686,  ...,  0.2000,  0.1686,  0.1373],\n         ...,\n         [ 0.2471,  0.3020,  0.3020,  ...,  0.3255,  0.2471,  0.3333],\n         [ 0.3098,  0.2941,  0.2471,  ...,  0.3725,  0.3020,  0.3255],\n         [ 0.2941,  0.2784,  0.2549,  ...,  0.4510,  0.3569,  0.3176]]]), 'input_ids': tensor([    0,   109,    52,  1160,     4,   448, 17922,     5,    66,    10,\n          515,   105,    52,  2227,     5,     2,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# 9. Cấu hình LoRA","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom peft import PeftModel\n\n# Cấu hình LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],  # Các lớp trong GPT2\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:31.246506Z","iopub.execute_input":"2025-05-27T15:02:31.246782Z","iopub.status.idle":"2025-05-27T15:02:31.756331Z","shell.execute_reply.started":"2025-05-27T15:02:31.246763Z","shell.execute_reply":"2025-05-27T15:02:31.755473Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"# 10. Định nghĩa mô hình","metadata":{}},{"cell_type":"code","source":"vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\ndel vit_model.classifier\nvit_model.vit.pooler = torch.nn.Sequential(OrderedDict([\n    ('dense', torch.nn.Linear(in_features=768, out_features=768, bias=True)),\n    ('activation', torch.nn.Tanh())\n]))\n\nconfig = GPT2Config.from_pretrained(\"gpt2\")\nconfig.add_cross_attention = True\nconfig.vocab_size = vocab_size\n\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config, ignore_mismatched_sizes=True)\ngpt2_model.resize_token_embeddings(config.vocab_size)\n\n# Áp dụng LoRA vào GPT2\ngpt2_model = get_peft_model(gpt2_model, lora_config)\ngpt2_model.print_trainable_parameters()  # kiểm tra số lượng tham số cần huấn luyện\n\nmodel = VisionEncoderDecoderModel(encoder=vit_model.vit, decoder=gpt2_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:31.757235Z","iopub.execute_input":"2025-05-27T15:02:31.757480Z","iopub.status.idle":"2025-05-27T15:02:40.773661Z","shell.execute_reply.started":"2025-05-27T15:02:31.757461Z","shell.execute_reply":"2025-05-27T15:02:40.772765Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dc24359cb274bb0ba07b812455f7a43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299bf316cdf24bf0aa392654336d3a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec49cfa60f64f9dba752b2dbbc81be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd40acd3e824c6c8ab20528659a1384"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized because the shapes did not match:\n- transformer.wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([64000, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10748bea4b384952a620837455f5bfc1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,179,648 || all params: 164,540,928 || trainable%: 0.7169\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# 11. Định nghĩa lớp mô hình","metadata":{}},{"cell_type":"code","source":"class LoRACaptionWrapper(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    def load_context(self, context):\n        try:\n            # Đường dẫn đầy đủ đến các thư mục con trong artifact\n            model_path = context.artifacts[\"model_dir\"]\n            tokenizer_path = os.path.join(model_path, \"tokenizer\")\n            feature_extractor_path = os.path.join(model_path, \"feature_extractor\")\n\n            \n            # Load các thành phần với đường dẫn chính xác\n            self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(self.device)\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n            self.feature_extractor = ViTImageProcessor.from_pretrained(feature_extractor_path)\n            \n            # Đặt model vào chế độ eval\n            self.model.eval()\n        except Exception as e:\n            raise ValueError(f\"Error loading model: {str(e)}\")\n\n    def predict(self, context, model_input):\n        try:\n            # Xử lý đầu vào\n            if isinstance(model_input, dict):\n                image_url = model_input[\"url\"][0] if \"url\" in model_input else model_input[\"image_path\"][0]\n            else:\n                image_url = model_input.iloc[0][\"url\"] if \"url\" in model_input.columns else model_input.iloc[0][\"image_path\"]\n            \n            image = self.load_image(image_url)\n            if image is None:\n                return [\"\"]\n                \n            # Tiền xử lý ảnh và tạo caption\n            pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n            \n            output_ids = self.model.generate(\n                pixel_values,\n                max_length=45,\n                min_length=20,\n                num_beams=4,\n                do_sample=True,\n                temperature=0.8,\n                top_k=20,\n                top_p=0.9,\n                no_repeat_ngram_size=3,\n                repetition_penalty=2.0,\n                early_stopping=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                decoder_start_token_id=self.tokenizer.bos_token_id\n            )\n            \n            caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            return [caption.replace(\"_\", \" \").strip()]\n            \n        except Exception as e:\n            print(f\"[ERROR] during prediction: {e}\")\n            return [\"\"]\n   \n    def load_image(self, image_url):\n        try:\n            import requests\n            import numpy as np\n            from PIL import Image\n            from io import BytesIO\n            \n            response = requests.get(image_url, timeout=10)\n            if response.status_code != 200:\n                return None\n                \n            image = Image.open(BytesIO(response.content))\n            return image.convert(\"RGB\") if image else None\n            \n        except Exception as e:\n            print(f\"[ERROR] loading image: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.774637Z","iopub.execute_input":"2025-05-27T15:02:40.774955Z","iopub.status.idle":"2025-05-27T15:02:40.896513Z","shell.execute_reply.started":"2025-05-27T15:02:40.774923Z","shell.execute_reply":"2025-05-27T15:02:40.895442Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n  color_warning(\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# 12. Huấn luyện mô hình","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ndef evaluate_model(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n\n            encoder_outputs = model.encoder(pixel_values=pixel_values)\n            outputs = model.decoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                encoder_hidden_states=encoder_outputs.last_hidden_state,\n                labels=input_ids\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n    return total_loss / len(val_dataloader)\n\ndef train_model(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr=5e-5, epochs=1, epoch_count=1, model_name='BartPho_ViT_GPT2_LoRA_ICG', data_part=\"Part_1\", df_log=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Optimizer chỉ update các tham số có requires_grad=True (LoRA adapter)\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Thống kê tham số trainable\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n\n    # Lấy ngày hiện tại theo format dd-mm-yyyy\n    today_str = datetime.now().strftime(\"%d-%m-%Y\")\n    run_name = f'{model_name}_{today_str}_{data_part}'\n\n    with mlflow.start_run(run_name=run_name):\n        if df_log is not None:\n            dataset = mlflow.data.from_pandas(df_log, targets=\"caption\")\n            mlflow.log_input(dataset, context=\"training\")\n\n        # Logging các siêu tham số\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"epoch_count\", epoch_count)\n        mlflow.log_param(\"data_part\", data_part)\n        mlflow.log_param(\"learning_rate\", lr)\n        mlflow.log_param(\"train_data_size\", len(train_dataloader.dataset))\n        mlflow.log_param(\"val_data_size\", len(val_dataloader.dataset))\n        mlflow.log_param(\"trainable_params\", trainable_params)\n        mlflow.log_param(\"total_params\", total_params)\n        mlflow.log_param(\"trainable_ratio\", round(trainable_params / total_params, 4))\n        mlflow.log_param(\"model_name\", model_name)\n\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch in train_dataloader:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n\n                encoder_outputs = model.encoder(pixel_values=pixel_values)\n                outputs = model.decoder(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_outputs.last_hidden_state,\n                    labels=input_ids\n                )\n                loss = outputs.loss\n                total_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_loss = total_loss / len(train_dataloader)\n            val_loss = evaluate_model(model, val_dataloader, device)\n\n            # Log loss theo epoch\n            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n            print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n            # Lưu mô hình sau mỗi epoch\n            epoch_save_path = f\"/kaggle/working/{model_name}_epoch_{epoch+1}\"\n            model.save_pretrained(epoch_save_path)\n            tokenizer.save_pretrained(os.path.join(epoch_save_path, \"tokenizer\"))\n            feature_extractor.save_pretrained(os.path.join(epoch_save_path, \"feature_extractor\"))\n            \n            # Log toàn bộ thư mục như một artifact\n            mlflow.log_artifacts(epoch_save_path, artifact_path=f\"epoch_{epoch+1}\")\n\n        # Lưu model cuối cùng riêng biệt\n        final_model_path = f\"/kaggle/working/{model_name}_final\"\n        model.save_pretrained(final_model_path)\n        tokenizer.save_pretrained(os.path.join(final_model_path, \"tokenizer\"))\n        feature_extractor.save_pretrained(os.path.join(final_model_path, \"feature_extractor\"))\n        mlflow.log_artifacts(final_model_path, artifact_path=\"final_model\")\n\n        # Log model dưới dạng pyfunc (nếu cần)\n        # Đảm bảo bạn đã định nghĩa class ImageCaptionModel trước\n\n        # Infer signature\n        input_example = pd.DataFrame({\"url\": [\"http://example.com/test.jpg\"]})\n        output_example = pd.DataFrame({\"caption\": [\"Mô tả ảnh\"]})\n        \n        # 2. Tạo model signature\n        signature = infer_signature(\n            input_example,\n            output_example,\n            params={\"input_types\": \"string\", \"output_types\": \"string\"}  # Chỉ định kiểu dữ liệu\n        )\n        mlflow.pyfunc.log_model(\n            artifact_path=\"model\",\n            python_model=LoRACaptionWrapper(),\n            artifacts={\"model_dir\": final_model_path},  # Đường dẫn đến thư mục chứa model, tokenizer và feature_extractor\n            registered_model_name=model_name,\n            signature=signature,\n            # input_example=input_example,\n            pip_requirements=pip_reqs\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.897571Z","iopub.execute_input":"2025-05-27T15:02:40.897837Z","iopub.status.idle":"2025-05-27T15:02:40.926123Z","shell.execute_reply.started":"2025-05-27T15:02:40.897817Z","shell.execute_reply":"2025-05-27T15:02:40.924956Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Tạo k Subset cho train, val\ntrain_subset = Subset(train_dataset, range(len(train_dataset)))  # Lấy train_df.shape[0] dữ liệu đầu tiên từ train_dataset\nval_subset = Subset(val_dataset, range(val_df.shape[0]))     # Lấy 4620 dữ liệu đầu tiên từ val_dataset\n\n# Tạo DataLoader cho các subset\ntrain_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\nval_dataloader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.927156Z","iopub.execute_input":"2025-05-27T15:02:40.927422Z","iopub.status.idle":"2025-05-27T15:02:40.939827Z","shell.execute_reply.started":"2025-05-27T15:02:40.927401Z","shell.execute_reply":"2025-05-27T15:02:40.938835Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# 13. Load model pretrain","metadata":{}},{"cell_type":"code","source":"# Đường dẫn thư mục chứa các tệp mô hình\nmodel_path = '/kaggle/input/phovit-gptcap/pytorch/default/15'\n\n# Tải mô hình VisionEncoderDecoder\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path, ignore_mismatched_sizes=True)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:40.940841Z","iopub.execute_input":"2025-05-27T15:02:40.941206Z","iopub.status.idle":"2025-05-27T15:02:46.819404Z","shell.execute_reply.started":"2025-05-27T15:02:40.941182Z","shell.execute_reply":"2025-05-27T15:02:46.818326Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# 14. Huấn luyện","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\ntrain_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    tokenizer,\n    feature_extractor,\n    lr=5e-5,\n    epochs=2,\n    epoch_count=17,\n    model_name=\"BartPho_ViT_GPT2_LoRA_ICG_Incremental\",\n    data_part=\"FULL\",\n    df_log=df\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:02:46.820508Z","iopub.execute_input":"2025-05-27T15:02:46.820814Z","iopub.status.idle":"2025-05-27T15:03:35.521995Z","shell.execute_reply.started":"2025-05-27T15:02:46.820788Z","shell.execute_reply":"2025-05-27T15:03:35.520364Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run BartPho_ViT_GPT2_LoRA_ICG Incremental_27-05-2025_FULL at: http://36.50.135.226:7893/#/experiments/10/runs/6439b7c1809f469f924ebca8c6be1057\n🧪 View experiment at: http://36.50.135.226:7893/#/experiments/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/821225077.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr, epochs, epoch_count, model_name, data_part, df_log)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    783\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/842534181.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/821225077.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, tokenizer, feature_extractor, lr, epochs, epoch_count, model_name, data_part, df_log)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{model_name}_{today_str}_{data_part}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf_log\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"caption\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive_run_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFINISHED\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mend_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mend_run\u001b[0;34m(status)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mlast_active_run_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0m_last_active_run_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_active_run_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_terminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_active_run_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlast_active_run_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_id_to_system_metrics_monitor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0msystem_metrics_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_id_to_system_metrics_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_active_run_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mset_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m   3353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3354\u001b[0m         \"\"\"\n\u001b[0;32m-> 3355\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_terminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mset_terminated\u001b[0;34m(self, run_id, status, end_time)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshut_down_async_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         self.store.update_run_info(\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mrun_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRunStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36mupdate_run_info\u001b[0;34m(self, run_id, run_status, end_time, run_name)\u001b[0m\n\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUpdateRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36m_call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_METHOD_TO_INFO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_host_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     def search_experiments(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_rest_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0mresponse_to_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0mjs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_to_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_parse_as_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             base_msg = (\n","\u001b[0;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: The run 6439b7c1809f469f924ebca8c6be1057 must be in the 'active' state. Current state is deleted."],"ename":"RestException","evalue":"INVALID_PARAMETER_VALUE: The run 6439b7c1809f469f924ebca8c6be1057 must be in the 'active' state. Current state is deleted.","output_type":"error"}],"execution_count":58}]}