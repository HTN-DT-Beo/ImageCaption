# ImageCaption
### Introduction
This neural system for image captioning is based on the combination of YOLOv8 for visual feature extraction and GPT-2 for natural language generation. The input is an image, and the output is a descriptive sentence of the image content. YOLOv8, a state-of-the-art object detection model, is used to extract relevant visual features from the image. These features are then passed to GPT-2, a transformer-based language model, which generates a fluent and coherent caption describing the image. The system is implemented using the PyTorch library for model training and inference, allowing for efficient end-to-end training of the image feature extraction and caption generation components.

### Prerequisites
* **Tensorflow** ([instructions](https://www.tensorflow.org/install/))
* **NumPy** ([instructions](https://scipy.org/install.html))
* **OpenCV** ([instructions](https://pypi.python.org/pypi/opencv-python))
* **Natural Language Toolkit (NLTK)** ([instructions](http://www.nltk.org/install.html))
* **Pandas** ([instructions](https://scipy.org/install.html))
* **Matplotlib** ([instructions](https://scipy.org/install.html))
* **tqdm** ([instructions](https://pypi.python.org/pypi/tqdm))

### Usage
* **Preparation:** Load the LAION-220K GPT-4 Vision Captions dataset from Hugging Face  [here](https://huggingface.co/datasets/laion/220k-GPT4Vision-captions-from-LIVIS) . The dataset contains a large collection of images along with captions that you can use for training the image captioning model. 

### Results
* **BLEU-1 = 88%**
* **BLEU-2 = 94%**
* **BLEU-3 = 96%**
* **BLEU-4 = 97%**
* **ROUGE-1 = 79.6%**
* **ROUGE-2 = 69.9%**
* **ROUGE-L = 73.9%**
* **CIDEr = 3.91**
* **METEOR = 38.5%**
![ROUGE-Score](![image](https://github.com/user-attachments/assets/3a30cc33-441f-4320-af92-63ac51559809))
![CIDEr-METEOR-Score](![image](https://github.com/user-attachments/assets/1f1e95d0-075c-48a9-9b57-a7dfb239d315))

Here are some captions generated by this model:
![examples](![image](https://github.com/user-attachments/assets/ca21c937-672b-451c-9f84-f34515a63927))

### References
* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. ICML 2015.
* [vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)
